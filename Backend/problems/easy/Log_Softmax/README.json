{
    "title": "Implementation of Log Softmax Function (Easy) âœ”",
    "sections": [
        {
            "title": "Table of Contents",
            "content": "\n- [Problem Statement](#problem-statement)\n- [Example](#example)\n- [Learn: Understanding Log Softmax Function](#learn-understanding-log-softmax-function)\n- [Solutions](#solutions)\n  - [Custom Implementation](#custom-implementation)\n  - [NumPy Implementation](#numpy-implementation)\n- [Code Explanation](#code-explanation)\n\n",
            "code_blocks": [],
            "links": [
                {
                    "text": "Problem Statement",
                    "url": "#problem-statement"
                },
                {
                    "text": "Example",
                    "url": "#example"
                },
                {
                    "text": "Learn: Understanding Log Softmax Function",
                    "url": "#learn-understanding-log-softmax-function"
                },
                {
                    "text": "Solutions",
                    "url": "#solutions"
                },
                {
                    "text": "Custom Implementation",
                    "url": "#custom-implementation"
                },
                {
                    "text": "NumPy Implementation",
                    "url": "#numpy-implementation"
                },
                {
                    "text": "Code Explanation",
                    "url": "#code-explanation"
                }
            ]
        },
        {
            "title": "Problem Statement",
            "content": "\n[Implementation of Log Softmax Function](https://www.deep-ml.com/problem/Log%20Softmax)\n\nIn machine learning and statistics, the softmax function is a generalization of the logistic function that converts a vector of scores into probabilities. The log-softmax function is the logarithm of the softmax function, and it is often used for numerical stability when computing the softmax of large numbers. Given a 1D numpy array of scores, implement a Python function to compute the log-softmax of the array.\n\n",
            "code_blocks": [],
            "links": [
                {
                    "text": "Implementation of Log Softmax Function",
                    "url": "https://www.deep-ml.com/problem/Log%20Softmax"
                }
            ]
        },
        {
            "title": "Example",
            "content": "\n\n",
            "code_blocks": [
                {
                    "language": "python",
                    "code": "A = np.array([1, 2, 3])\nprint(log_softmax(A))\n\nOutput:\narray([-2.4076, -1.4076, -0.4076])\n"
                }
            ],
            "links": []
        },
        {
            "title": "Learn: Understanding Log Softmax Function",
            "content": "\nThe log softmax function is a numerically stable way of calculating the logarithm of the softmax function. The softmax function converts a vector of arbitrary values (logits) into a vector of probabilities, where each value lies between 0 and 1, and the values sum to 1. The softmax function is given by:\n\n$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$$\n\nHowever, directly applying the logarithm to the softmax function can lead to numerical instability, especially when dealing with large numbers. To prevent this, we use the log-softmax function, which incorporates a shift by subtracting the maximum value from the input vector:\n\n$$\n\\text{log-softmax}(x_i) = x_i - \\max(x) - \\log\\left(\\sum_{j=1}^n e^{x_j - \\max(x)}\\right)\n$$\n\nThis formulation helps to avoid overflow issues that can occur when exponentiating large numbers. The log-softmax function is particularly useful in machine learning for calculating probabilities in a stable manner, especially when used with cross-entropy loss functions.\n\n",
            "code_blocks": [],
            "links": []
        },
        {
            "title": "Solutions",
            "content": "\n",
            "code_blocks": [],
            "links": []
        },
        {
            "title": "Custom Implementation",
            "content": "\n\n",
            "code_blocks": [
                {
                    "language": "python",
                    "code": "import numpy as np\n\ndef log_softmax(scores: list[int | float]) -> np.ndarray:\n    total_log = np.sum([np.exp(i - max(scores)) for i in scores])\n    log_softmax = [scores[i] - max(scores) - np.log(total_log) for i in range(len(scores))]\n    return log_softmax # type: ignore\n\nA = np.array([1, 2, 3])\nprint(log_softmax(A))\n"
                }
            ],
            "links": []
        },
        {
            "title": "NumPy Implementation",
            "content": "\n\n",
            "code_blocks": [
                {
                    "language": "python",
                    "code": "import numpy as np\n\ndef log_softmax(scores: list) -> np.ndarray:\n    # Subtract the maximum value for numerical stability\n    scores = scores - np.max(scores)\n    return scores - np.log(np.sum(np.exp(scores)))\n\nA = np.array([1, 2, 3])\nprint(log_softmax(A))\n"
                }
            ],
            "links": []
        },
        {
            "title": "Code Explanation",
            "content": "\nTwo implementations of the log_softmax function are provided: a custom implementation and a NumPy-based implementation.\n\n",
            "code_blocks": [],
            "links": []
        },
        {
            "title": "Custom Implementation",
            "content": "\nThe custom implementation follows these steps:\n\n1. Calculate the maximum value of the input scores.\n2. Compute the exponential of each score minus the maximum value.\n3. Sum these exponentials to get the total_log.\n4. Calculate the log-softmax for each score using the formula:\n   log_softmax(x_i) = x_i - max(x) - log(total_log)\n5. Return the resulting list of log-softmax values.\n\nThis implementation closely follows the mathematical definition of the log-softmax function, providing a clear and intuitive approach.\n\n",
            "code_blocks": [],
            "links": []
        },
        {
            "title": "NumPy Implementation",
            "content": "\nThe NumPy implementation leverages built-in NumPy functions for efficient computation:\n\n1. Subtract the maximum value from all scores for numerical stability.\n2. Apply the exponential function to all shifted scores.\n3. Sum the exponentials and take the logarithm.\n4. Subtract this logarithm from the shifted scores.\n\nThis implementation is more concise and potentially more efficient, especially for larger input arrays, as it utilizes NumPy's vectorized operations.\n\nBoth implementations produce the same result but differ in their approach. The custom implementation provides a more explicit calculation process, while the NumPy version is more compact and may offer better performance for larger datasets.\n",
            "code_blocks": [],
            "links": []
        }
    ]
}