[
  {
    "id": 1,
    "title": "Matrix-Vector Dot Product",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgZG90IHByb2R1Y3Qgb2YgYSBtYXRyaXggYW5kIGEgdmVjdG9yLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiBhIGxpc3QgcmVwcmVzZW50aW5nIHRoZSByZXN1bHRpbmcgdmVjdG9yIGlmIHRoZSBvcGVyYXRpb24gaXMgdmFsaWQsIG9yIC0xIGlmIHRoZSBtYXRyaXggYW5kIHZlY3RvciBkaW1lbnNpb25zIGFyZSBpbmNvbXBhdGlibGUuIEEgbWF0cml4IChhIGxpc3Qgb2YgbGlzdHMpIGNhbiBiZSBkb3R0ZWQgd2l0aCBhIHZlY3RvciAoYSBsaXN0KSBvbmx5IGlmIHRoZSBudW1iZXIgb2YgY29sdW1ucyBpbiB0aGUgbWF0cml4IGVxdWFscyB0aGUgbGVuZ3RoIG9mIHRoZSB2ZWN0b3IuIEZvciBleGFtcGxlLCBhbiBuIHggbSBtYXRyaXggcmVxdWlyZXMgYSB2ZWN0b3Igb2YgbGVuZ3RoIG0u",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def matrix_dot_vector_tg(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute the product of matrix `a` and vector `b` using tinygrad.\n        Will be tinygrad Tensors.\n        Returns a 1-D np.ndarray of length m, or -1 if dimensions mismatch.\n        \"\"\"\n        pass\n",
    "example_input": "a = [[1, 2], [2, 4]], b = [1, 2]",
    "example_output": "[5, 10]",
    "example_reasoning": "Row 1: (1 * 1) + (2 * 2) = 1 + 4 = 5; Row 2: (1 * 2) + (2 * 4) = 2 + 8 = 10",
    "learn_content": "CiMjIE1hdHJpeC1WZWN0b3IgRG90IFByb2R1Y3QKCkNvbnNpZGVyIGEgbWF0cml4ICRBJCBhbmQgYSB2ZWN0b3IgJHYkOgoKKipNYXRyaXggJEEkIChuIHggbSk6KioKJCQKQSA9IFxiZWdpbntwbWF0cml4fQphX3sxMX0gJiBhX3sxMn0gJiBcY2RvdHMgJiBhX3sxbX0gXFwKYV97MjF9ICYgYV97MjJ9ICYgXGNkb3RzICYgYV97Mm19IFxcClx2ZG90cyAmIFx2ZG90cyAmIFxkZG90cyAmIFx2ZG90cyBcXAphX3tuMX0gJiBhX3tuMn0gJiBcY2RvdHMgJiBhX3tubX0KXGVuZHtwbWF0cml4fQokJAoKKipWZWN0b3IgJHYkIChsZW5ndGggbSk6KioKJCQKdiA9IFxiZWdpbntwbWF0cml4fQp2XzEgXFwKdl8yIFxcClx2ZG90cyBcXAp2X20KXGVuZHtwbWF0cml4fQokJAoKVGhlIGRvdCBwcm9kdWN0ICRBIFxjZG90IHYkIHByb2R1Y2VzIGEgbmV3IHZlY3RvciBvZiBsZW5ndGggJG4kOgokJApBIFxjZG90IHYgPSBcYmVnaW57cG1hdHJpeH0KYV97MTF9dl8xICsgYV97MTJ9dl8yICsgXGNkb3RzICsgYV97MW19dl9tIFxcCmFfezIxfXZfMSArIGFfezIyfXZfMiArIFxjZG90cyArIGFfezJtfXZfbSBcXApcdmRvdHMgXFwKYV97bjF9dl8xICsgYV97bjJ9dl8yICsgXGNkb3RzICsgYV97bm19dl9tClxlbmR7cG1hdHJpeH0KJCQKCiMjIyBLZXkgUmVxdWlyZW1lbnQ6ClRoZSBudW1iZXIgb2YgY29sdW1ucyBpbiB0aGUgbWF0cml4ICgkbSQpIG11c3QgZXF1YWwgdGhlIGxlbmd0aCBvZiB0aGUgdmVjdG9yICgkbSQpLiBJZiBub3QsIHRoZSBvcGVyYXRpb24gaXMgdW5kZWZpbmVkLCBhbmQgdGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gLTEu",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def matrix_dot_vector_tg(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute the product of matrix `a` and vector `b` using tinygrad.\n        Inputs will be tinygrad Tensors.\n        Returns a 1-D np.ndarray of length m, or -1 if dimensions mismatch.\n        \"\"\"\n        if len(a[0]) != len(b):\n            return -1\n        return a @ b",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2,
              3
            ],
            [
              2,
              4,
              5
            ],
            [
              6,
              8,
              9
            ]
          ],
          [
            1,
            2,
            3
          ]
        ],
        "expected_output": [
          14,
          25,
          49
        ]
      },
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              2,
              4
            ],
            [
              6,
              8
            ],
            [
              12,
              4
            ]
          ],
          [
            1,
            2,
            3
          ]
        ],
        "expected_output": -1
      }
    ],
    "function_name": "matrix_dot_vector_tg"
  },
  {
    "id": 2,
    "title": "Transpose of a Matrix",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgdHJhbnNwb3NlIG9mIGEgZ2l2ZW4gbWF0cml4Lg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def transpose_matrix_tg(self, a: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Transpose a 2D matrix `a` using tinygrad.\n        Inputs are tinygrad Tensors.\n        Returns a transposed np.ndarray.\n        \"\"\"\n        pass",
    "example_input": "a = [[1,2,3],[4,5,6]]",
    "example_output": "[[1,4],[2,5],[3,6]]",
    "example_reasoning": "The transpose of a matrix is obtained by flipping rows and columns.",
    "learn_content": "CiMjIFRyYW5zcG9zZSBvZiBhIE1hdHJpeAoKTGV0J3MgY29uc2lkZXIgYSBtYXRyaXggJE0kIGFuZCBpdHMgdHJhbnNwb3NlICRNXlQkOgoKKipPcmlnaW5hbCBNYXRyaXggJE0kOioqCiQkCk0gPSBcYmVnaW57cG1hdHJpeH0gCmEgJiBiICYgYyBcXCAKZCAmIGUgJiBmIApcZW5ke3BtYXRyaXh9CiQkCgoqKlRyYW5zcG9zZWQgTWF0cml4ICRNXlQkOioqCiQkCk1eVCA9IFxiZWdpbntwbWF0cml4fSAKYSAmIGQgXFwgCmIgJiBlIFxcIApjICYgZiAKXGVuZHtwbWF0cml4fQokJAoKIyMjIEV4cGxhbmF0aW9uOgpUcmFuc3Bvc2luZyBhIG1hdHJpeCBpbnZvbHZlcyBjb252ZXJ0aW5nIGl0cyByb3dzIGludG8gY29sdW1ucyBhbmQgdmljZSB2ZXJzYS4gVGhpcyBvcGVyYXRpb24gaXMgZnVuZGFtZW50YWwgaW4gbGluZWFyIGFsZ2VicmEgZm9yIHZhcmlvdXMgY29tcHV0YXRpb25zIGFuZCB0cmFuc2Zvcm1hdGlvbnMu",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def transpose_matrix_tg(self, a) -> np.ndarray:\n        \"\"\"\n        Transpose a 2D matrix `a` using tinygrad.\n        Inputs are tinygrad Tensors.\n        Returns a transposed np.ndarray.\n        \"\"\"\n        return a.T",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ],
            [
              5,
              6
            ]
          ]
        ],
        "expected_output": [
          [
            1,
            3,
            5
          ],
          [
            2,
            4,
            6
          ]
        ]
      },
      {
        "test": [
          [
            [
              1,
              2,
              3
            ],
            [
              4,
              5,
              6
            ]
          ]
        ],
        "expected_output": [
          [
            1,
            4
          ],
          [
            2,
            5
          ],
          [
            3,
            6
          ]
        ]
      }
    ],
    "function_name": "transpose_matrix_tg"
  },
  {
    "id": 3,
    "title": "Reshape Matrix",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCByZXNoYXBlcyBhIGdpdmVuIG1hdHJpeCBpbnRvIGEgc3BlY2lmaWVkIHNoYXBlLiBpZiBpdCBjYW50IGJlIHJlc2hhcGVkIHJldHVybiBiYWNrIGFuIGVtcHR5IGxpc3QgYFsgXWA=",
    "starter_code": "import torch\n\nclass Solution:\n    def reshape_matrix(self, a, new_shape) -> torch.Tensor:\n        \"\"\"\n        Reshape a 2D matrix `a` to shape `new_shape` using PyTorch.\n        Inputs can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a tensor of shape `new_shape`, or an empty tensor on mismatch.\n        \"\"\"\n        # Dimension check\n        if len(a) * len(a[0]) != new_shape[0] * new_shape[1]:\n            return torch.tensor([])\n        # Convert to tensor and reshape\n        a_t = torch.as_tensor(a, dtype=torch.float)\n        # Your implementation here\n        pass\n",
    "example_input": "a = [[1,2,3,4],[5,6,7,8]], new_shape = (4, 2)",
    "example_output": "[[1, 2], [3, 4], [5, 6], [7, 8]]",
    "example_reasoning": "The given matrix is reshaped from 2x4 to 4x2.",
    "learn_content": "CiMjIFJlc2hhcGluZyBhIE1hdHJpeAoKTWF0cml4IHJlc2hhcGluZyBpbnZvbHZlcyBjaGFuZ2luZyB0aGUgc2hhcGUgb2YgYSBtYXRyaXggd2l0aG91dCBhbHRlcmluZyBpdHMgZGF0YS4gVGhpcyBpcyBlc3NlbnRpYWwgaW4gbWFueSBtYWNoaW5lIGxlYXJuaW5nIHRhc2tzIHdoZXJlIHRoZSBpbnB1dCBkYXRhIG5lZWRzIHRvIGJlIGZvcm1hdHRlZCBpbiBhIHNwZWNpZmljIHdheS4KCkZvciBleGFtcGxlLCBjb25zaWRlciBhIG1hdHJpeCAkTSQ6CgoqKk9yaWdpbmFsIE1hdHJpeCAkTSQ6KioKJCQKTSA9IFxiZWdpbntwbWF0cml4fSAKMSAmIDIgJiAzICYgNCBcXCAKNSAmIDYgJiA3ICYgOCAKXGVuZHtwbWF0cml4fQokJAoKKipSZXNoYXBlZCBNYXRyaXggJE0nJCB3aXRoIHNoYXBlICg0LCAyKToqKgokJApNJyA9IFxiZWdpbntwbWF0cml4fSAKMSAmIDIgXFwgCjMgJiA0IFxcIAo1ICYgNiBcXCAKNyAmIDggClxlbmR7cG1hdHJpeH0KJCQKCiMjIyBJbXBvcnRhbnQgTm90ZToKRW5zdXJlIHRoZSB0b3RhbCBudW1iZXIgb2YgZWxlbWVudHMgcmVtYWlucyBjb25zdGFudCBkdXJpbmcgcmVzaGFwaW5nLgo=",
    "solution_code": "import torch\n\nclass Solution:\n    def reshape_matrix(self, a, new_shape) -> torch.Tensor:\n        \"\"\"\n        Reshape a 2D matrix `a` to shape `new_shape` using PyTorch.\n        Inputs can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a tensor of shape `new_shape`, or an empty tensor on mismatch.\n        \"\"\"\n        # Dimension check\n        if len(a) * len(a[0]) != new_shape[0] * new_shape[1]:\n            return torch.tensor([])\n        a_t = torch.as_tensor(a, dtype=torch.float)\n        return a_t.reshape(new_shape)\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2,
              3,
              4
            ],
            [
              5,
              6,
              7,
              8
            ]
          ],
          [
            4,
            2
          ]
        ],
        "expected_output": [
          [
            1,
            2
          ],
          [
            3,
            4
          ],
          [
            5,
            6
          ],
          [
            7,
            8
          ]
        ]
      },
      {
        "test": [
          [
            [
              1,
              2,
              3,
              4
            ],
            [
              5,
              6,
              7,
              8
            ]
          ],
          [
            1,
            4
          ]
        ],
        "expected_output": []
      }
    ],
    "function_name": "reshape_matrix"
  },
  {
    "id": 4,
    "title": "Calculate Mean by Row or Column",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSBtZWFuIG9mIGEgbWF0cml4IGVpdGhlciBieSByb3cgb3IgYnkgY29sdW1uLCBiYXNlZCBvbiBhIGdpdmVuIG1vZGUuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBhIG1hdHJpeCAobGlzdCBvZiBsaXN0cykgYW5kIGEgbW9kZSAoJ3Jvdycgb3IgJ2NvbHVtbicpIGFzIGlucHV0IGFuZCByZXR1cm4gYSBsaXN0IG9mIG1lYW5zIGFjY29yZGluZyB0byB0aGUgc3BlY2lmaWVkIG1vZGUu",
    "starter_code": "import torch\n\nclass Solution:\n    def calculate_matrix_mean(self, matrix, mode: str) -> torch.Tensor:\n        \"\"\"\n        Calculate mean of a 2D matrix per row or per column using PyTorch.\n        Inputs can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a 1-D tensor of means or raises ValueError on invalid mode.\n        \"\"\"\n        a_t = torch.as_tensor(matrix, dtype=torch.float)\n        # Your implementation here\n        pass\n",
    "example_input": "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]], mode = 'column'",
    "example_output": "[4.0, 5.0, 6.0]",
    "example_reasoning": "Calculating the mean of each column results in [(1+4+7)/3, (2+5+8)/3, (3+6+9)/3].",
    "learn_content": "CiMjIENhbGN1bGF0ZSBNZWFuIGJ5IFJvdyBvciBDb2x1bW4KCkNhbGN1bGF0aW5nIHRoZSBtZWFuIG9mIGEgbWF0cml4IGJ5IHJvdyBvciBjb2x1bW4gaW52b2x2ZXMgYXZlcmFnaW5nIHRoZSBlbGVtZW50cyBhY3Jvc3MgdGhlIHNwZWNpZmllZCBkaW1lbnNpb24uIFRoaXMgb3BlcmF0aW9uIHByb3ZpZGVzIGluc2lnaHRzIGludG8gdGhlIGRpc3RyaWJ1dGlvbiBvZiB2YWx1ZXMgd2l0aGluIHRoZSBkYXRhc2V0LCB1c2VmdWwgZm9yIGRhdGEgbm9ybWFsaXphdGlvbiBhbmQgc2NhbGluZy4KCiMjIyBSb3cgTWVhbgpUaGUgbWVhbiBvZiBhIHJvdyBpcyBjb21wdXRlZCBieSBzdW1taW5nIGFsbCBlbGVtZW50cyBpbiB0aGUgcm93IGFuZCBkaXZpZGluZyBieSB0aGUgbnVtYmVyIG9mIGVsZW1lbnRzLiBGb3Igcm93ICRpJCwgdGhlIG1lYW4gaXM6CiQkClxtdV97XHRleHR7cm93IH0gaX0gPSBcZnJhY3sxfXtufSBcc3VtX3tqPTF9XntufSBhX3tpan0KJCQKd2hlcmUgJGFfe2lqfSQgaXMgdGhlIG1hdHJpeCBlbGVtZW50IGluIHRoZSAkaV57XHRleHR7dGh9fSQgcm93IGFuZCAkal57XHRleHR7dGh9fSQgY29sdW1uLCBhbmQgJG4kIGlzIHRoZSB0b3RhbCBudW1iZXIgb2YgY29sdW1ucy4KCiMjIyBDb2x1bW4gTWVhbgpTaW1pbGFybHksIHRoZSBtZWFuIG9mIGEgY29sdW1uIGlzIGZvdW5kIGJ5IHN1bW1pbmcgYWxsIGVsZW1lbnRzIGluIHRoZSBjb2x1bW4gYW5kIGRpdmlkaW5nIGJ5IHRoZSBudW1iZXIgb2YgZWxlbWVudHMuIEZvciBjb2x1bW4gJGokLCB0aGUgbWVhbiBpczoKJCQKXG11X3tcdGV4dHtjb2x1bW4gfSBqfSA9IFxmcmFjezF9e219IFxzdW1fe2k9MX1ee219IGFfe2lqfQokJAp3aGVyZSAkbSQgaXMgdGhlIHRvdGFsIG51bWJlciBvZiByb3dzLgoKVGhpcyBtYXRoZW1hdGljYWwgZm9ybXVsYXRpb24gaGVscHMgaW4gdW5kZXJzdGFuZGluZyBob3cgZGF0YSBpcyBhZ2dyZWdhdGVkIGFjcm9zcyBkaWZmZXJlbnQgZGltZW5zaW9ucywgYSBjcml0aWNhbCBzdGVwIGluIHZhcmlvdXMgZGF0YSBwcmVwcm9jZXNzaW5nIHRlY2huaXF1ZXMuCg==",
    "solution_code": "import torch\n\nclass Solution:\n    def calculate_matrix_mean(self, matrix, mode: str) -> torch.Tensor:\n        \"\"\"\n        Calculate mean of a 2D matrix per row or per column using PyTorch.\n        Inputs can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a 1-D tensor of means or raises ValueError on invalid mode.\n        \"\"\"\n        a_t = torch.as_tensor(matrix, dtype=torch.float)\n        if mode == 'column':\n            return a_t.mean(dim=0)\n        elif mode == 'row':\n            return a_t.mean(dim=1)\n        else:\n            raise ValueError(\"Mode must be 'row' or 'column'\")\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2,
              3
            ],
            [
              4,
              5,
              6
            ],
            [
              7,
              8,
              9
            ]
          ],
          "column"
        ],
        "expected_output": [
          4.0,
          5.0,
          6.0
        ]
      },
      {
        "test": [
          [
            [
              1,
              2,
              3
            ],
            [
              4,
              5,
              6
            ],
            [
              7,
              8,
              9
            ]
          ],
          "row"
        ],
        "expected_output": [
          2.0,
          5.0,
          8.0
        ]
      }
    ],
    "function_name": "calculate_matrix_mean"
  },
  {
    "id": 5,
    "title": "Scalar Multiplication of a Matrix",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBtdWx0aXBsaWVzIGEgbWF0cml4IGJ5IGEgc2NhbGFyIGFuZCByZXR1cm5zIHRoZSByZXN1bHQu",
    "starter_code": "import torch\n\nclass Solution:\n    def scalar_multiply(self, matrix, scalar) -> torch.Tensor:\n        \"\"\"\n        Multiply each element of a 2D matrix by a scalar using PyTorch.\n        Inputs can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a 2D tensor of the same shape.\n        \"\"\"\n        # Convert input to tensor\n        m_t = torch.as_tensor(matrix, dtype=torch.float)\n        # Your implementation here\n        pass\n",
    "example_input": "matrix = [[1, 2], [3, 4]], scalar = 2",
    "example_output": "[[2, 4], [6, 8]]",
    "example_reasoning": "Each element of the matrix is multiplied by the scalar.",
    "learn_content": "CiMjIFNjYWxhciBNdWx0aXBsaWNhdGlvbiBvZiBhIE1hdHJpeAoKV2hlbiBhIG1hdHJpeCAkQSQgaXMgbXVsdGlwbGllZCBieSBhIHNjYWxhciAkayQsIHRoZSBvcGVyYXRpb24gaXMgZGVmaW5lZCBhcyBtdWx0aXBseWluZyBlYWNoIGVsZW1lbnQgb2YgJEEkIGJ5ICRrJC4KCkdpdmVuIGEgbWF0cml4ICRBJDoKJCQKQSA9IFxiZWdpbntwbWF0cml4fSAKYV97MTF9ICYgYV97MTJ9IFxcIAphX3syMX0gJiBhX3syMn0gClxlbmR7cG1hdHJpeH0KJCQKCkFuZCBhIHNjYWxhciAkayQsIHRoZSByZXN1bHQgb2YgdGhlIHNjYWxhciBtdWx0aXBsaWNhdGlvbiAka0EkIGlzOgokJAprQSA9IFxiZWdpbntwbWF0cml4fSAKa2FfezExfSAmIGthX3sxMn0gXFwgCmthX3syMX0gJiBrYV97MjJ9IApcZW5ke3BtYXRyaXh9CiQkCgpUaGlzIG9wZXJhdGlvbiBzY2FsZXMgdGhlIG1hdHJpeCBieSAkayQgd2l0aG91dCBjaGFuZ2luZyBpdHMgZGltZW5zaW9uIG9yIHRoZSByZWxhdGl2ZSBwcm9wb3J0aW9uIG9mIGl0cyBlbGVtZW50cy4K",
    "solution_code": "import torch\n\nclass Solution:\n    def scalar_multiply(self, matrix, scalar) -> torch.Tensor:\n        \"\"\"\n        Multiply each element of a 2D matrix by a scalar using PyTorch.\n        Inputs can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a 2D tensor of the same shape.\n        \"\"\"\n        m_t = torch.as_tensor(matrix, dtype=torch.float)\n        return m_t * scalar\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ]
          ],
          2
        ],
        "expected_output": [
          [
            2,
            4
          ],
          [
            6,
            8
          ]
        ]
      },
      {
        "test": [
          [
            [
              0,
              -1
            ],
            [
              1,
              0
            ]
          ],
          -1
        ],
        "expected_output": [
          [
            0,
            1
          ],
          [
            -1,
            0
          ]
        ]
      }
    ],
    "function_name": "scalar_multiply"
  },
  {
    "id": 6,
    "title": "Calculate Eigenvalues of a Matrix",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSBlaWdlbnZhbHVlcyBvZiBhIDJ4MiBtYXRyaXguIFRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIGEgbGlzdCBjb250YWluaW5nIHRoZSBlaWdlbnZhbHVlcywgc29ydCB2YWx1ZXMgZnJvbSBoaWdoZXN0IHRvIGxvd2VzdC4=",
    "starter_code": "import torch\n\nclass Solution:\n    def calculate_eigenvalues(self, matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute eigenvalues of a 2\u00d72 matrix using PyTorch.\n        Input: 2\u00d72 tensor; Output: 1-D tensor with the two eigenvalues in ascending order.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "matrix = [[2, 1], [1, 2]]",
    "example_output": "[3.0, 1.0]",
    "example_reasoning": "The eigenvalues of the matrix are calculated using the characteristic equation of the matrix, which for a 2x2 matrix is $\\lambda^2 - \ttrace(A)\\lambda + \tdet(A) = 0$, where $\\lambda$ are the eigenvalues.",
    "learn_content": "CiMjIENhbGN1bGF0ZSBFaWdlbnZhbHVlcwoKRWlnZW52YWx1ZXMgb2YgYSBtYXRyaXggb2ZmZXIgc2lnbmlmaWNhbnQgaW5zaWdodCBpbnRvIHRoZSBtYXRyaXgncyBiZWhhdmlvciwgcGFydGljdWxhcmx5IGluIHRoZSBjb250ZXh0IG9mIGxpbmVhciB0cmFuc2Zvcm1hdGlvbnMgYW5kIHN5c3RlbXMgb2YgbGluZWFyIGVxdWF0aW9ucy4KCiMjIyBEZWZpbml0aW9uCkZvciBhIHNxdWFyZSBtYXRyaXggJEEkLCBlaWdlbnZhbHVlcyBhcmUgc2NhbGFycyAkXGxhbWJkYSQgdGhhdCBzYXRpc2Z5IHRoZSBlcXVhdGlvbiBmb3Igc29tZSBub24temVybyB2ZWN0b3IgJHYkIChlaWdlbnZlY3Rvcik6CiQkCkF2ID0gXGxhbWJkYSB2CiQkCgojIyMgQ2FsY3VsYXRpb24gZm9yIGEgMngyIE1hdHJpeApUaGUgZWlnZW52YWx1ZXMgb2YgYSAyeDIgbWF0cml4ICRBJCwgZ2l2ZW4gYnk6CiQkCkEgPSBcYmVnaW57cG1hdHJpeH0gCmEgJiBiIFxcIApjICYgZCAKXGVuZHtwbWF0cml4fQokJAphcmUgZGV0ZXJtaW5lZCBieSBzb2x2aW5nIHRoZSBjaGFyYWN0ZXJpc3RpYyBlcXVhdGlvbjoKJCQKXGRldChBIC0gXGxhbWJkYSBJKSA9IDAKJCQKClRoaXMgc2ltcGxpZmllcyB0byBhIHF1YWRyYXRpYyBlcXVhdGlvbjoKJCQKXGxhbWJkYV4yIC0gXHRleHR7dHJ9KEEpIFxsYW1iZGEgKyBcZGV0KEEpID0gMAokJAoKSGVyZSwgdGhlIHRyYWNlIG9mICRBJCwgZGVub3RlZCBhcyAkXHRleHR7dHJ9KEEpJCwgaXMgJGEgKyBkJCwgYW5kIHRoZSBkZXRlcm1pbmFudCBvZiAkQSQsIGRlbm90ZWQgYXMgJFxkZXQoQSkkLCBpcyAkYWQgLSBiYyQuIFNvbHZpbmcgdGhpcyBlcXVhdGlvbiB5aWVsZHMgdGhlIGVpZ2VudmFsdWVzLCAkXGxhbWJkYSQuCgojIyMgU2lnbmlmaWNhbmNlClVuZGVyc3RhbmRpbmcgZWlnZW52YWx1ZXMgaXMgZXNzZW50aWFsIGZvciBhbmFseXppbmcgdGhlIGVmZmVjdHMgb2YgbGluZWFyIHRyYW5zZm9ybWF0aW9ucyByZXByZXNlbnRlZCBieSB0aGUgbWF0cml4LiBUaGV5IGFyZSBjcnVjaWFsIGluIHZhcmlvdXMgYXBwbGljYXRpb25zLCBpbmNsdWRpbmcgc3RhYmlsaXR5IGFuYWx5c2lzLCB2aWJyYXRpb24gYW5hbHlzaXMsIGFuZCBQcmluY2lwYWwgQ29tcG9uZW50IEFuYWx5c2lzIChQQ0EpIGluIG1hY2hpbmUgbGVhcm5pbmcuCg==",
    "solution_code": "import torch\n\nclass Solution:\n    def calculate_eigenvalues(self, matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute eigenvalues of a 2\u00d72 matrix using PyTorch.\n        Input: 2\u00d72 tensor; Output: 1-D tensor with the two eigenvalues in ascending order.\n        \"\"\"\n        a = matrix[0,0]; b = matrix[0,1]\n        c = matrix[1,0]; d = matrix[1,1]\n        trace = a + d\n        det = a * d - b * c\n        disc = trace * trace - 4 * det\n        sqrt_disc = torch.sqrt(disc)\n        lambda1 = (trace + sqrt_disc) / 2\n        lambda2 = (trace - sqrt_disc) / 2\n        eig = torch.stack([lambda1, lambda2])\n        return torch.sort(eig).values\n",
    "test_cases": [
      {
        "test": [
          [
            [
              2,
              1
            ],
            [
              1,
              2
            ]
          ]
        ],
        "expected_output": [
          3.0,
          1.0
        ]
      },
      {
        "test": [
          [
            [
              4,
              -2
            ],
            [
              1,
              1
            ]
          ]
        ],
        "expected_output": [
          3.0,
          2.0
        ]
      }
    ],
    "function_name": "calculate_eigenvalues"
  },
  {
    "id": 7,
    "title": "Matrix Transformation ",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCB0cmFuc2Zvcm1zIGEgZ2l2ZW4gbWF0cml4IEEgdXNpbmcgdGhlIG9wZXJhdGlvbiAgJFReey0xfSBBIFMkLCB3aGVyZSBUIGFuZCBTIGFyZSBpbnZlcnRpYmxlIG1hdHJpY2VzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIGZpcnN0IHZhbGlkYXRlIGlmIHRoZSBtYXRyaWNlcyBUIGFuZCBTIGFyZSBpbnZlcnRpYmxlLCBhbmQgdGhlbiBwZXJmb3JtIHRoZSB0cmFuc2Zvcm1hdGlvbi4gSW4gY2FzZXMgd2hlcmUgdGhlcmUgaXMgbm8gc29sdXRpb24gcmV0dXJuIC0x",
    "starter_code": "import torch\n\nclass Solution:\n    def transform_matrix(self, A, T, S) -> torch.Tensor:\n        \"\"\"\n        Perform the change-of-basis transform T\u207b\u00b9 A S and round to 3 decimals using PyTorch.\n        Inputs A, T, S can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a 2\u00d72 tensor or tensor(-1.) if T or S is singular.\n        \"\"\"\n        A_t = torch.as_tensor(A, dtype=torch.float)\n        T_t = torch.as_tensor(T, dtype=torch.float)\n        S_t = torch.as_tensor(S, dtype=torch.float)\n        # Your implementation here\n        pass\n",
    "example_input": "A = [[1, 2], [3, 4]], T = [[2, 0], [0, 2]], S = [[1, 1], [0, 1]]",
    "example_output": "[[0.5,1.5],[1.5,3.5]]",
    "example_reasoning": "The matrices T and S are used to transform matrix A by computing $T^{-1}AS$.",
    "learn_content": "IyMjIE1hdHJpeCBUcmFuc2Zvcm1hdGlvbiB1c2luZyAkVF57LTF9IEEgUyQKClRyYW5zZm9ybWluZyBhIG1hdHJpeCAkQSQgdXNpbmcgdGhlIG9wZXJhdGlvbiAkVF57LTF9IEEgUyQgaW52b2x2ZXMgc2V2ZXJhbCBzdGVwcy4gVGhpcyBvcGVyYXRpb24gY2hhbmdlcyB0aGUgYmFzaXMgb2YgbWF0cml4ICRBJCB1c2luZyB0d28gbWF0cmljZXMgJFQkIGFuZCAkUyQsIHdpdGggJFQkIGFuZCAkUyQgYmVpbmcgaW52ZXJ0aWJsZSwgdG8gYXZvaWQgbG9zcyBvZiBpbmZvcm1hdGlvbi4KKE11bHRpcGx5aW5nIGJ5IG5vbi1pbnZlcnRpYmxlICRTJCB3b3VsZCByZXN1bHQgaW4gYSBsb3NzIG9mIGRpbWVuc2lvbnMpCgojIyMgU3RlcHMgZm9yIFRyYW5zZm9ybWF0aW9uCgpHaXZlbiBtYXRyaWNlcyAkQSQsICRUJCwgYW5kICRTJDoKCjEuICoqQ2hlY2sgSW52ZXJ0aWJpbGl0eSoqOiBWZXJpZnkgdGhhdCAkVCQgYW5kICRTJCBhcmUgaW52ZXJ0aWJsZSBieSBlbnN1cmluZyB0aGVpciBkZXRlcm1pbmFudHMgYXJlIG5vbi16ZXJvOyBvdGhlcndpc2UsIHJldHVybiAkLTEkLgoyLiAqKkNvbXB1dGUgSW52ZXJzZXMqKjogRmluZCB0aGUgaW52ZXJzIG9mICRUJCwgZGVub3RlZCBhcyAkVF57LTF9JC4KMy4gKipQZXJmb3JtIE1hdHJpeCBNdWx0aXBsaWNhdGlvbioqOiBDYWxjdWxhdGUgdGhlIHRyYW5zZm9ybWVkIG1hdHJpeDoKCiAgICQkCiAgIEEnID0gVF57LTF9IEEgUwogICAkJAoKIyMjIEV4YW1wbGUKCklmOgoKJCQKQSA9ClxiZWdpbntwbWF0cml4fSAKMSAmIDIgXFwgCjMgJiA0IApcZW5ke3BtYXRyaXh9CiQkCgokJApUID0KXGJlZ2lue3BtYXRyaXh9IAoyICYgMCBcXCAKMCAmIDIgClxlbmR7cG1hdHJpeH0KJCQKCiQkClMgPQpcYmVnaW57cG1hdHJpeH0gCjEgJiAxIFxcIAowICYgMSAKXGVuZHtwbWF0cml4fQokJAoKIyMjIyBDaGVjayBJbnZlcnRpYmlsaXR5OgoKLSAkXGRldChUKSA9IDQgXG5lcSAwJAotICRcZGV0KFMpID0gMSBcbmVxIDAkCgojIyMjIENvbXB1dGUgSW52ZXJzZXM6CgokJApUXnstMX0gPQpcYmVnaW57cG1hdHJpeH0gClxmcmFjezF9ezJ9ICYgMCBcXCAKMCAmIFxmcmFjezF9ezJ9IApcZW5ke3BtYXRyaXh9CiQkCgojIyMjIFBlcmZvcm0gdGhlIFRyYW5zZm9ybWF0aW9uOgoKJCQKQScgPSBUXnstMX0gQSBTCiQkCgokJApBJyA9ClxiZWdpbntwbWF0cml4fSAKXGZyYWN7MX17Mn0gJiAwIFxcIAowICYgXGZyYWN7MX17Mn0gClxlbmR7cG1hdHJpeH0KXGJlZ2lue3BtYXRyaXh9IAoxICYgMiBcXCAKMyAmIDQgClxlbmR7cG1hdHJpeH0KXGJlZ2lue3BtYXRyaXh9IAoxICYgMSBcXCAKMCAmIDEgClxlbmR7cG1hdHJpeH0KJCQKCiQkCkEnID0KXGJlZ2lue3BtYXRyaXh9IAowLjUgJiAxLjUgXFwgCjEuNSAmIDMuNSAKXGVuZHtwbWF0cml4fQokJA==",
    "solution_code": "import torch\n\nclass Solution:\n    def transform_matrix(self, A, T, S) -> torch.Tensor:\n        \"\"\"\n        Perform the change-of-basis transform T\u207b\u00b9 A S and round to 3 decimals using PyTorch.\n        Inputs A, T, S can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a 2\u00d72 tensor or tensor(-1.) if T or S is singular.\n        \"\"\"\n        A_t = torch.as_tensor(A, dtype=torch.float)\n        T_t = torch.as_tensor(T, dtype=torch.float)\n        S_t = torch.as_tensor(S, dtype=torch.float)\n        if torch.det(T_t) == 0 or torch.det(S_t) == 0:\n            return torch.tensor(-1.)\n        T_inv = torch.inverse(T_t)\n        out = T_inv @ A_t @ S_t\n        return torch.round(out * 1000) / 1000\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ]
          ],
          [
            [
              2,
              0
            ],
            [
              0,
              2
            ]
          ],
          [
            [
              1,
              1
            ],
            [
              0,
              1
            ]
          ]
        ],
        "expected_output": [
          [
            0.5,
            1.5
          ],
          [
            1.5,
            3.5
          ]
        ]
      },
      {
        "test": [
          [
            [
              1,
              0
            ],
            [
              0,
              1
            ]
          ],
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ]
          ],
          [
            [
              2,
              0
            ],
            [
              0,
              2
            ]
          ]
        ],
        "expected_output": [
          [
            -4.0,
            2.0
          ],
          [
            3.0,
            -1.0
          ]
        ]
      }
    ],
    "function_name": "transform_matrix"
  },
  {
    "id": 8,
    "title": "Calculate 2x2 Matrix Inverse",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSBpbnZlcnNlIG9mIGEgMngyIG1hdHJpeC4gUmV0dXJuICdOb25lJyBpZiB0aGUgbWF0cml4IGlzIG5vdCBpbnZlcnRpYmxlLg==",
    "starter_code": "import torch\n\nclass Solution:\n    def inverse_2x2(self, matrix) -> torch.Tensor | None:\n        \"\"\"\n        Compute inverse of a 2\u00d72 matrix using PyTorch.\n        Input can be Python list, NumPy array, or torch np.ndarray.\n        Returns a 2\u00d72 tensor or None if the matrix is singular.\n        \"\"\"\n        m = torch.as_tensor(matrix, dtype=torch.float)\n        # Your implementation here\n        pass\n",
    "example_input": "matrix = [[4, 7], [2, 6]]",
    "example_output": "[[0.6, -0.7], [-0.2, 0.4]]",
    "example_reasoning": "The inverse of a 2x2 matrix [a, b], [c, d] is given by (1/(ad-bc)) * [d, -b], [-c, a], provided ad-bc is not zero.",
    "learn_content": "CiMjIENhbGN1bGF0aW5nIHRoZSBJbnZlcnNlIG9mIGEgMngyIE1hdHJpeAoKVGhlIGludmVyc2Ugb2YgYSBtYXRyaXggXCggQSBcKSBpcyBhbm90aGVyIG1hdHJpeCwgb2Z0ZW4gZGVub3RlZCBcKCBBXnstMX0gXCksIHN1Y2ggdGhhdDoKJCQKQUFeey0xfSA9IEFeey0xfUEgPSBJCiQkCndoZXJlIFwoIEkgXCkgaXMgdGhlIGlkZW50aXR5IG1hdHJpeC4gRm9yIGEgMngyIG1hdHJpeDoKJCQKQSA9IFxiZWdpbntwbWF0cml4fSAKYSAmIGIgXFwgCmMgJiBkIApcZW5ke3BtYXRyaXh9CiQkCgpUaGUgaW52ZXJzZSBpcyBnaXZlbiBieToKJCQKQV57LTF9ID0gXGZyYWN7MX17XGRldChBKX0gXGJlZ2lue3BtYXRyaXh9IApkICYgLWIgXFwgCi1jICYgYSAKXGVuZHtwbWF0cml4fQokJAoKcHJvdmlkZWQgdGhhdCB0aGUgZGV0ZXJtaW5hbnQgXCggXGRldChBKSA9IGFkIC0gYmMgXCkgaXMgbm9uLXplcm8uIElmIFwoIFxkZXQoQSkgPSAwIFwpLCB0aGUgbWF0cml4IGRvZXMgbm90IGhhdmUgYW4gaW52ZXJzZS4KCiMjIyBJbXBvcnRhbmNlCkNhbGN1bGF0aW5nIHRoZSBpbnZlcnNlIG9mIGEgbWF0cml4IGlzIGVzc2VudGlhbCBpbiB2YXJpb3VzIGFwcGxpY2F0aW9ucywgc3VjaCBhcyBzb2x2aW5nIHN5c3RlbXMgb2YgbGluZWFyIGVxdWF0aW9ucywgd2hlcmUgdGhlIGludmVyc2UgaXMgdXNlZCB0byBmaW5kIHNvbHV0aW9ucyBlZmZpY2llbnRseS4K",
    "solution_code": "import torch\n\nclass Solution:\n    def inverse_2x2(self, matrix) -> torch.Tensor | None:\n        \"\"\"\n        Compute inverse of a 2\u00d72 matrix using PyTorch.\n        Input can be Python list, NumPy array, or torch np.ndarray.\n        Returns a 2\u00d72 tensor or None if the matrix is singular.\n        \"\"\"\n        m = torch.as_tensor(matrix, dtype=torch.float)\n        a, b = m[0,0], m[0,1]\n        c, d = m[1,0], m[1,1]\n        det = a * d - b * c\n        if det == 0:\n            return None\n        inv = torch.stack([\n            torch.stack([ d/det, -b/det]),\n            torch.stack([-c/det,  a/det])\n        ])\n        return inv\n",
    "test_cases": [
      {
        "test": [
          [
            [
              4,
              7
            ],
            [
              2,
              6
            ]
          ]
        ],
        "expected_output": [
          [
            0.6,
            -0.7
          ],
          [
            -0.2,
            0.4
          ]
        ]
      },
      {
        "test": [
          [
            [
              2,
              1
            ],
            [
              6,
              2
            ]
          ]
        ],
        "expected_output": [
          [
            -1.0,
            0.5
          ],
          [
            3.0,
            -1.0
          ]
        ]
      }
    ],
    "function_name": "inverse_2x2"
  },
  {
    "id": 9,
    "title": "Matrix times Matrix ",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "bXVsdGlwbHkgdHdvIG1hdHJpY2VzIHRvZ2V0aGVyIChyZXR1cm4gLTEgaWYgc2hhcGVzIG9mIG1hdHJpeCBkb24ndCBhbGlnbiksIGkuZS4gJEMgPSBBIFxjZG90IEIk",
    "starter_code": "import torch\n\nclass Solution:\n    def matrixmul(self, a, b) -> torch.Tensor:\n        \"\"\"\n        Multiply two matrices using PyTorch.\n        Inputs can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a 2D tensor of shape (m, n) or a scalar tensor -1 if dimensions mismatch.\n        \"\"\"\n        a_t = torch.as_tensor(a)\n        b_t = torch.as_tensor(b)\n        # dimension mismatch\n        if a_t.size(1) != b_t.size(0):\n            return torch.tensor(-1)\n        # matrix multiplication\n        return a_t.matmul(b_t)\n",
    "example_input": "A = [[1,2],[2,4]], B = [[2,1],[3,4]]",
    "example_output": "[[ 8,  9],[16, 18]]",
    "example_reasoning": "1\\*2 + 2\\*3 = 8;                   2\\*2 + 3\\*4 = 16;                   1\\*1 + 2\\*4 = 9;                   2\\*1 + 4\\*4 = 18                    Example 2:        input: A = [[1,2],                    [2,4]],                B = [[2,1],                    [3,4],                    [4,5]]        output: -1        reasoning: the length of the rows of A does not equal          the column length of B",
    "learn_content": "CiMjIE1hdHJpeCBNdWx0aXBsaWNhdGlvbgoKQ29uc2lkZXIgdHdvIG1hdHJpY2VzIFwoIEEgXCkgYW5kIFwoIEIgXCkgdG8gZGVtb25zdHJhdGUgdGhlaXIgbXVsdGlwbGljYXRpb24sIGRlZmluZWQgYXMgZm9sbG93czoKCioqTWF0cml4IFwoIEEgXCk6KioKJCQKQSA9IFxiZWdpbntwbWF0cml4fSAKYV97MTF9ICYgYV97MTJ9IFxcIAphX3syMX0gJiBhX3syMn0gClxlbmR7cG1hdHJpeH0KJCQKCioqTWF0cml4IFwoIEIgXCk6KioKJCQKQiA9IFxiZWdpbntwbWF0cml4fSAKYl97MTF9ICYgYl97MTJ9IFxcIApiX3syMX0gJiBiX3syMn0gClxlbmR7cG1hdHJpeH0KJCQKClRoZSBtdWx0aXBsaWNhdGlvbiBvZiBtYXRyaXggXCggQSBcKSBieSBtYXRyaXggXCggQiBcKSBpcyBjYWxjdWxhdGVkIGFzOgokJApBIFx0aW1lcyBCID0gXGJlZ2lue3BtYXRyaXh9IAphX3sxMX1iX3sxMX0gKyBhX3sxMn1iX3syMX0gJiBhX3sxMX1iX3sxMn0gKyBhX3sxMn1iX3syMn0gXFwgCmFfezIxfWJfezExfSArIGFfezIyfWJfezIxfSAmIGFfezIxfWJfezEyfSArIGFfezIyfWJfezIyfSAKXGVuZHtwbWF0cml4fQokJAoKVGhpcyBvcGVyYXRpb24gcmVzdWx0cyBpbiBhIG5ldyBtYXRyaXggd2hlcmUgZWFjaCBlbGVtZW50IGlzIHRoZSByZXN1bHQgb2YgdGhlIGRvdCBwcm9kdWN0IGJldHdlZW4gdGhlIHJvd3Mgb2YgbWF0cml4IFwoIEEgXCkgYW5kIHRoZSBjb2x1bW5zIG9mIG1hdHJpeCBcKCBCIFwpLgo=",
    "solution_code": "import torch\n\nclass Solution:\n    def matrixmul(self, a, b) -> torch.Tensor:\n        \"\"\"\n        Multiply two matrices using PyTorch.\n        Inputs can be Python lists, NumPy arrays, or torch Tensors.\n        Returns a 2D tensor of shape (m, n) or a scalar tensor -1 if dimensions mismatch.\n        \"\"\"\n        a_t = torch.as_tensor(a)\n        b_t = torch.as_tensor(b)\n        if a_t.size(1) != b_t.size(0):\n            return torch.tensor(-1)\n        return a_t.matmul(b_t)\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2,
              3
            ],
            [
              2,
              3,
              4
            ],
            [
              5,
              6,
              7
            ]
          ],
          [
            [
              3,
              2,
              1
            ],
            [
              4,
              3,
              2
            ],
            [
              5,
              4,
              3
            ]
          ]
        ],
        "expected_output": [
          [
            26,
            20,
            14
          ],
          [
            38,
            29,
            20
          ],
          [
            74,
            56,
            38
          ]
        ]
      },
      {
        "test": [
          [
            [
              0,
              0
            ],
            [
              2,
              4
            ],
            [
              1,
              2
            ]
          ],
          [
            [
              0,
              0
            ],
            [
              2,
              4
            ]
          ]
        ],
        "expected_output": [
          [
            0,
            0
          ],
          [
            8,
            16
          ],
          [
            4,
            8
          ]
        ]
      }
    ],
    "function_name": "matrixmul"
  },
  {
    "id": 10,
    "title": "Calculate Covariance Matrix",
    "difficulty": "easy",
    "category": "statistics",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBjb3ZhcmlhbmNlIG1hdHJpeCBmb3IgYSBnaXZlbiBzZXQgb2YgdmVjdG9ycy4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGEgbGlzdCBvZiBsaXN0cywgd2hlcmUgZWFjaCBpbm5lciBsaXN0IHJlcHJlc2VudHMgYSBmZWF0dXJlIHdpdGggaXRzIG9ic2VydmF0aW9ucywgYW5kIHJldHVybiBhIGNvdmFyaWFuY2UgbWF0cml4IGFzIGEgbGlzdCBvZiBsaXN0cy4gQWRkaXRpb25hbGx5LCBwcm92aWRlIHRlc3QgY2FzZXMgdG8gdmVyaWZ5IHRoZSBjb3JyZWN0bmVzcyBvZiB5b3VyIGltcGxlbWVudGF0aW9uLg==",
    "starter_code": "import torch\n\nclass Solution:\n    def calculate_covariance_matrix(self, vectors) -> torch.Tensor:\n        \"\"\"\n        Calculate the covariance matrix for given feature vectors using PyTorch.\n        Input: 2D array-like of shape (n_features, n_observations).\n        Returns a tensor of shape (n_features, n_features).\n        \"\"\"\n        v_t = torch.as_tensor(vectors, dtype=torch.float)\n        # Your implementation here\n        pass\n",
    "example_input": "[[1, 2, 3], [4, 5, 6]]",
    "example_output": "[[1.0, 1.0], [1.0, 1.0]]",
    "example_reasoning": "The covariance between the two features is calculated based on their deviations from the mean. For the given vectors, both covariances are 1.0, resulting in a symmetric covariance matrix.",
    "learn_content": "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",
    "solution_code": "import torch\n\nclass Solution:\n    def calculate_covariance_matrix(self, vectors) -> torch.Tensor:\n        \"\"\"\n        Calculate the covariance matrix for given feature vectors using PyTorch.\n        Input: 2D array-like of shape (n_features, n_observations).\n        Returns a tensor of shape (n_features, n_features).\n        \"\"\"\n        v_t = torch.as_tensor(vectors, dtype=torch.float)\n        # use built-in covariance\n        return torch.cov(v_t)\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2,
              3
            ],
            [
              4,
              5,
              6
            ]
          ]
        ],
        "expected_output": [
          [
            1.0,
            1.0
          ],
          [
            1.0,
            1.0
          ]
        ]
      },
      {
        "test": [
          [
            [
              1,
              5,
              6
            ],
            [
              2,
              3,
              4
            ],
            [
              7,
              8,
              9
            ]
          ]
        ],
        "expected_output": [
          [
            7.0,
            2.5,
            2.5
          ],
          [
            2.5,
            1.0,
            1.0
          ],
          [
            2.5,
            1.0,
            1.0
          ]
        ]
      }
    ],
    "function_name": "calculate_covariance_matrix"
  },
  {
    "id": 11,
    "title": "Solve Linear Equations using Jacobi Method",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCB1c2VzIHRoZSBKYWNvYmkgbWV0aG9kIHRvIHNvbHZlIGEgc3lzdGVtIG9mIGxpbmVhciBlcXVhdGlvbnMgZ2l2ZW4gYnkgQXggPSBiLiBUaGUgZnVuY3Rpb24gc2hvdWxkIGl0ZXJhdGUgbiB0aW1lcywgcm91bmRpbmcgZWFjaCBpbnRlcm1lZGlhdGUgc29sdXRpb24gdG8gZm91ciBkZWNpbWFsIHBsYWNlcywgYW5kIHJldHVybiB0aGUgYXBwcm94aW1hdGUgc29sdXRpb24geC4",
    "starter_code": "import torch\n\nclass Solution:\n    def solve_jacobi(self, A, b, n) -> torch.Tensor:\n        \"\"\"\n        Solve Ax = b using the Jacobi iterative method for n iterations.\n        A: (m,m) tensor; b: (m,) tensor; n: number of iterations.\n        Returns a 1-D tensor of length m, rounded to 4 decimals.\n        \"\"\"\n        A_t = torch.as_tensor(A, dtype=torch.float)\n        b_t = torch.as_tensor(b, dtype=torch.float)\n        # Your implementation here\n        pass\n",
    "example_input": "A = [[5, -2, 3], [-3, 9, 1], [2, -1, -7]], b = [-1, 2, 3], n=2",
    "example_output": "[0.146, 0.2032, -0.5175]",
    "example_reasoning": "The Jacobi method iteratively solves each equation for x[i] using the formula x[i] = (1/a_ii) * (b[i] - sum(a_ij * x[j] for j != i)), where a_ii is the diagonal element of A and a_ij are the off-diagonal elements.",
    "learn_content": "CiMjIFNvbHZpbmcgTGluZWFyIEVxdWF0aW9ucyBVc2luZyB0aGUgSmFjb2JpIE1ldGhvZAoKVGhlIEphY29iaSBtZXRob2QgaXMgYW4gaXRlcmF0aXZlIGFsZ29yaXRobSB1c2VkIGZvciBzb2x2aW5nIGEgc3lzdGVtIG9mIGxpbmVhciBlcXVhdGlvbnMgXCggQXggPSBiIFwpLiBUaGlzIG1ldGhvZCBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIGZvciBsYXJnZSBzeXN0ZW1zIHdoZXJlIGRpcmVjdCBtZXRob2RzLCBzdWNoIGFzIEdhdXNzaWFuIGVsaW1pbmF0aW9uLCBhcmUgY29tcHV0YXRpb25hbGx5IGV4cGVuc2l2ZS4KCgojIyMgQWxnb3JpdGhtIE92ZXJ2aWV3CgpGb3IgYSBzeXN0ZW0gb2YgZXF1YXRpb25zIHJlcHJlc2VudGVkIGJ5IFwoIEF4ID0gYiBcKSwgd2hlcmUgXCggQSBcKSBpcyBhIG1hdHJpeCBhbmQgXCggeCBcKSBhbmQgXCggYiBcKSBhcmUgdmVjdG9ycywgdGhlIEphY29iaSBtZXRob2QgaW52b2x2ZXMgdGhlIGZvbGxvd2luZyBzdGVwczoKCjEuICoqSW5pdGlhbGl6YXRpb24qKjogU3RhcnQgd2l0aCBhbiBpbml0aWFsIGd1ZXNzIGZvciBcKCB4IFwpLgoKMi4gKipJdGVyYXRpb24qKjogRm9yIGVhY2ggZXF1YXRpb24gXCggaSBcKSwgdXBkYXRlIFwoIHhbaV0gXCkgdXNpbmc6CiAgICQkCiAgIHhbaV0gPSBcZnJhY3sxfXthX3tpaX19IFxsZWZ0KGJbaV0gLSBcc3VtX3tqIFxuZXEgaX0gYV97aWp9IHhbal1ccmlnaHQpCiAgICQkCiAgIHdoZXJlIFwoIGFfe2lpfSBcKSBhcmUgdGhlIGRpYWdvbmFsIGVsZW1lbnRzIG9mIFwoIEEgXCksIGFuZCBcKCBhX3tpan0gXCkgYXJlIHRoZSBvZmYtZGlhZ29uYWwgZWxlbWVudHMuCgozLiAqKkNvbnZlcmdlbmNlKio6IFJlcGVhdCB0aGUgaXRlcmF0aW9uIHVudGlsIHRoZSBjaGFuZ2VzIGluIFwoIHggXCkgYXJlIGJlbG93IGEgY2VydGFpbiB0b2xlcmFuY2Ugb3IgdW50aWwgYSBtYXhpbXVtIG51bWJlciBvZiBpdGVyYXRpb25zIGlzIHJlYWNoZWQuCgpUaGlzIG1ldGhvZCBhc3N1bWVzIHRoYXQgYWxsIGRpYWdvbmFsIGVsZW1lbnRzIG9mIFwoIEEgXCkgYXJlIG5vbi16ZXJvIGFuZCB0aGF0IHRoZSBtYXRyaXggaXMgZGlhZ29uYWxseSBkb21pbmFudCBvciBwcm9wZXJseSBjb25kaXRpb25lZCBmb3IgY29udmVyZ2VuY2UuCgojIyMgUHJhY3RpY2FsIENvbnNpZGVyYXRpb25zCgotIFRoZSBtZXRob2QgbWF5IG5vdCBjb252ZXJnZSBmb3IgYWxsIG1hdHJpY2VzLgotIENob29zaW5nIGEgZ29vZCBpbml0aWFsIGd1ZXNzIGNhbiBpbXByb3ZlIGNvbnZlcmdlbmNlLgotIERpYWdvbmFsIGRvbWluYW5jZSBvZiBcKCBBIFwpIGVuc3VyZXMgdGhlIGNvbnZlcmdlbmNlIG9mIHRoZSBKYWNvYmkgbWV0aG9kLgoK",
    "solution_code": "import torch\n\nclass Solution:\n    def solve_jacobi(self, A, b, n) -> torch.Tensor:\n        \"\"\"\n        Solve Ax = b using the Jacobi iterative method for n iterations.\n        A: (m,m) tensor; b: (m,) tensor; n: number of iterations.\n        Returns a 1-D tensor of length m, rounded to 4 decimals.\n        \"\"\"\n        A_t = torch.as_tensor(A, dtype=torch.float)\n        b_t = torch.as_tensor(b, dtype=torch.float)\n        d = torch.diag(A_t)\n        nda = A_t - torch.diag(d)\n        x = torch.zeros_like(b_t)\n        for _ in range(n):\n            x = (b_t - nda.matmul(x)) / d\n        return torch.round(x * 10000) / 10000\n",
    "test_cases": [
      {
        "test": [
          [
            [
              5,
              -2,
              3
            ],
            [
              -3,
              9,
              1
            ],
            [
              2,
              -1,
              -7
            ]
          ],
          [
            -1,
            2,
            3
          ],
          2
        ],
        "expected_output": [
          0.146,
          0.2032,
          -0.5175
        ]
      },
      {
        "test": [
          [
            [
              4,
              1,
              2
            ],
            [
              1,
              5,
              1
            ],
            [
              2,
              1,
              3
            ]
          ],
          [
            4,
            6,
            7
          ],
          5
        ],
        "expected_output": [
          -0.0806,
          0.9324,
          2.4422
        ]
      }
    ],
    "function_name": "solve_jacobi"
  },
  {
    "id": 12,
    "title": "Singular Value Decomposition (SVD)",
    "difficulty": "hard",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gY2FsbGVkIHN2ZF8yeDJfc2luZ3VsYXJfdmFsdWVzKEEpIHRoYXQgZmluZHMgYW4gYXBwcm94aW1hdGUgc2luZ3VsYXIgdmFsdWUgZGVjb21wb3NpdGlvbiBvZiBhIHJlYWwgMiB4IDIgbWF0cml4IHVzaW5nIG9uZSBKYWNvYmkgcm90YXRpb24uCklucHV0CkE6IGEgTnVtUHkgYXJyYXkgb2Ygc2hhcGUgKDIsIDIpCgpSdWxlcwpZb3UgbWF5IHVzZSBiYXNpYyBOdW1QeSBvcGVyYXRpb25zIChtYXRyaXggbXVsdGlwbGljYXRpb24sIHRyYW5zcG9zZSwgZWxlbWVudCB3aXNlIG1hdGgsIGV0Yy4pLgpEbyBub3QgY2FsbCBudW1weS5saW5hbGcuc3ZkIG9yIGFueSBvdGhlciBoaWdoLWxldmVsIFNWRCByb3V0aW5lLgpTdGljayB0byBhIHNpbmdsZSBKYWNvYmkgc3RlcCBubyBpdGVyYXRpdmUgcmVmaW5lbWVudHMuCgpSZXR1cm4KQSB0dXBsZSAoVSwgzqMsIFZfVCkgd2hlcmUKVSBpcyBhIDIgeCAyIG9ydGhvZ29uYWwgbWF0cml4LArOoyBpcyBhIGxlbmd0aCAyIE51bVB5IGFycmF5IGNvbnRhaW5pbmcgdGhlIHNpbmd1bGFyIHZhbHVlcywgYW5kClZfVCBpcyB0aGUgdHJhbnNwb3NlIG9mIHRoZSByaWdodC1zaW5ndWxhci12ZWN0b3IgbWF0cml4IFYu",
    "starter_code": "import torch\n\nclass Solution:\n    def svd_2x2_singular_values(self, A: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Approximate the SVD of a 2\u00d72 matrix A using one Jacobi rotation.\n        Returns (U, S, Vt) where S is a 1-D tensor of singular values.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "a = [[2, 1], [1, 2]]",
    "example_output": "(array([[-0.70710678, -0.70710678],\n                        [-0.70710678,  0.70710678]]),\n        array([3., 1.]),\n        array([[-0.70710678, -0.70710678],\n               [-0.70710678,  0.70710678]]))",
    "example_reasoning": "U is the first matrix sigma is the second vector and V is the third matrix",
    "learn_content": "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",
    "solution_code": "import torch\n\nclass Solution:\n    def svd_2x2_singular_values(self, A: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Approximate the SVD of a 2\u00d72 matrix A using one Jacobi rotation.\n        Returns (U, S, Vt) where S is a 1-D tensor of singular values.\n        \"\"\"\n        # compute A\u1d40A\n        a2 = A.transpose(0,1) @ A\n        # initialize V\n        V = torch.eye(2, dtype=A.dtype, device=A.device)\n        # Jacobi rotation angle\n        if torch.isclose(a2[0,0], a2[1,1]):\n            theta = torch.tensor(torch.pi/4, dtype=A.dtype, device=A.device)\n        else:\n            theta = 0.5 * torch.atan2(2 * a2[0,1], a2[0,0] - a2[1,1])\n        c = torch.cos(theta)\n        s = torch.sin(theta)\n        R = torch.stack([torch.stack([c, -s]), torch.stack([s, c])])\n        # diagonalize\n        D = R.transpose(0,1) @ a2 @ R\n        V = V @ R\n        # singular values\n        S = torch.sqrt(torch.tensor([D[0,0], D[1,1]], dtype=A.dtype, device=A.device))\n        # compute U\n        S_inv = torch.diag(1.0 / S)\n        U = A @ V @ S_inv\n        return U, S, V.transpose(0,1)\n",
    "test_cases": [
      {
        "test": [
          [
            [
              2,
              1
            ],
            [
              1,
              2
            ]
          ]
        ],
        "expected_output": "(array([[ 0.70710678, -0.70710678],\n       [ 0.70710678,  0.70710678]]), array([3., 1.]), array([[ 0.70710678,  0.70710678],\n       [-0.70710678,  0.70710678]]))"
      },
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ]
          ]
        ],
        "expected_output": "array([[ 0.40455358, 0.9145143 ], [ 0.9145143 , -0.40455358]]), array([5.4649857 , 0.36596619]), array([[ 0.57604844, 0.81741556], [-0.81741556, 0.57604844]])"
      }
    ],
    "function_name": "svd_2x2_singular_values"
  },
  {
    "id": 13,
    "title": "Determinant of a 4x4 Matrix using Laplace's Expansion (hard)",
    "difficulty": "hard",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSBkZXRlcm1pbmFudCBvZiBhIDR4NCBtYXRyaXggdXNpbmcgTGFwbGFjZSdzIEV4cGFuc2lvbiBtZXRob2QuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBhIHNpbmdsZSBhcmd1bWVudCwgYSA0eDQgbWF0cml4IHJlcHJlc2VudGVkIGFzIGEgbGlzdCBvZiBsaXN0cywgYW5kIHJldHVybiB0aGUgZGV0ZXJtaW5hbnQgb2YgdGhlIG1hdHJpeC4gVGhlIGVsZW1lbnRzIG9mIHRoZSBtYXRyaXggY2FuIGJlIGludGVnZXJzIG9yIGZsb2F0aW5nLXBvaW50IG51bWJlcnMuIEltcGxlbWVudCB0aGUgZnVuY3Rpb24gcmVjdXJzaXZlbHkgdG8gaGFuZGxlIHRoZSBjb21wdXRhdGlvbiBvZiBkZXRlcm1pbmFudHMgZm9yIHRoZSAzeDMgbWlub3IgbWF0cmljZXMu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def determinant_4x4_tg(self, matrix) -> np.ndarray:\n        \"\"\"\n        Compute the determinant of a 4\u00d74 matrix using tinygrad.\n        Input can be a Python list, NumPy array, or tinygrad np.ndarray of shape (4,4).\n        Returns a 0-D np.ndarray containing the determinant.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "a = [[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]",
    "example_output": "0",
    "example_reasoning": "Using Laplace's Expansion, the determinant of a 4x4 matrix is calculated by expanding it into minors and cofactors along any row or column. Given the symmetrical and linear nature of this specific matrix, its determinant is 0. The calculation for a generic 4x4 matrix involves more complex steps, breaking it down into the determinants of 3x3 matrices.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def determinant_4x4_tg(self, matrix) -> np.ndarray:\n        \"\"\"\n        Compute the determinant of a 4\u00d74 matrix using tinygrad.\n        Input can be a Python list, NumPy array, or tinygrad np.ndarray of shape (4,4).\n        Returns a 0-D np.ndarray containing the determinant.\n        \"\"\"\n        # convert to NumPy array\n        if isinstance(matrix, np.ndarray):\n            arr = matrix.numpy()\n        else:\n            arr = np.array(matrix, dtype=float)\n        det = float(np.linalg.det(arr))\n        return np.array(det)\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2,
              3,
              4
            ],
            [
              5,
              6,
              7,
              8
            ],
            [
              9,
              10,
              11,
              12
            ],
            [
              13,
              14,
              15,
              16
            ]
          ]
        ],
        "expected_output": 0
      },
      {
        "test": [
          [
            [
              4,
              3,
              2,
              1
            ],
            [
              3,
              2,
              1,
              4
            ],
            [
              2,
              1,
              4,
              3
            ],
            [
              1,
              4,
              3,
              2
            ]
          ]
        ],
        "expected_output": -160
      }
    ],
    "function_name": "determinant_4x4_tg"
  },
  {
    "id": 14,
    "title": "Linear Regression Using Normal Equation",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBsaW5lYXIgcmVncmVzc2lvbiB1c2luZyB0aGUgbm9ybWFsIGVxdWF0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBtYXRyaXggWCAoZmVhdHVyZXMpIGFuZCBhIHZlY3RvciB5ICh0YXJnZXQpIGFzIGlucHV0LCBhbmQgcmV0dXJuIHRoZSBjb2VmZmljaWVudHMgb2YgdGhlIGxpbmVhciByZWdyZXNzaW9uIG1vZGVsLiBSb3VuZCB5b3VyIGFuc3dlciB0byBmb3VyIGRlY2ltYWwgcGxhY2VzLCAtMC4wIGlzIGEgdmFsaWQgcmVzdWx0IGZvciByb3VuZGluZyBhIHZlcnkgc21hbGwgbnVtYmVyLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def linear_regression_normal_equation_tg(self, X, y) -> np.ndarray:\n        \"\"\"\n        Solve linear regression via the normal equation using tinygrad.\n        X: list, NumPy array, or np.ndarray of shape (m,n); y: shape (m,) or (m,1).\n        Returns a 1-D np.ndarray of length n, rounded to 4 decimals.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "X = [[1, 1], [1, 2], [1, 3]], y = [1, 2, 3]",
    "example_output": "[0.0, 1.0]",
    "example_reasoning": "The linear model is y = 0.0 + 1.0*x, perfectly fitting the input data.",
    "learn_content": "CiMjIExpbmVhciBSZWdyZXNzaW9uIFVzaW5nIHRoZSBOb3JtYWwgRXF1YXRpb24KCkxpbmVhciByZWdyZXNzaW9uIGFpbXMgdG8gbW9kZWwgdGhlIHJlbGF0aW9uc2hpcCBiZXR3ZWVuIGEgc2NhbGFyIGRlcGVuZGVudCB2YXJpYWJsZSBcKCB5IFwpIGFuZCBvbmUgb3IgbW9yZSBleHBsYW5hdG9yeSB2YXJpYWJsZXMgKG9yIGluZGVwZW5kZW50IHZhcmlhYmxlcykgXCggWCBcKS4gVGhlIG5vcm1hbCBlcXVhdGlvbiBwcm92aWRlcyBhbiBhbmFseXRpY2FsIHNvbHV0aW9uIHRvIGZpbmQgdGhlIGNvZWZmaWNpZW50cyBcKCBcdGhldGEgXCkgdGhhdCBtaW5pbWl6ZSB0aGUgY29zdCBmdW5jdGlvbiBmb3IgbGluZWFyIHJlZ3Jlc3Npb24uCgpHaXZlbiBhIG1hdHJpeCBcKCBYIFwpICh3aXRoIGVhY2ggcm93IHJlcHJlc2VudGluZyBhIHRyYWluaW5nIGV4YW1wbGUgYW5kIGVhY2ggY29sdW1uIGEgZmVhdHVyZSkgYW5kIGEgdmVjdG9yIFwoIHkgXCkgKHJlcHJlc2VudGluZyB0aGUgdGFyZ2V0IHZhbHVlcyksIHRoZSBub3JtYWwgZXF1YXRpb24gaXM6CiQkClx0aGV0YSA9IChYXlRYKV57LTF9WF5UeQokJAoKIyMjIEV4cGxhbmF0aW9uIG9mIFRlcm1zCjEuIFwoIFheVCBcKSBpcyB0aGUgdHJhbnNwb3NlIG9mIFwoIFggXCkuCjIuIFwoIChYXlRYKV57LTF9IFwpIGlzIHRoZSBpbnZlcnNlIG9mIHRoZSBtYXRyaXggXCggWF5UWCBcKS4KMy4gXCggeSBcKSBpcyB0aGUgdmVjdG9yIG9mIHRhcmdldCB2YWx1ZXMuCgojIyMgS2V5IFBvaW50cwotICoqRmVhdHVyZSBTY2FsaW5nKio6IFRoaXMgbWV0aG9kIGRvZXMgbm90IHJlcXVpcmUgZmVhdHVyZSBzY2FsaW5nLgotICoqTGVhcm5pbmcgUmF0ZSoqOiBUaGVyZSBpcyBubyBuZWVkIHRvIGNob29zZSBhIGxlYXJuaW5nIHJhdGUuCi0gKipDb21wdXRhdGlvbmFsIENvc3QqKjogQ29tcHV0aW5nIHRoZSBpbnZlcnNlIG9mIFwoIFheVFggXCkgY2FuIGJlIGNvbXB1dGF0aW9uYWxseSBleHBlbnNpdmUgaWYgdGhlIG51bWJlciBvZiBmZWF0dXJlcyBpcyB2ZXJ5IGxhcmdlLgoKIyMjIFByYWN0aWNhbCBJbXBsZW1lbnRhdGlvbgpBIHByYWN0aWNhbCBpbXBsZW1lbnRhdGlvbiBpbnZvbHZlcyBhdWdtZW50aW5nIFwoIFggXCkgd2l0aCBhIGNvbHVtbiBvZiBvbmVzIHRvIGFjY291bnQgZm9yIHRoZSBpbnRlcmNlcHQgdGVybSBhbmQgdGhlbiBhcHBseWluZyB0aGUgbm9ybWFsIGVxdWF0aW9uIGRpcmVjdGx5IHRvIGNvbXB1dGUgXCggXHRoZXRhIFwpLgoK",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def linear_regression_normal_equation_tg(self, X, y) -> np.ndarray:\n        \"\"\"\n        Solve linear regression via the normal equation using tinygrad.\n        X: list, NumPy array, or np.ndarray of shape (m,n); y: shape (m,) or (m,1).\n        Returns a 1-D np.ndarray of length n, rounded to 4 decimals.\n        \"\"\"\n        X_np = np.array(X, dtype=float)\n        y_np = np.array(y, dtype=float).reshape(-1,1)\n        theta = np.linalg.inv(X_np.T.dot(X_np)).dot(X_np.T).dot(y_np)\n        theta = np.round(theta.flatten(), 4)\n        return np.array(theta)\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              1
            ],
            [
              1,
              2
            ],
            [
              1,
              3
            ]
          ],
          [
            1,
            2,
            3
          ]
        ],
        "expected_output": [
          -0.0,
          1.0
        ]
      },
      {
        "test": [
          [
            [
              1,
              3,
              4
            ],
            [
              1,
              2,
              5
            ],
            [
              1,
              3,
              2
            ]
          ],
          [
            1,
            2,
            1
          ]
        ],
        "expected_output": [
          4.0,
          -1.0,
          -0.0
        ]
      }
    ],
    "function_name": "linear_regression_normal_equation_tg"
  },
  {
    "id": 15,
    "title": "Linear Regression Using Gradient Descent",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBsaW5lYXIgcmVncmVzc2lvbiB1c2luZyBncmFkaWVudCBkZXNjZW50LiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgTnVtUHkgYXJyYXlzIFggKGZlYXR1cmVzIHdpdGggYSBjb2x1bW4gb2Ygb25lcyBmb3IgdGhlIGludGVyY2VwdCkgYW5kIHkgKHRhcmdldCkgYXMgaW5wdXQsIGFsb25nIHdpdGggbGVhcm5pbmcgcmF0ZSBhbHBoYSBhbmQgdGhlIG51bWJlciBvZiBpdGVyYXRpb25zLCBhbmQgcmV0dXJuIHRoZSBjb2VmZmljaWVudHMgb2YgdGhlIGxpbmVhciByZWdyZXNzaW9uIG1vZGVsIGFzIGEgTnVtUHkgYXJyYXkuIFJvdW5kIHlvdXIgYW5zd2VyIHRvIGZvdXIgZGVjaW1hbCBwbGFjZXMuIC0wLjAgaXMgYSB2YWxpZCByZXN1bHQgZm9yIHJvdW5kaW5nIGEgdmVyeSBzbWFsbCBudW1iZXIu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def linear_regression_gradient_descent_tg(self, X, y, alpha, iterations) -> np.ndarray:\n        \"\"\"\n        Solve linear regression via gradient descent using tinygrad autograd.\n        X: np.ndarray or convertible shape (m,n); y: shape (m,) or (m,1).\n        alpha: learning rate; iterations: number of steps.\n        Returns a 1-D np.ndarray of length n, rounded to 4 decimals.\n        \"\"\"\n        X_t = np.array(X).float()\n        y_t = np.array(y).float().reshape(-1,1)\n        m, n = X_t.shape\n        theta = np.array([[0.0] for _ in range(n)])\n        # Your implementation here\n        pass\n",
    "example_input": "X = np.array([[1, 1], [1, 2], [1, 3]]), y = np.array([1, 2, 3]), alpha = 0.01, iterations = 1000",
    "example_output": "np.array([0.1107, 0.9513])",
    "example_reasoning": "The linear model is y = 0.0 + 1.0*x, which fits the input data after gradient descent optimization.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def linear_regression_gradient_descent_tg(self, X, y, alpha, iterations) -> np.ndarray:\n        \"\"\"\n        Solve linear regression via gradient descent using tinygrad autograd.\n        X: np.ndarray or convertible shape (m,n); y: shape (m,) or (m,1).\n        alpha: learning rate; iterations: number of steps.\n        Returns a 1-D np.ndarray of length n, rounded to 4 decimals.\n        \"\"\"\n        X_t = np.array(X).float()\n        y_t = np.array(y).float().reshape(-1,1)\n        m, n = X_t.shape\n        theta = np.array([[0.0] for _ in range(n)])\n        for _ in range(iterations):\n            preds = X_t.matmul(theta)\n            loss = (preds - y_t).pow(2).mean()\n            loss.backward()\n            # gradient step\n            theta = theta - alpha * theta.grad\n            theta.grad = None\n        res = theta.reshape(n).detach().numpy()\n        return np.array(np.round(res, 4))\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              1
            ],
            [
              1,
              2
            ],
            [
              1,
              3
            ]
          ],
          [
            1,
            2,
            3
          ],
          0.01,
          1000
        ],
        "expected_output": [
          0.1107,
          0.9513
        ]
      }
    ],
    "function_name": "linear_regression_gradient_descent_tg"
  },
  {
    "id": 16,
    "title": "Feature Scaling Implementation",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBmZWF0dXJlIHNjYWxpbmcgb24gYSBkYXRhc2V0IHVzaW5nIGJvdGggc3RhbmRhcmRpemF0aW9uIGFuZCBtaW4tbWF4IG5vcm1hbGl6YXRpb24uIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBhIDJEIE51bVB5IGFycmF5IGFzIGlucHV0LCB3aGVyZSBlYWNoIHJvdyByZXByZXNlbnRzIGEgZGF0YSBzYW1wbGUgYW5kIGVhY2ggY29sdW1uIHJlcHJlc2VudHMgYSBmZWF0dXJlLiBJdCBzaG91bGQgcmV0dXJuIHR3byAyRCBOdW1QeSBhcnJheXM6IG9uZSBzY2FsZWQgYnkgc3RhbmRhcmRpemF0aW9uIGFuZCBvbmUgYnkgbWluLW1heCBub3JtYWxpemF0aW9uLiBNYWtlIHN1cmUgYWxsIHJlc3VsdHMgYXJlIHJvdW5kZWQgdG8gdGhlIG5lYXJlc3QgNHRoIGRlY2ltYWwu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def feature_scaling_tg(self, data) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Standardize and Min-Max normalize input data using tinygrad.\n        Input: np.ndarray or convertible of shape (m,n).\n        Returns (standardized_data, normalized_data), both rounded to 4 decimals.\n        \"\"\"\n        data_t = np.array(data).float()\n        # Your implementation here\n        pass\n",
    "example_input": "data = np.array([[1, 2], [3, 4], [5, 6]])",
    "example_output": "([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])",
    "example_reasoning": "Standardization rescales the feature to have a mean of 0 and a standard deviation of 1.\n        Min-max normalization rescales the feature to a range of [0, 1], where the minimum feature value\n        maps to 0 and the maximum to 1.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def feature_scaling_tg(self, data) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Standardize and Min-Max normalize input data using tinygrad.\n        Input: np.ndarray or convertible of shape (m,n).\n        Returns (standardized_data, normalized_data), both rounded to 4 decimals.\n        \"\"\"\n        data_t = np.array(data).float()\n        data_np = data_t.numpy()\n        mean = np.mean(data_np, axis=0)\n        std = np.std(data_np, axis=0)\n        standardized_np = (data_np - mean) / std\n        min_val = np.min(data_np, axis=0)\n        max_val = np.max(data_np, axis=0)\n        normalized_np = (data_np - min_val) / (max_val - min_val)\n        return np.array(np.round(standardized_np, 4)), np.array(np.round(normalized_np, 4))\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ],
            [
              5,
              6
            ]
          ]
        ],
        "expected_output": "([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"
      }
    ],
    "function_name": "feature_scaling_tg"
  },
  {
    "id": 17,
    "title": "K-Means Clustering",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "WW91ciB0YXNrIGlzIHRvIHdyaXRlIGEgUHl0aG9uIGZ1bmN0aW9uIHRoYXQgaW1wbGVtZW50cyB0aGUgay1NZWFucyBjbHVzdGVyaW5nIGFsZ29yaXRobS4gVGhpcyBmdW5jdGlvbiBzaG91bGQgdGFrZSBzcGVjaWZpYyBpbnB1dHMgYW5kIHByb2R1Y2UgYSBsaXN0IG9mIGZpbmFsIGNlbnRyb2lkcy4gay1NZWFucyBjbHVzdGVyaW5nIGlzIGEgbWV0aG9kIHVzZWQgdG8gcGFydGl0aW9uIGBuYCBwb2ludHMgaW50byBga2AgY2x1c3RlcnMuIFRoZSBnb2FsIGlzIHRvIGdyb3VwIHNpbWlsYXIgcG9pbnRzIHRvZ2V0aGVyIGFuZCByZXByZXNlbnQgZWFjaCBncm91cCBieSBpdHMgY2VudGVyIChjYWxsZWQgdGhlICpjZW50cm9pZCopLgoKIyMjIEZ1bmN0aW9uIElucHV0czoKCi0gYHBvaW50c2A6IEEgbGlzdCBvZiBwb2ludHMsIHdoZXJlIGVhY2ggcG9pbnQgaXMgYSB0dXBsZSBvZiBjb29yZGluYXRlcyAoZS5nLiwgYCh4LCB5KWAgZm9yIDJEIHBvaW50cykKLSBga2A6IEFuIGludGVnZXIgcmVwcmVzZW50aW5nIHRoZSBudW1iZXIgb2YgY2x1c3RlcnMgdG8gZm9ybQotIGBpbml0aWFsX2NlbnRyb2lkc2A6IEEgbGlzdCBvZiBpbml0aWFsIGNlbnRyb2lkIHBvaW50cywgZWFjaCBhIHR1cGxlIG9mIGNvb3JkaW5hdGVzCi0gYG1heF9pdGVyYXRpb25zYDogQW4gaW50ZWdlciByZXByZXNlbnRpbmcgdGhlIG1heGltdW0gbnVtYmVyIG9mIGl0ZXJhdGlvbnMgdG8gcGVyZm9ybQoKIyMjIEZ1bmN0aW9uIE91dHB1dDoKCkEgbGlzdCBvZiB0aGUgZmluYWwgY2VudHJvaWRzIG9mIHRoZSBjbHVzdGVycywgd2hlcmUgZWFjaCBjZW50cm9pZCBpcyByb3VuZGVkIHRvIHRoZSBuZWFyZXN0IGZvdXJ0aCBkZWNpbWFsLgoK",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def k_means_clustering_tg(self, points, k, initial_centroids, max_iterations) -> list[tuple[float, ...]]:\n        \"\"\"\n        Perform k-means clustering on `points` into `k` clusters using tinygrad.\n        points: list of lists or np.ndarray, shape (n_points, n_features)\n        initial_centroids: list of lists or np.ndarray, shape (k, n_features)\n        max_iterations: maximum number of iterations\n        Returns a list of k centroids as tuples, rounded to 4 decimals.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "points = [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], k = 2, initial_centroids = [(1, 1), (10, 1)], max_iterations = 10",
    "example_output": "[(1, 2), (10, 2)]",
    "example_reasoning": "Given the initial centroids and a maximum of 10 iterations,\n        the points are clustered around these points, and the centroids are\n        updated to the mean of the assigned points, resulting in the final\n        centroids which approximate the means of the two clusters.\n        The exact number of iterations needed may vary,\n        but the process will stop after 10 iterations at most.",
    "learn_content": "CiMjIEstTWVhbnMgQ2x1c3RlcmluZyBBbGdvcml0aG0gSW1wbGVtZW50YXRpb24KCiMjIyBBbGdvcml0aG0gU3RlcHMKCjEuICoqSW5pdGlhbGl6YXRpb24qKiAgCiAgIFVzZSB0aGUgcHJvdmlkZWQgYGluaXRpYWxfY2VudHJvaWRzYCBhcyB5b3VyIHN0YXJ0aW5nIHBvaW50LiBUaGlzIHN0ZXAgaXMgYWxyZWFkeSBkb25lIGZvciB5b3UgaW4gdGhlIGlucHV0LgoKMi4gKipBc3NpZ25tZW50IFN0ZXAqKiAgCiAgIEZvciBlYWNoIHBvaW50IGluIHlvdXIgZGF0YXNldDoKICAgLSBDYWxjdWxhdGUgaXRzIGRpc3RhbmNlIHRvIGVhY2ggY2VudHJvaWQuCiAgIC0gQXNzaWduIHRoZSBwb2ludCB0byB0aGUgY2x1c3RlciBvZiB0aGUgbmVhcmVzdCBjZW50cm9pZC4gIAogICAqSGludCo6IENvbnNpZGVyIGNyZWF0aW5nIGEgaGVscGVyIGZ1bmN0aW9uIHRvIGNhbGN1bGF0ZSB0aGUgRXVjbGlkZWFuIGRpc3RhbmNlIGJldHdlZW4gdHdvIHBvaW50cy4KCjMuICoqVXBkYXRlIFN0ZXAqKiAgCiAgIEZvciBlYWNoIGNsdXN0ZXI6CiAgIC0gQ2FsY3VsYXRlIHRoZSBtZWFuIG9mIGFsbCBwb2ludHMgYXNzaWduZWQgdG8gdGhlIGNsdXN0ZXIuCiAgIC0gVXBkYXRlIHRoZSBjZW50cm9pZCB0byB0aGlzIG5ldyBtZWFuIHBvc2l0aW9uLiAgCiAgICpIaW50KjogQmUgY2FyZWZ1bCB3aXRoIHBvdGVudGlhbCBlbXB0eSBjbHVzdGVycy4gRGVjaWRlIGhvdyB5b3UnbGwgaGFuZGxlIHRoZW0gKGUuZy4sIGtlZXAgdGhlIHByZXZpb3VzIGNlbnRyb2lkKS4KCjQuICoqSXRlcmF0aW9uKiogIAogICBSZXBlYXQgc3RlcHMgMiBhbmQgMyB1bnRpbCBlaXRoZXI6CiAgIC0gVGhlIGNlbnRyb2lkcyBubyBsb25nZXIgY2hhbmdlIHNpZ25pZmljYW50bHkgKHRoaXMgY2FzZSBkb2VzIG5vdCBuZWVkIHRvIGJlIGluY2x1ZGVkIGluIHlvdXIgc29sdXRpb24pLCBvcgogICAtIFlvdSByZWFjaCB0aGUgYG1heF9pdGVyYXRpb25zYCBsaW1pdC4gIAogICAqSGludCo6IFlvdSBtaWdodCB3YW50IHRvIGtlZXAgdHJhY2sgb2YgdGhlIHByZXZpb3VzIGNlbnRyb2lkcyB0byBjaGVjayBmb3Igc2lnbmlmaWNhbnQgY2hhbmdlcy4KCjUuICoqUmVzdWx0KiogIAogICBSZXR1cm4gdGhlIGxpc3Qgb2YgZmluYWwgY2VudHJvaWRzLCBlbnN1cmluZyBlYWNoIGNvb3JkaW5hdGUgaXMgcm91bmRlZCB0byB0aGUgbmVhcmVzdCBmb3VydGggZGVjaW1hbC4KCg==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def k_means_clustering_tg(self, points, k, initial_centroids, max_iterations) -> list[tuple[float, ...]]:\n        \"\"\"\n        Perform k-means clustering on `points` into `k` clusters using tinygrad.\n        points: list of lists or np.ndarray, shape (n_points, n_features)\n        initial_centroids: list of lists or np.ndarray, shape (k, n_features)\n        max_iterations: maximum number of iterations\n        Returns a list of k centroids as tuples, rounded to 4 decimals.\n        \"\"\"\n        pts = np.array(points, dtype=float)\n        centroids = np.array(initial_centroids, dtype=float)\n        for _ in range(max_iterations):\n            # compute distances (k, n_points)\n            dists = np.array([np.linalg.norm(pts - c, axis=1) for c in centroids])\n            # assign points\n            assignments = dists.argmin(axis=0)\n            new_centroids = np.array([\n                pts[assignments == i].mean(axis=0) if np.any(assignments == i) else centroids[i]\n                for i in range(k)\n            ])\n            new_centroids = np.round(new_centroids, 4)\n            if np.array_equal(new_centroids, centroids):\n                break\n            centroids = new_centroids\n        return [tuple(c.tolist()) for c in centroids]\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              1,
              4
            ],
            [
              1,
              0
            ],
            [
              10,
              2
            ],
            [
              10,
              4
            ],
            [
              10,
              0
            ]
          ],
          2,
          [
            [
              1,
              1
            ],
            [
              10,
              1
            ]
          ],
          10
        ],
        "expected_output": [
          [
            1.0,
            2.0
          ],
          [
            10.0,
            2.0
          ]
        ]
      },
      {
        "test": [
          [
            [
              0,
              0,
              0
            ],
            [
              2,
              2,
              2
            ],
            [
              1,
              1,
              1
            ],
            [
              9,
              10,
              9
            ],
            [
              10,
              11,
              10
            ],
            [
              12,
              11,
              12
            ]
          ],
          2,
          [
            [
              1,
              1,
              1
            ],
            [
              10,
              10,
              10
            ]
          ],
          10
        ],
        "expected_output": [
          [
            1.0,
            1.0,
            1.0
          ],
          [
            10.3333,
            10.6667,
            10.3333
          ]
        ]
      }
    ],
    "function_name": "k_means_clustering_tg"
  },
  {
    "id": 18,
    "title": "Implement K-Fold Cross-Validation",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gZ2VuZXJhdGUgdHJhaW4gYW5kIHRlc3Qgc3BsaXRzIGZvciBLLUZvbGQgQ3Jvc3MtVmFsaWRhdGlvbi4gWW91ciB0YXNrIGlzIHRvIGRpdmlkZSB0aGUgZGF0YXNldCBpbnRvIGsgZm9sZHMgYW5kIHJldHVybiBhIGxpc3Qgb2YgdHJhaW4tdGVzdCBpbmRpY2VzIGZvciBlYWNoIGZvbGQu",
    "starter_code": "\n\nclass Solution:\n    def k_fold_cross_validation_tg(self, X, y, k=5, shuffle=True) -> list[tuple[list[int], list[int]]]:\n        \"\"\"\n        Return train/test index splits for k-fold cross-validation using pure Python or tinygrad.\n        X: list or np.ndarray of shape (n_samples, ...)\n        y: list or np.ndarray of shape (n_samples, ...)\n        k: number of folds\n        shuffle: whether to shuffle indices before splitting\n        Returns list of (train_idx, test_idx) pairs, each as Python lists of ints.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False)",
    "example_output": "[([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]",
    "example_reasoning": "The function splits the dataset into 5 folds without shuffling and returns train-test splits for each iteration.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyBLLUZvbGQgQ3Jvc3MtVmFsaWRhdGlvbgoKSy1Gb2xkIENyb3NzLVZhbGlkYXRpb24gaXMgYSByZXNhbXBsaW5nIHRlY2huaXF1ZSB1c2VkIHRvIGV2YWx1YXRlIG1hY2hpbmUgbGVhcm5pbmcgbW9kZWxzIGJ5IHBhcnRpdGlvbmluZyB0aGUgZGF0YXNldCBpbnRvIG11bHRpcGxlIGZvbGRzLgoKIyMjIEhvdyBpdCBXb3JrcwoxLiBUaGUgZGF0YXNldCBpcyBzcGxpdCBpbnRvICoqayoqIGVxdWFsIChvciBhbG1vc3QgZXF1YWwpIHBhcnRzIGNhbGxlZCBmb2xkcy4KMi4gRWFjaCBmb2xkIGlzIHVzZWQgKipvbmNlKiogYXMgYSB0ZXN0IHNldCwgd2hpbGUgdGhlIHJlbWFpbmluZyAqKmstMSoqIGZvbGRzIGZvcm0gdGhlIHRyYWluaW5nIHNldC4KMy4gVGhlIHByb2Nlc3MgaXMgcmVwZWF0ZWQgKiprIHRpbWVzKiosIGVuc3VyaW5nIGVhY2ggZm9sZCBzZXJ2ZXMgYXMgYSB0ZXN0IHNldCBleGFjdGx5IG9uY2UuCgojIyMgV2h5IFVzZSBLLUZvbGQgQ3Jvc3MtVmFsaWRhdGlvbj8KLSBJdCBwcm92aWRlcyBhIG1vcmUgKipyb2J1c3QqKiBlc3RpbWF0ZSBvZiBtb2RlbCBwZXJmb3JtYW5jZSB0aGFuIGEgc2luZ2xlIHRyYWluLXRlc3Qgc3BsaXQuCi0gUmVkdWNlcyBiaWFzIGludHJvZHVjZWQgYnkgYSBzaW5nbGUgdHJhaW5pbmcvdGVzdGluZyBzcGxpdC4KLSBBbGxvd3MgZXZhbHVhdGlvbiBhY3Jvc3MgbXVsdGlwbGUgZGF0YSBkaXN0cmlidXRpb25zLgoKIyMjIEltcGxlbWVudGF0aW9uIFN0ZXBzCjEuIFNodWZmbGUgdGhlIGRhdGEgaWYgcmVxdWlyZWQuCjIuIFNwbGl0IHRoZSBkYXRhc2V0IGludG8gKiprKiogZXF1YWwgKG9yIG5lYXJseSBlcXVhbCkgZm9sZHMuCjMuIEl0ZXJhdGUgb3ZlciBlYWNoIGZvbGQsIHVzaW5nIGl0IGFzIHRoZSB0ZXN0IHNldCB3aGlsZSB1c2luZyB0aGUgcmVtYWluaW5nIGRhdGEgYXMgdGhlIHRyYWluaW5nIHNldC4KNC4gUmV0dXJuIHRyYWluLXRlc3QgaW5kaWNlcyBmb3IgZWFjaCBpdGVyYXRpb24uCgpCeSBpbXBsZW1lbnRpbmcgdGhpcyBmdW5jdGlvbiwgeW91IHdpbGwgbGVhcm4gaG93IHRvICoqc3BsaXQgYSBkYXRhc2V0IGZvciBjcm9zcy12YWxpZGF0aW9uKiosIGEgY3J1Y2lhbCBzdGVwIGluIG1vZGVsIGV2YWx1YXRpb24u",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def k_fold_cross_validation_tg(self, X, y, k=5, shuffle=True) -> list[tuple[list[int], list[int]]]:\n        \"\"\"\n        Return train/test index splits for k-fold cross-validation using NumPy backend.\n        X: list or NumPy array or np.ndarray of shape (n_samples, ...)\n        y: list or NumPy array or np.ndarray of shape (n_samples, ...)\n        k: number of folds\n        shuffle: whether to shuffle indices before splitting\n        Returns list of (train_idx, test_idx) pairs, each as Python lists of ints.\n        \"\"\"\n        X_np = np.array(X)\n        n_samples = X_np.shape[0]\n        indices = np.arange(n_samples)\n        if shuffle:\n            np.random.shuffle(indices)\n        base = n_samples // k\n        extras = n_samples % k\n        fold_sizes = [base + (1 if i < extras else 0) for i in range(k)]\n        folds = []\n        start = 0\n        for fs in fold_sizes:\n            folds.append(indices[start:start+fs].tolist())\n            start += fs\n        result = []\n        for i in range(k):\n            test_idx = folds[i]\n            train_idx = [idx for j, f in enumerate(folds) if j != i for idx in f]\n            result.append((train_idx, test_idx))\n        return result\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nprint(k_fold_cross_validation_tg(np.array(Tensor([0,1,2,3,4,5,6,7,8,9])), np.array(Tensor([0,1,2,3,4,5,6,7,8,9])), k=5, shuffle=False))"
        ],
        "expected_output": [
          [
            [
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9
            ],
            [
              0,
              1
            ]
          ],
          [
            [
              0,
              1,
              4,
              5,
              6,
              7,
              8,
              9
            ],
            [
              2,
              3
            ]
          ],
          [
            [
              0,
              1,
              2,
              3,
              6,
              7,
              8,
              9
            ],
            [
              4,
              5
            ]
          ],
          [
            [
              0,
              1,
              2,
              3,
              4,
              5,
              8,
              9
            ],
            [
              6,
              7
            ]
          ],
          [
            [
              0,
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            [
              8,
              9
            ]
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nprint(k_fold_cross_validation_tg(np.array(Tensor([0,1,2,3,4,5,6,7,8,9])), np.array(Tensor([0,1,2,3,4,5,6,7,8,9])), k=2, shuffle=True))"
        ],
        "expected_output": [
          [
            [
              2,
              9,
              4,
              3,
              6
            ],
            [
              8,
              1,
              5,
              0,
              7
            ]
          ],
          [
            [
              8,
              1,
              5,
              0,
              7
            ],
            [
              2,
              9,
              4,
              3,
              6
            ]
          ]
        ]
      }
    ],
    "function_name": "k_fold_cross_validation_tg"
  },
  {
    "id": 19,
    "title": "Principal Component Analysis (PCA) Implementation",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBQcmluY2lwYWwgQ29tcG9uZW50IEFuYWx5c2lzIChQQ0EpIGZyb20gc2NyYXRjaC4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGEgMkQgTnVtUHkgYXJyYXkgYXMgaW5wdXQsIHdoZXJlIGVhY2ggcm93IHJlcHJlc2VudHMgYSBkYXRhIHNhbXBsZSBhbmQgZWFjaCBjb2x1bW4gcmVwcmVzZW50cyBhIGZlYXR1cmUuIFRoZSBmdW5jdGlvbiBzaG91bGQgc3RhbmRhcmRpemUgdGhlIGRhdGFzZXQsIGNvbXB1dGUgdGhlIGNvdmFyaWFuY2UgbWF0cml4LCBmaW5kIHRoZSBlaWdlbnZhbHVlcyBhbmQgZWlnZW52ZWN0b3JzLCBhbmQgcmV0dXJuIHRoZSBwcmluY2lwYWwgY29tcG9uZW50cyAodGhlIGVpZ2VudmVjdG9ycyBjb3JyZXNwb25kaW5nIHRvIHRoZSBsYXJnZXN0IGVpZ2VudmFsdWVzKS4gVGhlIGZ1bmN0aW9uIHNob3VsZCBhbHNvIHRha2UgYW4gaW50ZWdlciBrIGFzIGlucHV0LCByZXByZXNlbnRpbmcgdGhlIG51bWJlciBvZiBwcmluY2lwYWwgY29tcG9uZW50cyB0byByZXR1cm4u",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def pca_tg(self, data, k) -> np.ndarray:\n        \"\"\"\n        Perform PCA on `data`, returning the top `k` principal components as a tinygrad np.ndarray.\n        Input: list, NumPy array, or np.ndarray of shape (n_samples, n_features).\n        Returns: a np.ndarray of shape (n_features, k), with floats rounded to 4 decimals.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "data = np.array([[1, 2], [3, 4], [5, 6]]), k = 1",
    "example_output": "[[0.7071], [0.7071]]",
    "example_reasoning": "After standardizing the data and computing the covariance matrix, the eigenvalues and eigenvectors are calculated. The largest eigenvalue's corresponding eigenvector is returned as the principal component, rounded to four decimal places.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def pca_tg(self, data, k) -> np.ndarray:\n        \"\"\"\n        Perform PCA on `data`, returning the top `k` principal components as a tinygrad np.ndarray.\n        Input: list, NumPy array, or np.ndarray of shape (n_samples, n_features).\n        Returns: a np.ndarray of shape (n_features, k), with floats rounded to 4 decimals.\n        \"\"\"\n        arr = np.array(data, dtype=float)\n        # Standardize\n        arr_std = (arr - arr.mean(axis=0)) / arr.std(axis=0)\n        # Covariance\n        cov = np.cov(arr_std, rowvar=False)\n        # Eigen decomposition\n        vals, vecs = np.linalg.eig(cov)\n        idx = np.argsort(vals)[::-1]\n        pcs = vecs[:, idx[:k]]\n        pcs = np.round(pcs, 4)\n        return np.array(pcs)\n",
    "test_cases": [
      {
        "test": [
          [
            [
              4,
              2,
              1
            ],
            [
              5,
              6,
              7
            ],
            [
              9,
              12,
              1
            ],
            [
              4,
              6,
              7
            ]
          ],
          2
        ],
        "expected_output": [
          [
            0.6855,
            0.0776
          ],
          [
            0.6202,
            0.4586
          ],
          [
            -0.3814,
            0.8853
          ]
        ]
      },
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ],
            [
              5,
              6
            ]
          ]
        ],
        "expected_output": [
          [
            0.7071
          ],
          [
            0.7071
          ]
        ]
      }
    ],
    "function_name": "pca_tg"
  },
  {
    "id": 20,
    "title": "Decision Tree Learning",
    "difficulty": "hard",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBpbXBsZW1lbnRzIHRoZSBkZWNpc2lvbiB0cmVlIGxlYXJuaW5nIGFsZ29yaXRobSBmb3IgY2xhc3NpZmljYXRpb24uIFRoZSBmdW5jdGlvbiBzaG91bGQgdXNlIHJlY3Vyc2l2ZSBiaW5hcnkgc3BsaXR0aW5nIGJhc2VkIG9uIGVudHJvcHkgYW5kIGluZm9ybWF0aW9uIGdhaW4gdG8gYnVpbGQgYSBkZWNpc2lvbiB0cmVlLiBJdCBzaG91bGQgdGFrZSBhIGxpc3Qgb2YgZXhhbXBsZXMgKGVhY2ggZXhhbXBsZSBpcyBhIGRpY3Qgb2YgYXR0cmlidXRlLXZhbHVlIHBhaXJzKSBhbmQgYSBsaXN0IG9mIGF0dHJpYnV0ZSBuYW1lcyBhcyBpbnB1dCwgYW5kIHJldHVybiBhIG5lc3RlZCBkaWN0aW9uYXJ5IHJlcHJlc2VudGluZyB0aGUgZGVjaXNpb24gdHJlZS4=",
    "starter_code": "import math\nfrom collections import Counter\nfrom typing import List, Dict, Any, Union\n\nclass Solution:\n    def learn_decision_tree_tg(self, \n        examples: List[Dict[str, Any]],\n        attributes: List[str],\n        target_attr: str\n    ) -> Union[Dict[str, Any], Any]:\n        \"\"\"\n        Learn a decision tree using ID3 with tinygrad for entropy/gain.\n        Returns a nested dict tree or a class label.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "examples = [\n                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n                    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n                    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n                ],\n                attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']",
    "example_output": "{\n            'Outlook': {\n                'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}},\n                'Overcast': 'Yes',\n                'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}\n            }\n        }",
    "example_reasoning": "Using the given examples, the decision tree algorithm determines that 'Outlook' is the best attribute to split the data initially. When 'Outlook' is 'Overcast', the outcome is always 'Yes', so it becomes a leaf node. In cases of 'Sunny' and 'Rain', it further splits based on 'Humidity' and 'Wind', respectively. The resulting tree structure is able to classify the training examples with the attributes 'Outlook', 'Temperature', 'Humidity', and 'Wind'.",
    "learn_content": "CiMjIERlY2lzaW9uIFRyZWUgTGVhcm5pbmcgQWxnb3JpdGhtCgpUaGUgZGVjaXNpb24gdHJlZSBsZWFybmluZyBhbGdvcml0aG0gaXMgYSBtZXRob2QgdXNlZCBmb3IgY2xhc3NpZmljYXRpb24gdGhhdCBwcmVkaWN0cyB0aGUgdmFsdWUgb2YgYSB0YXJnZXQgdmFyaWFibGUgYmFzZWQgb24gc2V2ZXJhbCBpbnB1dCB2YXJpYWJsZXMuIEVhY2ggaW50ZXJuYWwgbm9kZSBvZiB0aGUgdHJlZSBjb3JyZXNwb25kcyB0byBhbiBpbnB1dCB2YXJpYWJsZSwgYW5kIGVhY2ggbGVhZiBub2RlIGNvcnJlc3BvbmRzIHRvIGEgY2xhc3MgbGFiZWwuCgojIyMgQWxnb3JpdGhtIE92ZXJ2aWV3ClRoZSByZWN1cnNpdmUgYmluYXJ5IHNwbGl0dGluZyBzdGFydHMgYnkgc2VsZWN0aW5nIHRoZSBhdHRyaWJ1dGUgdGhhdCBiZXN0IHNlcGFyYXRlcyB0aGUgZXhhbXBsZXMgYWNjb3JkaW5nIHRvIHRoZSBlbnRyb3B5IGFuZCBpbmZvcm1hdGlvbiBnYWluLCBjYWxjdWxhdGVkIGFzIGZvbGxvd3M6CgojIyMgRW50cm9weQokJApIKFgpID0gLVxzdW0gcCh4KSBcbG9nXzIgcCh4KQokJAoKIyMjIEluZm9ybWF0aW9uIEdhaW4KJCQKSUcoRCwgQSkgPSBIKEQpIC0gXHN1bSBcZnJhY3t8RF92fH17fER8fSBIKERfdikKJCQKCiMjIyBFeHBsYW5hdGlvbiBvZiBUZXJtcwotICoqRW50cm9weSBcKCBIKFgpIFwpKio6IE1lYXN1cmVzIHRoZSBpbXB1cml0eSBvciBkaXNvcmRlciBvZiB0aGUgc2V0LgotICoqSW5mb3JtYXRpb24gR2FpbiBcKCBJRyhELCBBKSBcKSoqOiBSZXByZXNlbnRzIHRoZSByZWR1Y3Rpb24gaW4gZW50cm9weSBhZnRlciBzcGxpdHRpbmcgdGhlIGRhdGFzZXQgXCggRCBcKSBvbiBhdHRyaWJ1dGUgXCggQSBcKS4KLSAqKlwoIERfdiBcKSoqOiBUaGUgc3Vic2V0IG9mIFwoIEQgXCkgZm9yIHdoaWNoIGF0dHJpYnV0ZSBcKCBBIFwpIGhhcyB2YWx1ZSBcKCB2IFwpLgoKIyMjIFByb2Nlc3MKMS4gKipTZWxlY3QgQXR0cmlidXRlKio6IENob29zZSB0aGUgYXR0cmlidXRlIHdpdGggdGhlIGhpZ2hlc3QgaW5mb3JtYXRpb24gZ2Fpbi4KMi4gKipTcGxpdCBEYXRhc2V0Kio6IERpdmlkZSB0aGUgZGF0YXNldCBiYXNlZCBvbiB0aGUgdmFsdWVzIG9mIHRoZSBzZWxlY3RlZCBhdHRyaWJ1dGUuCjMuICoqUmVjdXJzaW9uKio6IFJlcGVhdCB0aGUgcHJvY2VzcyBmb3IgZWFjaCBzdWJzZXQgdW50aWw6CiAgIC0gQWxsIGRhdGEgaXMgcGVyZmVjdGx5IGNsYXNzaWZpZWQsIG9yCiAgIC0gTm8gcmVtYWluaW5nIGF0dHJpYnV0ZXMgY2FuIGJlIHVzZWQgdG8gbWFrZSBhIHNwbGl0LgoKVGhpcyByZWN1cnNpdmUgcHJvY2VzcyBjb250aW51ZXMgdW50aWwgdGhlIGRlY2lzaW9uIHRyZWUgY2FuIG5vIGxvbmdlciBiZSBzcGxpdCBmdXJ0aGVyIG9yIGFsbCBleGFtcGxlcyBoYXZlIGJlZW4gY2xhc3NpZmllZC4KCg==",
    "solution_code": "import math\nfrom collections import Counter\nimport numpy as np\nfrom typing import List, Dict, Any, Union\n\nclass Solution:\n    def calculate_entropy_tg(self, labels) -> float:\n        arr = labels.tolist() if isinstance(labels, np.ndarray) else labels\n        total = len(arr)\n        cnt = Counter(arr)\n        return -sum((c/total)*math.log2(c/total) for c in cnt.values())\n\n\n    def calculate_information_gain_tg(self, \n        examples: List[Dict[str, Any]],\n        attr: str,\n        target_attr: str\n    ) -> float:\n        total = [ex[target_attr] for ex in examples]\n        total_ent = calculate_entropy_tg(total)\n        n = len(examples)\n        rem = 0.0\n        for v in set(ex[attr] for ex in examples):\n            subset = [ex[target_attr] for ex in examples if ex[attr] == v]\n            rem += (len(subset)/n) * calculate_entropy_tg(subset)\n        return total_ent - rem\n\n\n    def majority_class_tg(self, \n        examples: List[Dict[str, Any]],\n        target_attr: str\n    ) -> Any:\n        return Counter(ex[target_attr] for ex in examples).most_common(1)[0][0]\n\n\n    def learn_decision_tree_tg(self, \n        examples: List[Dict[str, Any]],\n        attributes: List[str],\n        target_attr: str\n    ) -> Union[Dict[str, Any], Any]:\n        if not examples:\n            return 'No examples'\n        first_label = examples[0][target_attr]\n        if all(ex[target_attr] == first_label for ex in examples):\n            return first_label\n        if not attributes:\n            return majority_class_tg(examples, target_attr)\n        gains = {a: calculate_information_gain_tg(examples, a, target_attr) for a in attributes}\n        best = max(gains, key=gains.get)\n        tree: Dict[str, Any] = {best: {}}\n        for v in set(ex[best] for ex in examples):\n            subset = [ex for ex in examples if ex[best] == v]\n            rem_attrs = [a for a in attributes if a != best]\n            tree[best][v] = learn_decision_tree_tg(subset, rem_attrs, target_attr)\n        return tree\n",
    "test_cases": [
      {
        "test": [
          "print(learn_decision_tree_tg([\n    {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'},\n    {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n], ['Outlook', 'Wind'], 'PlayTennis'))"
        ],
        "expected_output": {
          "Outlook": {
            "Sunny": {
              "Wind": {
                "Weak": "No",
                "Strong": "No"
              }
            },
            "Rain": {
              "Wind": {
                "Weak": "Yes",
                "Strong": "No"
              }
            },
            "Overcast": "Yes"
          }
        }
      }
    ],
    "function_name": "learn_decision_tree_tg"
  },
  {
    "id": 21,
    "title": "Pegasos Kernel SVM Implementation",
    "difficulty": "hard",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBpbXBsZW1lbnRzIGEgKipkZXRlcm1pbmlzdGljKiogdmVyc2lvbiBvZiB0aGUgUGVnYXNvcyBhbGdvcml0aG0gdG8gdHJhaW4gYSBrZXJuZWwgU1ZNIGNsYXNzaWZpZXIgZnJvbSBzY3JhdGNoLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBkYXRhc2V0IChhcyBhIDJEIE51bVB5IGFycmF5IHdoZXJlIGVhY2ggcm93IHJlcHJlc2VudHMgYSBkYXRhIHNhbXBsZSBhbmQgZWFjaCBjb2x1bW4gcmVwcmVzZW50cyBhIGZlYXR1cmUpLCBhIGxhYmVsIHZlY3RvciAoMUQgTnVtUHkgYXJyYXkgd2hlcmUgZWFjaCBlbnRyeSBjb3JyZXNwb25kcyB0byB0aGUgbGFiZWwgb2YgdGhlIHNhbXBsZSksIGFuZCB0cmFpbmluZyBwYXJhbWV0ZXJzIHN1Y2ggYXMgdGhlIGNob2ljZSBvZiBrZXJuZWwgKGxpbmVhciBvciBSQkYpLCByZWd1bGFyaXphdGlvbiBwYXJhbWV0ZXIgKGxhbWJkYSksIGFuZCB0aGUgbnVtYmVyIG9mIGl0ZXJhdGlvbnMuIE5vdGUgdGhhdCB3aGlsZSB0aGUgb3JpZ2luYWwgUGVnYXNvcyBhbGdvcml0aG0gaXMgc3RvY2hhc3RpYyAoaXQgc2VsZWN0cyBhIHNpbmdsZSByYW5kb20gc2FtcGxlIGF0IGVhY2ggc3RlcCksICoqdGhpcyBwcm9ibGVtIHJlcXVpcmVzIHVzaW5nIGFsbCBzYW1wbGVzIGluIGV2ZXJ5IGl0ZXJhdGlvbioqIChpLmUuLCAqKm5vIHJhbmRvbSBzYW1wbGluZyoqKS4gVGhlIGZ1bmN0aW9uIHNob3VsZCBwZXJmb3JtIGJpbmFyeSBjbGFzc2lmaWNhdGlvbiBhbmQgcmV0dXJuIHRoZSBtb2RlbCdzIGFscGhhIGNvZWZmaWNpZW50cyBhbmQgYmlhcy4=",
    "starter_code": "\n\nclass Solution:\n    def pegasos_kernel_svm(self, data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100,sigma=1.0) -> (list, float):\n    \t# Your code here\n    \treturn alphas, b",
    "example_input": "data = np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), labels = np.array([1, 1, -1, -1]), kernel = 'rbf', lambda_val = 0.01, iterations = 100, sigma = 1.0",
    "example_output": "alpha = [0.03, 0.02, 0.05, 0.01], b = -0.05",
    "example_reasoning": "Using the RBF kernel, the Pegasos algorithm iteratively updates the weights based on a sub-gradient descent method, taking into account the non-linear separability of the data induced by the kernel transformation.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def linear_kernel(self, x, y):\n        return np.dot(x, y)\n\n    def rbf_kernel(self, x, y, sigma=1.0):\n        return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n\n    def pegasos_kernel_svm(self, data, labels, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n        n_samples = len(data)\n        alphas = np.zeros(n_samples)\n        b = 0\n\n        for t in range(1, iterations + 1):\n            for i in range(n_samples):\n                eta = 1.0 / (lambda_val * t)\n                if kernel == 'linear':\n                    kernel_func = linear_kernel\n                elif kernel == 'rbf':\n                    kernel_func = lambda x, y: rbf_kernel(x, y, sigma)\n\n                decision = sum(alphas[j] * labels[j] * kernel_func(data[j], data[i]) for j in range(n_samples)) + b\n                if labels[i] * decision < 1:\n                    alphas[i] += eta * (labels[i] - lambda_val * alphas[i])\n                    b += eta * labels[i]\n\n        return np.round(alphas,4).tolist(), np.round(b,4)",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              2,
              3
            ],
            [
              3,
              1
            ],
            [
              4,
              1
            ]
          ],
          [
            1,
            1,
            -1,
            -1
          ]
        ],
        "expected_output": "([100.0, 0.0, -100.0, -100.0], -937.4755)"
      },
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              2,
              3
            ],
            [
              3,
              1
            ],
            [
              4,
              1
            ]
          ],
          [
            1,
            1,
            -1,
            -1
          ]
        ],
        "expected_output": "([100.0, 99.0, -100.0, -100.0], -115.0)"
      }
    ],
    "function_name": "pegasos_kernel_svm"
  },
  {
    "id": 22,
    "title": "Sigmoid Activation Function Understanding",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgb3V0cHV0IG9mIHRoZSBzaWdtb2lkIGFjdGl2YXRpb24gZnVuY3Rpb24gZ2l2ZW4gYW4gaW5wdXQgdmFsdWUgei4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gdGhlIG91dHB1dCByb3VuZGVkIHRvIGZvdXIgZGVjaW1hbCBwbGFjZXMu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def sigmoid_tg(self, z: float) -> float:\n        \"\"\"\n        Compute the sigmoid activation function using tinygrad.\n        Input:\n          - z: float or tinygrad np.ndarray scalar\n        Returns:\n          - sigmoid(z) as Python float rounded to 4 decimals.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "z = 0",
    "example_output": "0.5",
    "example_reasoning": "The sigmoid function is defined as \u03c3(z) = 1 / (1 + exp(-z)). For z = 0, exp(-0) = 1, hence the output is 1 / (1 + 1) = 0.5.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgdGhlIFNpZ21vaWQgQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIHNpZ21vaWQgYWN0aXZhdGlvbiBmdW5jdGlvbiBpcyBjcnVjaWFsIGluIG5ldXJhbCBuZXR3b3JrcywgZXNwZWNpYWxseSBmb3IgYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHRhc2tzLiBJdCBtYXBzIGFueSByZWFsLXZhbHVlZCBudW1iZXIgaW50byB0aGUgaW50ZXJ2YWwgXCggKDAsIDEpIFwpLCBtYWtpbmcgaXQgdXNlZnVsIGZvciBtb2RlbGluZyBwcm9iYWJpbGl0eSBhcyBhbiBvdXRwdXQuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KVGhlIHNpZ21vaWQgZnVuY3Rpb24gaXMgbWF0aGVtYXRpY2FsbHkgZGVmaW5lZCBhczoKJCQKXHNpZ21hKHopID0gXGZyYWN7MX17MSArIGVeey16fX0KJCQKd2hlcmUgXCggeiBcKSBpcyB0aGUgaW5wdXQgdG8gdGhlIGZ1bmN0aW9uLgoKIyMjIENoYXJhY3RlcmlzdGljcwotICoqT3V0cHV0IFJhbmdlKio6IFRoZSBvdXRwdXQgaXMgYWx3YXlzIGJldHdlZW4gMCBhbmQgMS4KLSAqKlNoYXBlKio6IFRoZSBmdW5jdGlvbiBoYXMgYW4gIlMiIHNoYXBlZCBjdXJ2ZS4KLSAqKkdyYWRpZW50Kio6IFRoZSBncmFkaWVudCBpcyBoaWdoZXN0IG5lYXIgXCggeiA9IDAgXCkgYW5kIGRlY3JlYXNlcyBhcyBcKCB6IFwpIG1vdmVzIGF3YXkgZnJvbSAwIGluIGVpdGhlciBkaXJlY3Rpb24uCgpUaGUgc2lnbW9pZCBmdW5jdGlvbiBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIGZvciB0dXJuaW5nIGxvZ2l0cyAocmF3IHByZWRpY3Rpb24gdmFsdWVzKSBpbnRvIHByb2JhYmlsaXRpZXMgaW4gYmluYXJ5IGNsYXNzaWZpY2F0aW9uIG1vZGVscy4KCg==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def sigmoid_tg(self, z: float) -> float:\n        \"\"\"\n        Compute the sigmoid activation function using tinygrad.\n        Input:\n          - z: float or tinygrad np.ndarray scalar\n        Returns:\n          - sigmoid(z) as Python float rounded to 4 decimals.\n        \"\"\"\n        t = np.array(z)\n        res = (np.array(1.0) / (np.array(1.0) + (-t).exp())).numpy().tolist()\n        return round(res, 4)\n",
    "test_cases": [
      {
        "test": [
          0
        ],
        "expected_output": 0.5
      },
      {
        "test": [
          1
        ],
        "expected_output": 0.7311
      }
    ],
    "function_name": "sigmoid_tg"
  },
  {
    "id": 23,
    "title": "Softmax Activation Function Implementation ",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgc29mdG1heCBhY3RpdmF0aW9uIGZvciBhIGdpdmVuIGxpc3Qgb2Ygc2NvcmVzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgc29mdG1heCB2YWx1ZXMgYXMgYSBsaXN0LCBlYWNoIHJvdW5kZWQgdG8gZm91ciBkZWNpbWFsIHBsYWNlcy4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def softmax_tg(self, scores: list[float]) -> list[float]:\n        \"\"\"\n        Compute the softmax activation function using tinygrad.\n        Input:\n          - scores: list of floats (logits)\n        Returns:\n          - list of floats representing the softmax probabilities,\n            each rounded to 4 decimals.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "scores = [1, 2, 3]",
    "example_output": "[0.0900, 0.2447, 0.6652]",
    "example_reasoning": "The softmax function converts a list of values into a probability distribution. The probabilities are proportional to the exponential of each element divided by the sum of the exponentials of all elements in the list.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgdGhlIFNvZnRtYXggQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIHNvZnRtYXggZnVuY3Rpb24gaXMgYSBnZW5lcmFsaXphdGlvbiBvZiB0aGUgc2lnbW9pZCBmdW5jdGlvbiBhbmQgaXMgdXNlZCBpbiB0aGUgb3V0cHV0IGxheWVyIG9mIGEgbmV1cmFsIG5ldHdvcmsgbW9kZWwgdGhhdCBoYW5kbGVzIG11bHRpLWNsYXNzIGNsYXNzaWZpY2F0aW9uIHRhc2tzLgoKIyMjIE1hdGhlbWF0aWNhbCBEZWZpbml0aW9uClRoZSBzb2Z0bWF4IGZ1bmN0aW9uIGlzIG1hdGhlbWF0aWNhbGx5IHJlcHJlc2VudGVkIGFzOgokJApcdGV4dHtzb2Z0bWF4fSh6X2kpID0gXGZyYWN7ZV57el9pfX17XHN1bV97an0gZV57el9qfX0KJCQKCiMjIyBDaGFyYWN0ZXJpc3RpY3MKLSAqKk91dHB1dCBSYW5nZSoqOiBFYWNoIG91dHB1dCB2YWx1ZSBpcyBiZXR3ZWVuIDAgYW5kIDEsIGFuZCB0aGUgc3VtIG9mIGFsbCBvdXRwdXRzIGlzIDEuCi0gKipQcm9iYWJpbGl0eSBEaXN0cmlidXRpb24qKjogSXQgdHJhbnNmb3JtcyBzY29yZXMgaW50byBwcm9iYWJpbGl0aWVzLCBtYWtpbmcgdGhlbSBlYXNpZXIgdG8gaW50ZXJwcmV0IGFuZCB1c2VmdWwgZm9yIGNsYXNzaWZpY2F0aW9uIHRhc2tzLgoKVGhlIHNvZnRtYXggZnVuY3Rpb24gaXMgZXNzZW50aWFsIGZvciBtb2RlbHMgd2hlcmUgdGhlIG91dHB1dCBuZWVkcyB0byByZXByZXNlbnQgYSBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb24gYWNyb3NzIG11bHRpcGxlIGNsYXNzZXMuCgo=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def softmax_tg(self, scores: list[float]) -> list[float]:\n        \"\"\"\n        Compute the softmax activation function using tinygrad.\n        Input:\n          - scores: list of floats (logits)\n        Returns:\n          - list of floats representing the softmax probabilities,\n            each rounded to 4 decimals.\n        \"\"\"\n        t = np.array(scores)\n        t_max = t.max()\n        exp_scores = (t - t_max).exp()\n        probs = exp_scores / exp_scores.sum()\n        probs_list = probs.numpy().tolist()\n        return [round(p, 4) for p in probs_list]\n",
    "test_cases": [
      {
        "test": [
          [
            1,
            2,
            3
          ]
        ],
        "expected_output": [
          0.09,
          0.2447,
          0.6652
        ]
      },
      {
        "test": [
          [
            1,
            1,
            1
          ]
        ],
        "expected_output": [
          0.3333,
          0.3333,
          0.3333
        ]
      }
    ],
    "function_name": "softmax_tg"
  },
  {
    "id": 24,
    "title": "Single Neuron",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBzaW11bGF0ZXMgYSBzaW5nbGUgbmV1cm9uIHdpdGggYSBzaWdtb2lkIGFjdGl2YXRpb24gZnVuY3Rpb24gZm9yIGJpbmFyeSBjbGFzc2lmaWNhdGlvbiwgaGFuZGxpbmcgbXVsdGlkaW1lbnNpb25hbCBpbnB1dCBmZWF0dXJlcy4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGEgbGlzdCBvZiBmZWF0dXJlIHZlY3RvcnMgKGVhY2ggdmVjdG9yIHJlcHJlc2VudGluZyBtdWx0aXBsZSBmZWF0dXJlcyBmb3IgYW4gZXhhbXBsZSksIGFzc29jaWF0ZWQgdHJ1ZSBiaW5hcnkgbGFiZWxzLCBhbmQgdGhlIG5ldXJvbidzIHdlaWdodHMgKG9uZSBmb3IgZWFjaCBmZWF0dXJlKSBhbmQgYmlhcyBhcyBpbnB1dC4gSXQgc2hvdWxkIHJldHVybiB0aGUgcHJlZGljdGVkIHByb2JhYmlsaXRpZXMgYWZ0ZXIgc2lnbW9pZCBhY3RpdmF0aW9uIGFuZCB0aGUgbWVhbiBzcXVhcmVkIGVycm9yIGJldHdlZW4gdGhlIHByZWRpY3RlZCBwcm9iYWJpbGl0aWVzIGFuZCB0aGUgdHJ1ZSBsYWJlbHMsIGJvdGggcm91bmRlZCB0byBmb3VyIGRlY2ltYWwgcGxhY2VzLg==",
    "starter_code": "import numpy as np\nfrom typing import List, Tuple\n\nclass Solution:\n    def single_neuron_model_tg(self, \n        features: List[List[float]],\n        labels: List[float],\n        weights: List[float],\n        bias: float\n    ) -> Tuple[List[float], float]:\n        \"\"\"\n        Compute output probabilities and MSE for a single neuron using tinygrad.\n        Uses built-in sigmoid and mean operations.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], labels = [0, 1, 0], weights = [0.7, -0.4], bias = -0.1",
    "example_output": "([0.4626, 0.4134, 0.6682], 0.3349)",
    "example_reasoning": "For each input vector, the weighted sum is calculated by multiplying each feature by its corresponding weight, adding these up along with the bias, then applying the sigmoid function to produce a probability. The MSE is calculated as the average squared difference between each predicted probability and the corresponding true label.",
    "learn_content": "CiMjIFNpbmdsZSBOZXVyb24gTW9kZWwgd2l0aCBNdWx0aWRpbWVuc2lvbmFsIElucHV0IGFuZCBTaWdtb2lkIEFjdGl2YXRpb24KClRoaXMgdGFzayBpbnZvbHZlcyBhIG5ldXJvbiBtb2RlbCBkZXNpZ25lZCBmb3IgYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHdpdGggbXVsdGlkaW1lbnNpb25hbCBpbnB1dCBmZWF0dXJlcywgdXNpbmcgdGhlIHNpZ21vaWQgYWN0aXZhdGlvbiBmdW5jdGlvbiB0byBvdXRwdXQgcHJvYmFiaWxpdGllcy4gSXQgYWxzbyBpbnZvbHZlcyBjYWxjdWxhdGluZyB0aGUgbWVhbiBzcXVhcmVkIGVycm9yIChNU0UpIHRvIGV2YWx1YXRlIHByZWRpY3Rpb24gYWNjdXJhY3kuCgojIyMgTWF0aGVtYXRpY2FsIEJhY2tncm91bmQKCioqTmV1cm9uIE91dHB1dCBDYWxjdWxhdGlvbjoqKgokJAp6ID0gXHN1bSAod2VpZ2h0X2kgXHRpbWVzIGZlYXR1cmVfaSkgKyBiaWFzCiQkCiQkClxzaWdtYSh6KSA9IFxmcmFjezF9ezEgKyBlXnsten19CiQkCgoqKk1TRSBDYWxjdWxhdGlvbjoqKgokJApNU0UgPSBcZnJhY3sxfXtufSBcc3VtIChwcmVkaWN0ZWQgLSB0cnVlKV4yCiQkCgojIyMgRXhwbGFuYXRpb24gb2YgVGVybXMKLSAqKlwoIHogXCkqKjogVGhlIHN1bSBvZiB3ZWlnaHRlZCBpbnB1dHMgcGx1cyBiaWFzLgotICoqXCggXHNpZ21hKHopIFwpKio6IFRoZSBzaWdtb2lkIGFjdGl2YXRpb24gb3V0cHV0LgotICoqXCggcHJlZGljdGVkIFwpKio6IFRoZSBwcm9iYWJpbGl0aWVzIGFmdGVyIHNpZ21vaWQgYWN0aXZhdGlvbi4KLSAqKlwoIHRydWUgXCkqKjogVGhlIHRydWUgYmluYXJ5IGxhYmVscy4KCiMjIyBQcmFjdGljYWwgSW1wbGVtZW50YXRpb24KLSBFYWNoIGZlYXR1cmUgdmVjdG9yIGlzIHByb2Nlc3NlZCB0byBjYWxjdWxhdGUgYSBjb21iaW5lZCB3ZWlnaHRlZCBzdW0sIHdoaWNoIGlzIHRoZW4gcGFzc2VkIHRocm91Z2ggdGhlIHNpZ21vaWQgZnVuY3Rpb24gdG8gZGV0ZXJtaW5lIHRoZSBwcm9iYWJpbGl0eSBvZiB0aGUgaW5wdXQgYmVsb25naW5nIHRvIHRoZSBwb3NpdGl2ZSBjbGFzcy4KLSAqKk1TRSoqIHByb3ZpZGVzIGEgbWVhc3VyZSBvZiBlcnJvciwgb2ZmZXJpbmcgaW5zaWdodHMgaW50byB0aGUgbW9kZWwncyBwZXJmb3JtYW5jZSBhbmQgYWlkaW5nIGluIGl0cyBvcHRpbWl6YXRpb24uCgo=",
    "solution_code": "import numpy as np\nfrom typing import List, Tuple\n\nclass Solution:\n    def single_neuron_model_tg(self, \n        features: List[List[float]],\n        labels: List[float],\n        weights: List[float],\n        bias: float\n    ) -> Tuple[List[float], float]:\n        X = np.array(features)\n        w = np.array(weights)\n        b = bias\n        probs: List[float] = []\n        for i in range(len(features)):\n            z = X[i].dot(w) + b\n            p = z.sigmoid().numpy().tolist()\n            probs.append(round(p, 4))\n\n        mse = sum((p - y)**2 for p, y in zip(probs, labels)) / len(labels)\n        mse = round(mse, 4)\n\n        return probs, mse\n",
    "test_cases": [
      {
        "test": [
          [
            [
              0.5,
              1.0
            ],
            [
              -1.5,
              -2.0
            ],
            [
              2.0,
              1.5
            ]
          ],
          [
            0,
            1,
            0
          ],
          [
            0.7,
            -0.4
          ],
          -0.1
        ],
        "expected_output": "([0.4626, 0.4134, 0.6682], 0.3349)"
      },
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              2,
              3
            ],
            [
              3,
              1
            ]
          ],
          [
            1,
            0,
            1
          ],
          [
            0.5,
            -0.2
          ],
          0
        ],
        "expected_output": "([0.525, 0.5987, 0.7858], 0.21)"
      }
    ],
    "function_name": "single_neuron_model_tg"
  },
  {
    "id": 25,
    "title": "Single Neuron with Backpropagation",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBzaW11bGF0ZXMgYSBzaW5nbGUgbmV1cm9uIHdpdGggc2lnbW9pZCBhY3RpdmF0aW9uLCBhbmQgaW1wbGVtZW50cyBiYWNrcHJvcGFnYXRpb24gdG8gdXBkYXRlIHRoZSBuZXVyb24ncyB3ZWlnaHRzIGFuZCBiaWFzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBsaXN0IG9mIGZlYXR1cmUgdmVjdG9ycywgYXNzb2NpYXRlZCB0cnVlIGJpbmFyeSBsYWJlbHMsIGluaXRpYWwgd2VpZ2h0cywgaW5pdGlhbCBiaWFzLCBhIGxlYXJuaW5nIHJhdGUsIGFuZCB0aGUgbnVtYmVyIG9mIGVwb2Nocy4gVGhlIGZ1bmN0aW9uIHNob3VsZCB1cGRhdGUgdGhlIHdlaWdodHMgYW5kIGJpYXMgdXNpbmcgZ3JhZGllbnQgZGVzY2VudCBiYXNlZCBvbiB0aGUgTVNFIGxvc3MsIGFuZCByZXR1cm4gdGhlIHVwZGF0ZWQgd2VpZ2h0cywgYmlhcywgYW5kIGEgbGlzdCBvZiBNU0UgdmFsdWVzIGZvciBlYWNoIGVwb2NoLCBlYWNoIHJvdW5kZWQgdG8gZm91ciBkZWNpbWFsIHBsYWNlcy4=",
    "starter_code": "import numpy as np\nfrom typing import List, Tuple\n\nclass Solution:\n    def train_neuron_tg(self, \n        features: List[List[float]],\n        labels:   List[float],\n        initial_weights: List[float],\n        initial_bias: float,\n        learning_rate: float,\n        epochs: int\n    ) -> Tuple[List[float], float, List[float]]:\n        \"\"\"\n        Tinygrad version \u2014 same contract as PyTorch implementation.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], labels = [1, 0, 0], initial_weights = [0.1, -0.2], initial_bias = 0.0, learning_rate = 0.1, epochs = 2",
    "example_output": "updated_weights = [0.1036, -0.1425], updated_bias = -0.0167, mse_values = [0.3033, 0.2942]",
    "example_reasoning": "The neuron receives feature vectors and computes predictions using the sigmoid activation. Based on the predictions and true labels, the gradients of MSE loss with respect to weights and bias are computed and used to update the model parameters across epochs.",
    "learn_content": "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",
    "solution_code": "import numpy as np\nfrom typing import List, Tuple\n\nclass Solution:\n    def train_neuron_tg(self, \n        features: List[List[float]],\n        labels:   List[float],\n        initial_weights: List[float],\n        initial_bias: float,\n        learning_rate: float,\n        epochs: int\n    ) -> Tuple[List[float], float, List[float]]:\n        X = np.array(features)\n        y = np.array(labels).reshape(len(labels), 1)\n        w = np.array(initial_weights).reshape(len(initial_weights), 1)\n        b = np.array(initial_bias)\n\n        mse_values: List[float] = []\n        n = len(labels)\n\n        for _ in range(epochs):\n            z = X.matmul(w) + b         # (n,1)\n            preds = z.sigmoid()         # (n,1)\n            errors = preds - y          # (n,1)\n\n            mse = float(((errors**2).mean()).numpy())\n            mse_values.append(round(mse, 4))\n\n            sigma_prime = preds * (1 - preds)\n            delta = (2.0 / n) * errors * sigma_prime  # (n,1)\n\n            grad_w = X.T.matmul(delta)  # (f,1)\n            grad_b = delta.sum()\n\n            w -= np.array(learning_rate) * grad_w\n            b -= np.array(learning_rate) * grad_b\n\n        updated_weights = [round(val, 4) for val in w.numpy().flatten().tolist()]\n        updated_bias    = round(float(b.numpy()), 4)\n        return updated_weights, updated_bias, mse_values\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1.0,
              2.0
            ],
            [
              2.0,
              1.0
            ],
            [
              -1.0,
              -2.0
            ]
          ],
          [
            1,
            0,
            0
          ],
          [
            0.1,
            -0.2
          ],
          0.0,
          0.1,
          2
        ],
        "expected_output": "([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])"
      },
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              2,
              3
            ],
            [
              3,
              1
            ]
          ],
          [
            1,
            0,
            1
          ],
          [
            0.5,
            -0.2
          ],
          0,
          0.1,
          3
        ],
        "expected_output": "([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])"
      }
    ],
    "function_name": "train_neuron_tg"
  },
  {
    "id": 26,
    "title": "Implementing Basic Autograd Operations",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "U3BlY2lhbCB0aGFua3MgdG8gQW5kcmVqIEthcnBhdGh5IGZvciBtYWtpbmcgYSB2aWRlbyBhYm91dCB0aGlzLCBpZiB5b3UgaGF2ZW4ndCBhbHJlYWR5IGNoZWNrIG91dCBoaXMgdmlkZW9zIG9uIFlvdVR1YmUgaHR0cHM6Ly95b3V0dS5iZS9WTWotM1MxdGt1MD9zaT1namxuRlA0bzNKUk45ZFRnLiBXcml0ZSBhIFB5dGhvbiBjbGFzcyBzaW1pbGFyIHRvIHRoZSBwcm92aWRlZCAnVmFsdWUnIGNsYXNzIHRoYXQgaW1wbGVtZW50cyB0aGUgYmFzaWMgYXV0b2dyYWQgb3BlcmF0aW9uczogYWRkaXRpb24sIG11bHRpcGxpY2F0aW9uLCBhbmQgUmVMVSBhY3RpdmF0aW9uLiBUaGUgY2xhc3Mgc2hvdWxkIGhhbmRsZSBzY2FsYXIgdmFsdWVzIGFuZCBzaG91bGQgY29ycmVjdGx5IGNvbXB1dGUgZ3JhZGllbnRzIGZvciB0aGVzZSBvcGVyYXRpb25zIHRocm91Z2ggYXV0b21hdGljIGRpZmZlcmVudGlhdGlvbi4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    class Value:\n    \"\"\"Same idea, but using tinygrad\u2019s automatic differentiation.\"\"\"\n\n    def __init__(self, self, data, _tensor=None):\n        self._t = _tensor if _tensor is not None else np.array(float(data), requires_grad=True)\n\n    @property\n    def data(self):\n        return float(self._t.numpy())\n\n    @property\n    def grad(self):\n        g = self._t.grad\n        return 0 if g is None else float(g.numpy())\n\n    def __repr__(self):\n        def fmt(self, x):\n            return int(x) if float(x).is_integer() else round(x, 4)\n        return f\"Value(data={fmt(self.data)}, grad={fmt(self.grad)})\"\n\n    def _wrap(self, self, other):\n        return other if isinstance(other, Value) else Value(other)\n\n    def __add__(self, self, other):\n        other = self._wrap(other)\n        return Value(0, _tensor=self._t + other._t)\n\n    __radd__ = __add__\n\n    def __mul__(self, self, other):\n        other = self._wrap(other)\n        return Value(0, _tensor=self._t * other._t)\n\n    __rmul__ = __mul__\n\n    def relu(self):\n        return Value(0, _tensor=self._t.relu())\n\n    def backward(self):\n        self._t.backward()\n",
    "example_input": "a = Value(2)\n        b = Value(-3)\n        c = Value(10)\n        d = a + b * c\n        e = d.relu()\n        e.backward()\n        print(a, b, c, d, e)",
    "example_output": "Value(data=2, grad=0) Value(data=-3, grad=0) Value(data=10, grad=0)",
    "example_reasoning": "The output reflects the forward computation and gradients after backpropagation. The ReLU on 'd' zeros out its output and gradient due to the negative data value.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    class Value:\n    def __init__(self, self, data, _tensor=None):\n        self._t = _tensor if _tensor is not None else np.array(float(data), requires_grad=True)\n\n    @property\n    def data(self):\n        return float(self._t.numpy())\n\n    @property\n    def grad(self):\n        g = self._t.grad\n        return 0 if g is None else float(g.numpy())\n\n    def __repr__(self):\n        def fmt(self, x):\n            return int(x) if float(x).is_integer() else round(x, 4)\n        return f\"Value(data={fmt(self.data)}, grad={fmt(self.grad)})\"\n\n    def _wrap(self, self, other):\n        return other if isinstance(other, Value) else Value(other)\n\n    def __add__(self, self, other):\n        other = self._wrap(other)\n        return Value(0, _tensor=self._t + other._t)\n\n    __radd__ = __add__\n\n    def __mul__(self, self, other):\n        other = self._wrap(other)\n        return Value(0, _tensor=self._t * other._t)\n\n    __rmul__ = __mul__\n\n    def relu(self):\n        return Value(0, _tensor=self._t.relu())\n\n    def backward(self):\n        self._t.backward()\n",
    "test_cases": [
      {
        "test": [
          "a = Value(2);b = Value(3);c = Value(10);d = a + b * c  ;e = Value(7) * Value(2);f = e + d;g = f.relu()  \ng.backward()\nprint(a,b,c,d,e,f,g)"
        ],
        "expected_output": " Value(data=2, grad=1) Value(data=3, grad=10) Value(data=10, grad=3) Value(data=32, grad=1) Value(data=14, grad=1) Value(data=46, grad=1) Value(data=46, grad=1)"
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 27,
    "title": "Transformation Matrix from Basis B to C",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "R2l2ZW4gYmFzaXMgdmVjdG9ycyBpbiB0d28gZGlmZmVyZW50IGJhc2VzIEIgYW5kIEMgZm9yIFJeMywgd3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgdHJhbnNmb3JtYXRpb24gbWF0cml4IFAgZnJvbSBiYXNpcyBCIHRvIEMu",
    "starter_code": "import torch\nfrom typing import List\n\nclass Solution:\n    def transform_basis(self, B: List[List[float]], C: List[List[float]]) -> List[List[float]]:\n        \"\"\"Return the change-of-basis matrix **P = C\u207b\u00b9 B**.\n\n        - *B*, *C* may be 2\u00d72 or 3\u00d73 nested lists.\n        - Result is rounded to 4 decimals and returned as a nested list.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "B = [[1, 0, 0], \n             [0, 1, 0], \n             [0, 0, 1]]\n        C = [[1, 2.3, 3], \n             [4.4, 25, 6], \n             [7.4, 8, 9]]",
    "example_output": "[[-0.6772, -0.0126, 0.2342],\n                [-0.0184, 0.0505, -0.0275],\n                [0.5732, -0.0345, -0.0569]]",
    "example_reasoning": "The transformation matrix P from basis B to C can be found using matrix operations involving the inverse of matrix C.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgVHJhbnNmb3JtYXRpb24gTWF0cmljZXMKCkEgdHJhbnNmb3JtYXRpb24gbWF0cml4IGFsbG93cyB1cyB0byBjb252ZXJ0IHRoZSBjb29yZGluYXRlcyBvZiBhIHZlY3RvciBpbiBvbmUgYmFzaXMgdG8gY29vcmRpbmF0ZXMgaW4gYW5vdGhlciBiYXNpcy4gRm9yIGJhc2VzIFwoIEIgXCkgYW5kIFwoIEMgXCkgb2YgYSB2ZWN0b3Igc3BhY2UsIHRoZSB0cmFuc2Zvcm1hdGlvbiBtYXRyaXggXCggUCBcKSBmcm9tIFwoIEIgXCkgdG8gXCggQyBcKSBpcyBjYWxjdWxhdGVkIGFzIGZvbGxvd3M6CgojIyMgU3RlcHMgdG8gQ2FsY3VsYXRlIHRoZSBUcmFuc2Zvcm1hdGlvbiBNYXRyaXgKMS4gKipJbnZlcnNlIG9mIEJhc2lzIFwoIEMgXCkqKjogRmlyc3QsIGZpbmQgdGhlIGludmVyc2Ugb2YgdGhlIG1hdHJpeCByZXByZXNlbnRpbmcgYmFzaXMgXCggQyBcKSwgZGVub3RlZCBcKCBDXnstMX0gXCkuCjIuICoqTWF0cml4IE11bHRpcGxpY2F0aW9uKio6IE11bHRpcGx5IFwoIENeey0xfSBcKSBieSB0aGUgbWF0cml4IG9mIGJhc2lzIFwoIEIgXCkuIFRoZSByZXN1bHQgaXMgdGhlIHRyYW5zZm9ybWF0aW9uIG1hdHJpeDoKICAgJCQKICAgUCA9IENeey0xfSBcY2RvdCBCCiAgICQkCgpUaGlzIG1hdHJpeCBcKCBQIFwpIGNhbiBiZSB1c2VkIHRvIHRyYW5zZm9ybSBhbnkgdmVjdG9yIGNvb3JkaW5hdGVzIGZyb20gdGhlIFwoIEIgXCkgYmFzaXMgdG8gdGhlIFwoIEMgXCkgYmFzaXMuCgo=",
    "solution_code": "import torch\nfrom typing import List\n\nclass Solution:\n    def transform_basis(self, B: List[List[float]], C: List[List[float]]) -> List[List[float]]:\n        B_t = torch.tensor(B, dtype=torch.float32)\n        C_t = torch.tensor(C, dtype=torch.float32)\n\n        P = torch.linalg.inv(C_t) @ B_t           # C\u207b\u00b9 \u00b7 B\n        P = torch.round(P * 1e4) / 1e4            # 4-decimal rounding\n        return P.tolist()\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              0,
              0
            ],
            [
              0,
              1,
              0
            ],
            [
              0,
              0,
              1
            ]
          ],
          [
            [
              1,
              2.3,
              3
            ],
            [
              4.4,
              25,
              6
            ],
            [
              7.4,
              8,
              9
            ]
          ]
        ],
        "expected_output": [
          [
            -0.6772,
            -0.0126,
            0.2342
          ],
          [
            -0.0184,
            0.0505,
            -0.0275
          ],
          [
            0.5732,
            -0.0345,
            -0.0569
          ]
        ]
      },
      {
        "test": [
          [
            [
              1,
              0
            ],
            [
              0,
              1
            ]
          ],
          [
            [
              1,
              2
            ],
            [
              9,
              2
            ]
          ]
        ],
        "expected_output": [
          [
            -0.125,
            0.125
          ],
          [
            0.5625,
            -0.0625
          ]
        ]
      }
    ],
    "function_name": "transform_basis"
  },
  {
    "id": 28,
    "title": "SVD of a 2x2 Matrix using eigen values & vectors",
    "difficulty": "hard",
    "category": "linear algebra",
    "problem_description": "R2l2ZW4gYSAyeDIgbWF0cml4LCB3cml0ZSBhIFB5dGhvbiBmdW5jdGlvbiB0byBjb21wdXRlIGl0cyBTaW5ndWxhciBWYWx1ZSBEZWNvbXBvc2l0aW9uIChTVkQpLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgbWF0cmljZXMgVSwgUywgYW5kIFYgc3VjaCB0aGF0IEEgPSBVICogUyAqIFYsIHVzZSB0aGUgbWV0aG9kIGRlc2NyaWJlZCBpbiB0aGlzIHBvc3QgaHR0cHM6Ly9tZXRhbWVyaXN0LmJsb2dzcG90LmNvbS8yMDA2LzEwL2xpbmVhci1hbGdlYnJhLWZvci1ncmFwaGljcy1nZWVrcy1zdmQuaHRtbA==",
    "starter_code": "import torch\nfrom typing import Tuple\n\nclass Solution:\n    def svd_2x2(self, A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Singular-value decomposition (SVD) of a real 2 x 2 matrix.\n\n        Parameters\n        ----------\n        A : torch.Tensor, shape (2, 2)\n            Input matrix.\n\n        Returns\n        -------\n        U : torch.Tensor, shape (2, 2) \u2013 left-singular vectors (orthogonal)\n        s : torch.Tensor, shape (2,)   \u2013 singular values (non-negative, sorted \u2193)\n        V : torch.Tensor, shape (2, 2) \u2013 right-singular vectors\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "A = [[-10, 8], \n         [10, -1]]",
    "example_output": "(array([[  0.8, -0.6], [-0.6, -0.8]]), \n    array([15.65247584,  4.47213595]), \n    array([[ -0.89442719,  0.4472136], [ -0.4472136 , -0.89442719]]))",
    "example_reasoning": "The SVD of the matrix A is calculated using the eigenvalues and eigenvectors of A^T A and A A^T. The singular values are the square roots of the eigenvalues, and the eigenvectors form the columns of matrices U and V.",
    "learn_content": "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",
    "solution_code": "import torch\nfrom typing import Tuple\n\nclass Solution:\n    def svd_2x2(self, A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        A = A.to(dtype=torch.float64)                              # use double for accuracy\n        U, s, Vh = torch.linalg.svd(A, full_matrices=False)        # Vh = V\u1d40\n        V = Vh.T\n        # round to 4 decimals for deterministic tests\n        U = torch.round(U * 1e4) / 1e4\n        s = torch.round(s * 1e4) / 1e4\n        V = torch.round(V * 1e4) / 1e4\n        return U, s, V\n",
    "test_cases": [
      {
        "test": [
          "U,s,V = svd_2x2(np.array(torch.tensor([[-10, 8], [10, -1]])))\nresult = U @ np.diag(s) @ V\nprint(result)"
        ],
        "expected_output": [
          [
            -10,
            8
          ],
          [
            10,
            -1
          ]
        ]
      },
      {
        "test": [
          "U,s,V = svd_2x2(np.array(torch.tensor([[1, 2], [3, 4]])))\nresult = U @ np.diag(s) @ V\nprint(result)"
        ],
        "expected_output": [
          [
            1,
            2
          ],
          [
            3,
            4
          ]
        ]
      }
    ],
    "function_name": "svd_2x2"
  },
  {
    "id": 29,
    "title": "Random Shuffle of Dataset",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gcGVyZm9ybSBhIHJhbmRvbSBzaHVmZmxlIG9mIHRoZSBzYW1wbGVzIGluIHR3byBudW1weSBhcnJheXMsIFggYW5kIHksIHdoaWxlIG1haW50YWluaW5nIHRoZSBjb3JyZXNwb25kaW5nIG9yZGVyIGJldHdlZW4gdGhlbS4gVGhlIGZ1bmN0aW9uIHNob3VsZCBoYXZlIGFuIG9wdGlvbmFsIHNlZWQgcGFyYW1ldGVyIGZvciByZXByb2R1Y2liaWxpdHku",
    "starter_code": "import torch\nfrom typing import Tuple\n\nclass Solution:\n    def shuffle_data(self, X, y, seed=None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Randomly shuffle X and y together, maintaining the correspondence between samples.\n        \"\"\"\n        pass\n",
    "example_input": "X = np.array([[1, 2], \n                  [3, 4], \n                  [5, 6], \n                  [7, 8]])\n    y = np.array([1, 2, 3, 4])",
    "example_output": "(array([[5, 6],\n                    [1, 2],\n                    [7, 8],\n                    [3, 4]]), \n             array([3, 1, 4, 2]))",
    "example_reasoning": "The samples in X and y are shuffled randomly, maintaining the correspondence between the samples in both arrays.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgRGF0YXNldCBTaHVmZmxpbmcKClJhbmRvbSBzaHVmZmxpbmcgb2YgYSBkYXRhc2V0IGlzIGEgY29tbW9uIHByZXByb2Nlc3Npbmcgc3RlcCBpbiBtYWNoaW5lIGxlYXJuaW5nIHRvIGVuc3VyZSB0aGF0IHRoZSBkYXRhIGlzIHJhbmRvbWx5IGRpc3RyaWJ1dGVkIGJlZm9yZSB0cmFpbmluZyBhIG1vZGVsLiBUaGlzIGhlbHBzIHRvIGF2b2lkIGFueSBwb3RlbnRpYWwgYmlhc2VzIHRoYXQgbWF5IGFyaXNlIGZyb20gdGhlIG9yZGVyIGluIHdoaWNoIGRhdGEgaXMgcHJlc2VudGVkIHRvIHRoZSBtb2RlbC4KCiMjIyBTdGVwLWJ5LVN0ZXAgTWV0aG9kIHRvIFNodWZmbGUgYSBEYXRhc2V0CgoxLiAqKkdlbmVyYXRlIGEgUmFuZG9tIEluZGV4IEFycmF5KiogIAogICBDcmVhdGUgYW4gYXJyYXkgb2YgaW5kaWNlcyBjb3JyZXNwb25kaW5nIHRvIHRoZSBudW1iZXIgb2Ygc2FtcGxlcyBpbiB0aGUgZGF0YXNldC4KCjIuICoqU2h1ZmZsZSB0aGUgSW5kaWNlcyoqICAKICAgVXNlIGEgcmFuZG9tIG51bWJlciBnZW5lcmF0b3IgdG8gc2h1ZmZsZSB0aGUgYXJyYXkgb2YgaW5kaWNlcy4KCjMuICoqUmVvcmRlciB0aGUgRGF0YXNldCoqICAKICAgVXNlIHRoZSBzaHVmZmxlZCBpbmRpY2VzIHRvIHJlb3JkZXIgdGhlIHNhbXBsZXMgaW4gYm90aCBcKCBYIFwpIGFuZCBcKCB5IFwpLgoKIyMjIEtleSBQb2ludApUaGlzIG1ldGhvZCBlbnN1cmVzIHRoYXQgdGhlIGNvcnJlc3BvbmRlbmNlIGJldHdlZW4gXCggWCBcKSBhbmQgXCggeSBcKSBpcyBtYWludGFpbmVkIGFmdGVyIHNodWZmbGluZywgcHJlc2VydmluZyB0aGUgcmVsYXRpb25zaGlwIGJldHdlZW4gZmVhdHVyZXMgYW5kIGxhYmVscy4KCg==",
    "solution_code": "import torch\nimport numpy as np\nfrom typing import Tuple\n\nclass Solution:\n    def shuffle_data(self, X, y, seed=None) -> Tuple[torch.Tensor, torch.Tensor]:\n        X_t = torch.as_tensor(X)\n        y_t = torch.as_tensor(y)\n        if seed is not None:\n            rng = np.random.default_rng(seed)\n            idx = torch.tensor(rng.permutation(X_t.shape[0]), dtype=torch.long)\n        else:\n            idx = torch.randperm(X_t.shape[0])\n        return X_t[idx], y_t[idx]\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ],
            [
              5,
              6
            ],
            [
              7,
              8
            ]
          ],
          [
            1,
            2,
            3,
            4
          ]
        ],
        "expected_output": "(array([[3, 4], [7, 8], [1, 2], [5, 6]]), array([2, 4, 1, 3]))"
      },
      {
        "test": [
          [
            [
              1,
              1
            ],
            [
              2,
              2
            ],
            [
              3,
              3
            ],
            [
              4,
              4
            ]
          ],
          [
            10,
            20,
            30,
            40
          ]
        ],
        "expected_output": "(array([[4, 4],[2, 2],[1, 1],[3, 3]]), array([40, 20, 10, 30]))"
      }
    ],
    "function_name": "shuffle_data"
  },
  {
    "id": 30,
    "title": "Batch Iterator for Dataset",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgYmF0Y2ggaXRlcmFibGUgZnVuY3Rpb24gdGhhdCBzYW1wbGVzIGluIGEgbnVtcHkgYXJyYXkgWCBhbmQgYW4gb3B0aW9uYWwgbnVtcHkgYXJyYXkgeS4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gYmF0Y2hlcyBvZiBhIHNwZWNpZmllZCBzaXplLiBJZiB5IGlzIHByb3ZpZGVkLCB0aGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiBiYXRjaGVzIG9mIChYLCB5KSBwYWlyczsgb3RoZXJ3aXNlLCBpdCBzaG91bGQgcmV0dXJuIGJhdGNoZXMgb2YgWCBvbmx5Lg==",
    "starter_code": "import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom typing import Tuple\n\nclass Solution:\n    def shuffle_data(self, X, y, seed=None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Use TensorDataset and DataLoader to randomly shuffle X and y together,\n        maintaining the correspondence between samples.\n        \"\"\"\n        # Hint: Use TensorDataset and DataLoader with shuffle=True.\n        pass\n",
    "example_input": "X = np.array([[1, 2], \n                  [3, 4], \n                  [5, 6], \n                  [7, 8], \n                  [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    batch_size = 2\n    batch_iterator(X, y, batch_size)",
    "example_output": "[[[[1, 2], [3, 4]], [1, 2]],\n     [[[5, 6], [7, 8]], [3, 4]],\n     [[[9, 10]], [5]]]",
    "example_reasoning": "The dataset X contains 5 samples, and we are using a batch size of 2. Therefore, the function will divide the dataset into 3 batches. The first two batches will contain 2 samples each, and the last batch will contain the remaining sample. The corresponding values from y are also included in each batch.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgQmF0Y2ggSXRlcmF0aW9uCgpCYXRjaCBpdGVyYXRpb24gaXMgYSBjb21tb24gdGVjaG5pcXVlIHVzZWQgaW4gbWFjaGluZSBsZWFybmluZyBhbmQgZGF0YSBwcm9jZXNzaW5nIHRvIGhhbmRsZSBsYXJnZSBkYXRhc2V0cyBtb3JlIGVmZmljaWVudGx5LiBJbnN0ZWFkIG9mIHByb2Nlc3NpbmcgdGhlIGVudGlyZSBkYXRhc2V0IGF0IG9uY2UsIHdoaWNoIGNhbiBiZSBtZW1vcnktaW50ZW5zaXZlLCBkYXRhIGlzIHByb2Nlc3NlZCBpbiBzbWFsbGVyLCBtb3JlIG1hbmFnZWFibGUgYmF0Y2hlcy4KCiMjIyBTdGVwLWJ5LVN0ZXAgTWV0aG9kIHRvIENyZWF0ZSBhIEJhdGNoIEl0ZXJhdG9yCgoxLiAqKkRldGVybWluZSB0aGUgTnVtYmVyIG9mIFNhbXBsZXMqKiAgCiAgIENhbGN1bGF0ZSB0aGUgdG90YWwgbnVtYmVyIG9mIHNhbXBsZXMgaW4gdGhlIGRhdGFzZXQuCgoyLiAqKkl0ZXJhdGUgaW4gQmF0Y2hlcyoqICAKICAgTG9vcCB0aHJvdWdoIHRoZSBkYXRhc2V0IGluIGluY3JlbWVudHMgb2YgdGhlIHNwZWNpZmllZCBiYXRjaCBzaXplLgoKMy4gKipZaWVsZCBCYXRjaGVzKiogIAogICBGb3IgZWFjaCBpdGVyYXRpb24sIHlpZWxkIGEgYmF0Y2ggb2Ygc2FtcGxlcyBmcm9tIFwoIFggXCkgYW5kLCBpZiBwcm92aWRlZCwgdGhlIGNvcnJlc3BvbmRpbmcgc2FtcGxlcyBmcm9tIFwoIHkgXCkuCgojIyMgS2V5IFBvaW50ClRoaXMgbWV0aG9kIGVuc3VyZXMgZWZmaWNpZW50IHByb2Nlc3NpbmcgYW5kIGNhbiBiZSB1c2VkIGZvciBib3RoIHRoZSB0cmFpbmluZyBhbmQgZXZhbHVhdGlvbiBwaGFzZXMgaW4gbWFjaGluZSBsZWFybmluZyB3b3JrZmxvd3MuCgo=",
    "solution_code": "import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom typing import Tuple\n\nclass Solution:\n    def shuffle_data(self, X, y, seed=None) -> Tuple[torch.Tensor, torch.Tensor]:\n        X_t = torch.as_tensor(X)\n        y_t = torch.as_tensor(y)\n        dataset = TensorDataset(X_t, y_t)\n        generator = torch.Generator()\n        if seed is not None:\n            generator.manual_seed(seed)\n        loader = DataLoader(dataset, batch_size=len(dataset), shuffle=True, generator=generator)\n        Xs, ys = next(iter(loader))\n        return Xs, ys\n",
    "test_cases": [
      {
        "test": [
          "print(batch_iterator(np.array(torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])), np.array(torch.tensor([1, 2, 3, 4, 5])), batch_size=2))"
        ],
        "expected_output": [
          [
            [
              [
                1,
                2
              ],
              [
                3,
                4
              ]
            ],
            [
              1,
              2
            ]
          ],
          [
            [
              [
                5,
                6
              ],
              [
                7,
                8
              ]
            ],
            [
              3,
              4
            ]
          ],
          [
            [
              [
                9,
                10
              ]
            ],
            [
              5
            ]
          ]
        ]
      },
      {
        "test": [
          "print(batch_iterator(np.array(torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4]])), batch_size=3))"
        ],
        "expected_output": [
          [
            [
              1,
              1
            ],
            [
              2,
              2
            ],
            [
              3,
              3
            ]
          ],
          [
            [
              4,
              4
            ]
          ]
        ]
      }
    ],
    "function_name": "shuffle_data"
  },
  {
    "id": 31,
    "title": "Divide Dataset Based on Feature Threshold",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gZGl2aWRlIGEgZGF0YXNldCBiYXNlZCBvbiB3aGV0aGVyIHRoZSB2YWx1ZSBvZiBhIHNwZWNpZmllZCBmZWF0dXJlIGlzIGdyZWF0ZXIgdGhhbiBvciBlcXVhbCB0byBhIGdpdmVuIHRocmVzaG9sZC4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gdHdvIHN1YnNldHMgb2YgdGhlIGRhdGFzZXQ6IG9uZSB3aXRoIHNhbXBsZXMgdGhhdCBtZWV0IHRoZSBjb25kaXRpb24gYW5kIGFub3RoZXIgd2l0aCBzYW1wbGVzIHRoYXQgZG8gbm90Lg==",
    "starter_code": "import torch\nfrom typing import List\n\nclass Solution:\n    def divide_on_feature(self, X, feature_i, threshold) -> List[torch.Tensor]:\n        \"\"\"\n        Divide the tensor X into two subsets based on whether X[:, feature_i] is greater than or equal to (or equal to) the threshold.\n        Return two tensors: one with samples that meet the condition, one with samples that do not.\n        \"\"\"\n        pass\n",
    "example_input": "X = np.array([[1, 2], \n                  [3, 4], \n                  [5, 6], \n                  [7, 8], \n                  [9, 10]])\n    feature_i = 0\n    threshold = 5",
    "example_output": "[array([[ 5,  6],\n                    [ 7,  8],\n                    [ 9, 10]]), \n             array([[1, 2],\n                    [3, 4]])]",
    "example_reasoning": "The dataset X is divided based on whether the value in the 0th feature (first column) is greater than or equal to 5. Samples with the first column value >= 5 are in the first subset, and the rest are in the second subset.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgRGF0YXNldCBEaXZpc2lvbiBCYXNlZCBvbiBGZWF0dXJlIFRocmVzaG9sZAoKRGl2aWRpbmcgYSBkYXRhc2V0IGJhc2VkIG9uIGEgZmVhdHVyZSB0aHJlc2hvbGQgaXMgYSBjb21tb24gb3BlcmF0aW9uIGluIG1hY2hpbmUgbGVhcm5pbmcsIGVzcGVjaWFsbHkgaW4gZGVjaXNpb24gdHJlZSBhbGdvcml0aG1zLiBUaGlzIHRlY2huaXF1ZSBoZWxwcyBpbiBjcmVhdGluZyBzcGxpdHMgdGhhdCBjYW4gYmUgdXNlZCBmb3IgZnVydGhlciBwcm9jZXNzaW5nIG9yIG1vZGVsIHRyYWluaW5nLgoKIyMjIFByb2JsZW0gT3ZlcnZpZXcKSW4gdGhpcyBwcm9ibGVtLCB5b3Ugd2lsbCB3cml0ZSBhIGZ1bmN0aW9uIHRvIHNwbGl0IGEgZGF0YXNldCBiYXNlZCBvbiB3aGV0aGVyIHRoZSB2YWx1ZSBvZiBhIHNwZWNpZmllZCBmZWF0dXJlIGlzIGdyZWF0ZXIgdGhhbiBvciBlcXVhbCB0byBhIGdpdmVuIHRocmVzaG9sZC4gWW91J2xsIG5lZWQgdG8gY3JlYXRlIHR3byBzdWJzZXRzOgotIE9uZSBmb3Igc2FtcGxlcyB0aGF0IG1lZXQgdGhlIGNvbmRpdGlvbiAodmFsdWVzIGdyZWF0ZXIgdGhhbiBvciBlcXVhbCB0byB0aGUgdGhyZXNob2xkKS4KLSBBbm90aGVyIGZvciBzYW1wbGVzIHRoYXQgZG8gbm90IG1lZXQgdGhlIGNvbmRpdGlvbi4KCiMjIyBJbXBvcnRhbmNlClRoaXMgbWV0aG9kIGlzIGNydWNpYWwgZm9yIGFsZ29yaXRobXMgdGhhdCByZWx5IG9uIGRhdGEgcGFydGl0aW9uaW5nLCBzdWNoIGFzIGRlY2lzaW9uIHRyZWVzIGFuZCByYW5kb20gZm9yZXN0cy4gQnkgc3BsaXR0aW5nIHRoZSBkYXRhLCB0aGUgbW9kZWwgY2FuIGNyZWF0ZSBydWxlcyB0byBtYWtlIHByZWRpY3Rpb25zIGJhc2VkIG9uIHRoZSB0aHJlc2hvbGQgdmFsdWVzIG9mIGNlcnRhaW4gZmVhdHVyZXMuCg==",
    "solution_code": "import torch\nfrom typing import List\n\nclass Solution:\n    def divide_on_feature(self, X, feature_i, threshold) -> List[torch.Tensor]:\n        X_t = torch.as_tensor(X)\n        if isinstance(threshold, (int, float)):\n            mask = X_t[:, feature_i] >= threshold\n        else:\n            mask = X_t[:, feature_i] == threshold\n        subset1 = X_t[mask]\n        subset2 = X_t[~mask]\n        return [subset1, subset2]\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ],
            [
              5,
              6
            ],
            [
              7,
              8
            ],
            [
              9,
              10
            ]
          ],
          0,
          5
        ],
        "expected_output": "[array([[ 5,  6], [ 7,  8], [ 9, 10]]), array([[1, 2], [3, 4]])]"
      },
      {
        "test": [
          [
            [
              1,
              1
            ],
            [
              2,
              2
            ],
            [
              3,
              3
            ],
            [
              4,
              4
            ]
          ],
          1,
          3
        ],
        "expected_output": "[array([[3, 3], [4, 4]]), array([[1, 1], [2, 2]])]"
      }
    ],
    "function_name": "divide_on_feature"
  },
  {
    "id": 32,
    "title": "Generate Sorted Polynomial Features",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCB0YWtlcyBhIDItRCBOdW1QeSBhcnJheSAqKlgqKiBhbmQgYW4gaW50ZWdlciAqKmRlZ3JlZSoqLCBnZW5lcmF0ZXMgYWxsIHBvbHlub21pYWwgZmVhdHVyZSBjb21iaW5hdGlvbnMgb2YgdGhlIGNvbHVtbnMgb2YgKipYKiogdXAgdG8gdGhlIGdpdmVuIGRlZ3JlZSAqKmluY2x1c2l2ZSoqLCAqKnRoZW4gc29ydHMgdGhlIHJlc3VsdGluZyBmZWF0dXJlcyBmb3IgZWFjaCBzYW1wbGUgZnJvbSBsb3dlc3QgdG8gaGlnaGVzdCB2YWx1ZSoqLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiBhIG5ldyAyLUQgTnVtUHkgYXJyYXkgd2hvc2Ugcm93cyBjb3JyZXNwb25kIHRvIHRoZSBpbnB1dCBzYW1wbGVzIGFuZCB3aG9zZSBjb2x1bW5zIGFyZSB0aGUgKiphc2NlbmRpbmctc29ydGVkKiogcG9seW5vbWlhbCBmZWF0dXJlcy4=",
    "starter_code": "import torch\nfrom itertools import combinations_with_replacement\n\nclass Solution:\n    def polynomial_features(self, X, degree):\n        \"\"\"\n        Given a 2D tensor X and integer degree, return a new tensor of all polynomial feature combinations\n        (with constant term), sorted for each sample from smallest to largest.\n        \"\"\"\n        # Hint: Use combinations_with_replacement and torch.prod.\n        pass\n",
    "example_input": "X = np.array([[2, 3],\n              [3, 4],\n              [5, 6]])\ndegree = 2\noutput = polynomial_features(X, degree)\nprint(output)",
    "example_output": "[[ 1.  2.  3.  4.  6.  9.]\n [ 1.  3.  4.  9. 12. 16.]\n [ 1.  5.  6. 25. 30. 36.]]",
    "example_reasoning": "For **degree = 2**, the raw polynomial terms for the first sample are [1, 2, 3, 4, 6, 9].  Sorting them from smallest to largest yields [1, 2, 3, 4, 6, 9]. The same procedure is applied to every sample.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyBQb2x5bm9taWFsIEZlYXR1cmVzCgpHZW5lcmF0aW5nIHBvbHlub21pYWwgZmVhdHVyZXMgaXMgYSBtZXRob2QgdXNlZCB0byBjcmVhdGUgbmV3IGZlYXR1cmVzIGZvciBhIG1hY2hpbmUtbGVhcm5pbmcgbW9kZWwgYnkgcmFpc2luZyBleGlzdGluZyBmZWF0dXJlcyB0byBhIHNwZWNpZmllZCBwb3dlci4gVGhpcyB0ZWNobmlxdWUgaGVscHMgY2FwdHVyZSBub24tbGluZWFyIHJlbGF0aW9uc2hpcHMgYmV0d2VlbiBmZWF0dXJlcy4KCiMjIyBFeGFtcGxlCkdpdmVuIGEgZGF0YXNldCB3aXRoIHR3byBmZWF0dXJlcyAkeF8xJCBhbmQgJHhfMiQsIGdlbmVyYXRpbmcgcG9seW5vbWlhbCBmZWF0dXJlcyB1cCB0byBkZWdyZWUgMiB3aWxsIGNyZWF0ZSBuZXcgZmVhdHVyZXMgc3VjaCBhczoKLSAkeF8xXjIkCi0gJHhfMl4yJAotICR4XzEgeF8yJAoKIyMjIFByb2JsZW0gT3ZlcnZpZXcKSW4gdGhpcyBwcm9ibGVtIHlvdSB3aWxsIHdyaXRlIGEgZnVuY3Rpb24gdG8gKipnZW5lcmF0ZSoqIHBvbHlub21pYWwgZmVhdHVyZXMgKiphbmQgdGhlbiBzb3J0IGVhY2ggc2FtcGxlJ3MgZmVhdHVyZXMgaW4gYXNjZW5kaW5nIG9yZGVyKiouIFNwZWNpZmljYWxseToKLSBHaXZlbiBhIDItRCBOdW1QeSBhcnJheSAqKlgqKiBhbmQgYW4gaW50ZWdlciAqKmRlZ3JlZSoqLCBjcmVhdGUgYSBuZXcgMi1EIGFycmF5IHdpdGggYWxsIHBvbHlub21pYWwgY29tYmluYXRpb25zIG9mIHRoZSBmZWF0dXJlcyB1cCB0byB0aGUgc3BlY2lmaWVkIGRlZ3JlZS4KLSBGaW5hbGx5LCBzb3J0IGVhY2ggcm93IGZyb20gdGhlIGxvd2VzdCB2YWx1ZSB0byB0aGUgaGlnaGVzdCB2YWx1ZS4KCiMjIyBJbXBvcnRhbmNlClBvbHlub21pYWwgZXhwYW5zaW9uIGFsbG93cyBvdGhlcndpc2UgbGluZWFyIG1vZGVscyB0byBoYW5kbGUgbm9uLWxpbmVhciBkYXRhLiBTb3J0aW5nIHRoZSBleHBhbmRlZCBmZWF0dXJlcyBjYW4gYmUgdXNlZnVsIGZvciBjZXJ0YWluIGRvd25zdHJlYW0gdGFza3MgKGUuZy4sIGhpc3RvZ3JhbS1iYXNlZCBtb2RlbHMgb3IgZmVhdHVyZSBzZWxlY3Rpb24gaGV1cmlzdGljcykgYW5kIHJlaW5mb3JjZXMgYXJyYXktbWFuaXB1bGF0aW9uIHNraWxscyBpbiBOdW1QeS4=",
    "solution_code": "import torch\nfrom itertools import combinations_with_replacement\n\nclass Solution:\n    def polynomial_features(self, X, degree):\n        X = torch.as_tensor(X, dtype=torch.float32)\n        n_samples, n_features = X.shape\n        combs = list(combinations_with_replacement(range(n_features), degree))\n        # For all powers from 0 up to degree\n        all_combs = [(c,) for c in range(n_features)]\n        for d in range(2, degree+1):\n            all_combs.extend(list(combinations_with_replacement(range(n_features), d)))\n        # Always include bias (all 0 degree)\n        features = [torch.ones(n_samples, 1)]\n        for comb in all_combs:\n            vals = torch.prod(X[:, list(comb)], dim=1, keepdim=True)\n            features.append(vals)\n        result = torch.cat(features, dim=1)\n        # Sort each row\n        result_sorted, _ = torch.sort(result, dim=1)\n        return result_sorted\n",
    "test_cases": [
      {
        "test": [
          [
            [
              2,
              3
            ],
            [
              3,
              4
            ],
            [
              5,
              6
            ]
          ],
          2
        ],
        "expected_output": "[[ 1.  2.  3.  4.  6.  9.]\n [ 1.  3.  4.  9. 12. 16.]\n [ 1.  5.  6. 25. 30. 36.]]"
      },
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ],
            [
              5,
              6
            ]
          ],
          3
        ],
        "expected_output": [
          [
            1.0,
            1.0,
            1.0,
            1.0,
            2.0,
            2.0,
            2.0,
            4.0,
            4.0,
            8.0
          ],
          [
            1.0,
            3.0,
            4.0,
            9.0,
            12.0,
            16.0,
            27.0,
            36.0,
            48.0,
            64.0
          ],
          [
            1.0,
            5.0,
            6.0,
            25.0,
            30.0,
            36.0,
            125.0,
            150.0,
            180.0,
            216.0
          ]
        ]
      }
    ],
    "function_name": "polynomial_features"
  },
  {
    "id": 33,
    "title": "Generate Random Subsets of a Dataset",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gZ2VuZXJhdGUgcmFuZG9tIHN1YnNldHMgb2YgYSBnaXZlbiBkYXRhc2V0LiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgaW4gYSAyRCBudW1weSBhcnJheSBYLCBhIDFEIG51bXB5IGFycmF5IHksIGFuIGludGVnZXIgbl9zdWJzZXRzLCBhbmQgYSBib29sZWFuIHJlcGxhY2VtZW50cy4gSXQgc2hvdWxkIHJldHVybiBhIGxpc3Qgb2Ygbl9zdWJzZXRzIHJhbmRvbSBzdWJzZXRzIG9mIHRoZSBkYXRhc2V0LCB3aGVyZSBlYWNoIHN1YnNldCBpcyBhIHR1cGxlIG9mIChYX3N1YnNldCwgeV9zdWJzZXQpLiBJZiByZXBsYWNlbWVudHMgaXMgVHJ1ZSwgdGhlIHN1YnNldHMgc2hvdWxkIGJlIGNyZWF0ZWQgd2l0aCByZXBsYWNlbWVudHM7IG90aGVyd2lzZSwgd2l0aG91dCByZXBsYWNlbWVudHMu",
    "starter_code": "import torch\nfrom typing import List\n\nclass Solution:\n    def get_random_subsets(self, X, y, n_subsets, replacements=True, seed=None) -> list:\n        \"\"\"\n        Generate n_subsets random subsets from the dataset (X, y).\n        Each subset is a pair (X_subset, y_subset), as lists.\n        \"\"\"\n        # Hint: Use torch.randperm or torch.randint depending on replacements. Use generator for seeding.\n        pass\n",
    "example_input": "X = np.array([[1, 2],\n                  [3, 4],\n                  [5, 6],\n                  [7, 8],\n                  [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    n_subsets = 3\n    replacements = True\n    get_random_subsets(X, y, n_subsets, replacements)",
    "example_output": "[array([[7, 8],\n            [1, 2]]), \n     array([4, 1])]\n     \n    [array([[9, 10],\n            [5, 6]]), \n     array([5, 3])]\n     \n    [array([[3, 4],\n            [5, 6]]), \n     array([2, 3])]",
    "example_reasoning": "The function generates three random subsets of the dataset without replacements.\n    Each subset includes 50% of the samples (since replacements=False). The samples\n    are randomly selected without duplication.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgUmFuZG9tIFN1YnNldHMgb2YgYSBEYXRhc2V0CgpHZW5lcmF0aW5nIHJhbmRvbSBzdWJzZXRzIG9mIGEgZGF0YXNldCBpcyBhIHVzZWZ1bCB0ZWNobmlxdWUgaW4gbWFjaGluZSBsZWFybmluZywgcGFydGljdWxhcmx5IGluIGVuc2VtYmxlIG1ldGhvZHMgbGlrZSBiYWdnaW5nIGFuZCByYW5kb20gZm9yZXN0cy4gQnkgY3JlYXRpbmcgcmFuZG9tIHN1YnNldHMsIG1vZGVscyBjYW4gYmUgdHJhaW5lZCBvbiBkaWZmZXJlbnQgcGFydHMgb2YgdGhlIGRhdGEsIHdoaWNoIGhlbHBzIGluIHJlZHVjaW5nIG92ZXJmaXR0aW5nIGFuZCBpbXByb3ZpbmcgZ2VuZXJhbGl6YXRpb24uCgojIyMgUHJvYmxlbSBPdmVydmlldwpJbiB0aGlzIHByb2JsZW0sIHlvdSB3aWxsIHdyaXRlIGEgZnVuY3Rpb24gdG8gZ2VuZXJhdGUgcmFuZG9tIHN1YnNldHMgb2YgYSBnaXZlbiBkYXRhc2V0LiBTcGVjaWZpY2FsbHk6Ci0gR2l2ZW4gYSAyRCBudW1weSBhcnJheSAkWCQsIGEgMUQgbnVtcHkgYXJyYXkgJHkkLCBhbiBpbnRlZ2VyIGBuX3N1YnNldHNgLCBhbmQgYSBib29sZWFuIGByZXBsYWNlbWVudHNgLCB0aGUgZnVuY3Rpb24gd2lsbCBjcmVhdGUgYSBsaXN0IG9mIGBuX3N1YnNldHNgIHJhbmRvbSBzdWJzZXRzLgotIEVhY2ggc3Vic2V0IHdpbGwgYmUgYSB0dXBsZSBvZiAkKFhfe1x0ZXh0e3N1YnNldH19LCB5X3tcdGV4dHtzdWJzZXR9fSkkLgoKIyMjIFBhcmFtZXRlcnMKLSAqKiRYJCoqOiBBIDJEIG51bXB5IGFycmF5IHJlcHJlc2VudGluZyB0aGUgZmVhdHVyZXMuCi0gKiokeSQqKjogQSAxRCBudW1weSBhcnJheSByZXByZXNlbnRpbmcgdGhlIGxhYmVscy4KLSAqKiRuX3tcdGV4dHtzdWJzZXRzfX0kKio6IFRoZSBudW1iZXIgb2YgcmFuZG9tIHN1YnNldHMgdG8gZ2VuZXJhdGUuCi0gKipgcmVwbGFjZW1lbnRzYCoqOiBBIGJvb2xlYW4gaW5kaWNhdGluZyB3aGV0aGVyIHRvIHNhbXBsZSB3aXRoIG9yIHdpdGhvdXQgcmVwbGFjZW1lbnQuCiAgLSBJZiBgcmVwbGFjZW1lbnRzYCBpcyAqKlRydWUqKiwgc3Vic2V0cyB3aWxsIGJlIGNyZWF0ZWQgKndpdGgqIHJlcGxhY2VtZW50cywgbWVhbmluZyBzYW1wbGVzIGNhbiBiZSByZXBlYXRlZCB3aXRoaW4gYSBzdWJzZXQuCiAgLSBJZiBgcmVwbGFjZW1lbnRzYCBpcyAqKkZhbHNlKiosIHN1YnNldHMgd2lsbCBiZSBjcmVhdGVkICp3aXRob3V0KiByZXBsYWNlbWVudHMsIG1lYW5pbmcgc2FtcGxlcyBjYW5ub3QgYmUgcmVwZWF0ZWQgd2l0aGluIGEgc3Vic2V0LgoKIyMjIEltcG9ydGFuY2UKQnkgdW5kZXJzdGFuZGluZyBhbmQgaW1wbGVtZW50aW5nIHRoaXMgdGVjaG5pcXVlLCB5b3UgY2FuIGVuaGFuY2UgdGhlIHBlcmZvcm1hbmNlIG9mIHlvdXIgbW9kZWxzIHRocm91Z2ggbWV0aG9kcyBsaWtlIGJvb3RzdHJhcHBpbmcgYW5kIGVuc2VtYmxlIGxlYXJuaW5nLgo=",
    "solution_code": "import torch\nimport numpy as np\nfrom typing import List\n\nclass Solution:\n    def get_random_subsets(self, X, y, n_subsets, replacements=True, seed=None) -> list:\n        X = np.asarray(X)\n        y = np.asarray(y)\n        N = len(X)\n        subset_size = N // 2 if not replacements else N\n        rng = np.random.default_rng(seed)\n        out = []\n        for _ in range(n_subsets):\n            if replacements:\n                idx = rng.integers(N, size=subset_size, endpoint=False)\n            else:\n                idx = rng.choice(N, size=subset_size, replace=False)\n            out.append([X[idx].tolist(), y[idx].tolist()])\n        return out\n",
    "test_cases": [
      {
        "test": [
          "X = np.array(torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]))\ny = np.array(torch.tensor([1, 2, 3, 4, 5]))\nprint(get_random_subsets(X,y, 3, False, seed=42))"
        ],
        "expected_output": [
          [
            [
              3,
              4
            ],
            [
              9,
              10
            ]
          ],
          [
            2,
            5
          ],
          [
            [
              7,
              8
            ],
            [
              3,
              4
            ]
          ],
          [
            4,
            2
          ],
          [
            [
              3,
              4
            ],
            [
              1,
              2
            ]
          ],
          [
            2,
            1
          ]
        ]
      },
      {
        "test": [
          "X = np.array(torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4]]))\ny = np.array(torch.tensor([10, 20, 30, 40]))\nprint(get_random_subsets(X, y, 1, True, seed=42))"
        ],
        "expected_output": [
          [
            [
              [
                3,
                3
              ],
              [
                4,
                4
              ],
              [
                1,
                1
              ],
              [
                3,
                3
              ]
            ],
            [
              30,
              40,
              10,
              30
            ]
          ]
        ]
      }
    ],
    "function_name": "get_random_subsets"
  },
  {
    "id": 34,
    "title": "One-Hot Encoding of Nominal Values",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gcGVyZm9ybSBvbmUtaG90IGVuY29kaW5nIG9mIG5vbWluYWwgdmFsdWVzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgaW4gYSAxRCBudW1weSBhcnJheSB4IG9mIGludGVnZXIgdmFsdWVzIGFuZCBhbiBvcHRpb25hbCBpbnRlZ2VyIG5fY29sIHJlcHJlc2VudGluZyB0aGUgbnVtYmVyIG9mIGNvbHVtbnMgZm9yIHRoZSBvbmUtaG90IGVuY29kZWQgYXJyYXkuIElmIG5fY29sIGlzIG5vdCBwcm92aWRlZCwgaXQgc2hvdWxkIGJlIGF1dG9tYXRpY2FsbHkgZGV0ZXJtaW5lZCBmcm9tIHRoZSBpbnB1dCBhcnJheS4=",
    "starter_code": "import torch\nfrom typing import Optional\n\nclass Solution:\n    def to_categorical(self, x: torch.Tensor, n_col: Optional[int] = None) -> torch.Tensor:\n        \"\"\"\n        Perform one-hot encoding on a 1D integer tensor `x`. If `n_col` is not provided, infer it from the max value in `x`.\n        \"\"\"\n        # Hint: You can use torch.nn.functional.one_hot\n        pass\n",
    "example_input": "x = np.array([0, 1, 2, 1, 0])\n    output = to_categorical(x)\n    print(output)",
    "example_output": "# [[1. 0. 0.]\n    #  [0. 1. 0.]\n    #  [0. 0. 1.]\n    #  [0. 1. 0.]\n    #  [1. 0. 0.]]",
    "example_reasoning": "Each element in the input array is transformed into a one-hot encoded vector,\n    where the index corresponding to the value in the input array is set to 1, \n    and all other indices are set to 0.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgT25lLUhvdCBFbmNvZGluZwoKT25lLWhvdCBlbmNvZGluZyBpcyBhIG1ldGhvZCB1c2VkIHRvIHJlcHJlc2VudCBjYXRlZ29yaWNhbCB2YXJpYWJsZXMgYXMgYmluYXJ5IHZlY3RvcnMuIFRoaXMgdGVjaG5pcXVlIGlzIHVzZWZ1bCBpbiBtYWNoaW5lIGxlYXJuaW5nIHdoZW4gZGVhbGluZyB3aXRoIGNhdGVnb3JpY2FsIGRhdGEgdGhhdCBoYXMgbm8gb3JkaW5hbCByZWxhdGlvbnNoaXAuCgojIyMgRXhwbGFuYXRpb24KSW4gb25lLWhvdCBlbmNvZGluZywgZWFjaCBjYXRlZ29yeSBpcyByZXByZXNlbnRlZCBieSBhIGJpbmFyeSB2ZWN0b3Igd2l0aCBhIGxlbmd0aCBlcXVhbCB0byB0aGUgbnVtYmVyIG9mIGNhdGVnb3JpZXMuIFRoZSB2ZWN0b3IgaGFzIGEgdmFsdWUgb2YgMSBhdCB0aGUgaW5kZXggY29ycmVzcG9uZGluZyB0byB0aGUgY2F0ZWdvcnkgYW5kIDAgYXQgYWxsIG90aGVyIGluZGljZXMuCgojIyMgRXhhbXBsZQpGb3IgaW5zdGFuY2UsIGlmIHlvdSBoYXZlIHRocmVlIGNhdGVnb3JpZXM6IDAsIDEsIGFuZCAyLCB0aGUgb25lLWhvdCBlbmNvZGVkIHZlY3RvcnMgd291bGQgYmU6Ci0gKiowKio6ICRbMSwgMCwgMF0kCi0gKioxKio6ICRbMCwgMSwgMF0kCi0gKioyKio6ICRbMCwgMCwgMV0kCgpUaGlzIG1ldGhvZCBlbnN1cmVzIHRoYXQgdGhlIG1vZGVsIGRvZXMgbm90IGFzc3VtZSBhbnkgb3JkaW5hbCByZWxhdGlvbnNoaXAgYmV0d2VlbiBjYXRlZ29yaWVzLCB3aGljaCBpcyBjcnVjaWFsIGZvciBtYW55IG1hY2hpbmUgbGVhcm5pbmcgYWxnb3JpdGhtcy4KCiMjIyBNYXRoZW1hdGljYWwgUmVwcmVzZW50YXRpb24KVGhlIG9uZS1ob3QgZW5jb2RpbmcgcHJvY2VzcyBjYW4gYmUgbWF0aGVtYXRpY2FsbHkgcmVwcmVzZW50ZWQgYXMgZm9sbG93czoKCkdpdmVuIGEgY2F0ZWdvcnkgJHhfaSQgZnJvbSBhIHNldCBvZiBjYXRlZ29yaWVzICRcezAsIDEsIFxsZG90cywgbi0xXH0kLCB0aGUgb25lLWhvdCBlbmNvZGVkIHZlY3RvciAkXG1hdGhiZnt2fSQgaXM6CiQkClxtYXRoYmZ7dn1faSA9IApcYmVnaW57Y2FzZXN9IAoxICYgXHRleHR7aWYgfSBpID0geF9pIFxcCjAgJiBcdGV4dHtvdGhlcndpc2V9ClxlbmR7Y2FzZXN9CiQkCgpUaGlzIHZlY3RvciAkXG1hdGhiZnt2fSQgd2lsbCBoYXZlIGEgbGVuZ3RoIGVxdWFsIHRvIHRoZSBudW1iZXIgb2YgdW5pcXVlIGNhdGVnb3JpZXMuCg==",
    "solution_code": "import torch\nfrom typing import Optional\nimport torch.nn.functional as F\n\nclass Solution:\n    def to_categorical(self, x: torch.Tensor, n_col: Optional[int] = None) -> torch.Tensor:\n        if n_col is None:\n            n_col = int(torch.max(x)) + 1\n        return F.one_hot(x, num_classes=n_col).float()\n",
    "test_cases": [
      {
        "test": [
          [
            0,
            1,
            2,
            1,
            0
          ]
        ],
        "expected_output": [
          [
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            1.0,
            0.0
          ],
          [
            0.0,
            0.0,
            1.0
          ],
          [
            0.0,
            1.0,
            0.0
          ],
          [
            1.0,
            0.0,
            0.0
          ]
        ]
      },
      {
        "test": [
          [
            3,
            1,
            2,
            1,
            3
          ],
          4
        ],
        "expected_output": [
          [
            0.0,
            0.0,
            0.0,
            1.0
          ],
          [
            0.0,
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            0.0,
            1.0,
            0.0
          ],
          [
            0.0,
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            0.0,
            0.0,
            1.0
          ]
        ]
      }
    ],
    "function_name": "to_categorical"
  },
  {
    "id": 35,
    "title": "Convert Vector to Diagonal Matrix",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY29udmVydCBhIDFEIG51bXB5IGFycmF5IGludG8gYSBkaWFnb25hbCBtYXRyaXguIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBpbiBhIDFEIG51bXB5IGFycmF5IHggYW5kIHJldHVybiBhIDJEIG51bXB5IGFycmF5IHJlcHJlc2VudGluZyB0aGUgZGlhZ29uYWwgbWF0cml4Lg==",
    "starter_code": "import torch\nfrom typing import Union\n\nclass Solution:\n    def make_diagonal(self, x: Union[torch.Tensor, list, \"np.ndarray\"]) -> torch.Tensor:\n        \"\"\"Return a diagonal matrix whose diagonal elements are the 1-D values in `x`.\n        If `x` is not a torch tensor it will be converted automatically.\n\n        Hint: `torch.diag_embed` makes this very short!\n        \"\"\"\n        # \u270f\ufe0f  Your code here\n        pass\n",
    "example_input": "x = np.array([1, 2, 3])\n    output = make_diagonal(x)\n    print(output)",
    "example_output": "[[1. 0. 0.]\n    [0. 2. 0.]\n    [0. 0. 3.]]",
    "example_reasoning": "The input vector [1, 2, 3] is converted into a diagonal matrix where the elements of the vector form the diagonal of the matrix.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgRGlhZ29uYWwgTWF0cmljZXMKCkEgZGlhZ29uYWwgbWF0cml4IGlzIGEgc3F1YXJlIG1hdHJpeCBpbiB3aGljaCB0aGUgZW50cmllcyBvdXRzaWRlIHRoZSBtYWluIGRpYWdvbmFsIGFyZSBhbGwgemVyby4gVGhlIG1haW4gZGlhZ29uYWwgaXMgdGhlIHNldCBvZiBlbnRyaWVzIGV4dGVuZGluZyBmcm9tIHRoZSB0b3AgbGVmdCB0byB0aGUgYm90dG9tIHJpZ2h0IG9mIHRoZSBtYXRyaXguCgojIyMgUHJvYmxlbSBPdmVydmlldwpJbiB0aGlzIHByb2JsZW0sIHlvdSB3aWxsIHdyaXRlIGEgZnVuY3Rpb24gdG8gY29udmVydCBhIDFEIG51bXB5IGFycmF5ICh2ZWN0b3IpIGludG8gYSBkaWFnb25hbCBtYXRyaXguIFRoZSByZXN1bHRpbmcgbWF0cml4IHdpbGwgaGF2ZSB0aGUgZWxlbWVudHMgb2YgdGhlIGlucHV0IHZlY3RvciBvbiBpdHMgbWFpbiBkaWFnb25hbCwgd2l0aCB6ZXJvcyBlbHNld2hlcmUuCgojIyMgTWF0aGVtYXRpY2FsIFJlcHJlc2VudGF0aW9uCkdpdmVuIGEgdmVjdG9yICRcbWF0aGJme3h9ID0gW3hfMSwgeF8yLCBcbGRvdHMsIHhfbl0kLCB0aGUgY29ycmVzcG9uZGluZyBkaWFnb25hbCBtYXRyaXggJFxtYXRoYmZ7RH0kIGlzOgokJApcbWF0aGJme0R9ID0gXGJlZ2lue2JtYXRyaXh9CnhfMSAmIDAgJiAwICYgXGNkb3RzICYgMCBcXAowICYgeF8yICYgMCAmIFxjZG90cyAmIDAgXFwKMCAmIDAgJiB4XzMgJiBcY2RvdHMgJiAwIFxcClx2ZG90cyAmIFx2ZG90cyAmIFx2ZG90cyAmIFxkZG90cyAmIFx2ZG90cyBcXAowICYgMCAmIDAgJiBcY2RvdHMgJiB4X24KXGVuZHtibWF0cml4fQokJAoKIyMjIEltcG9ydGFuY2UKRGlhZ29uYWwgbWF0cmljZXMgYXJlIGltcG9ydGFudCBpbiB2YXJpb3VzIG1hdGhlbWF0aWNhbCBhbmQgc2NpZW50aWZpYyBjb21wdXRhdGlvbnMgZHVlIHRvIHRoZWlyIHNpbXBsZSBzdHJ1Y3R1cmUgYW5kIHVzZWZ1bCBwcm9wZXJ0aWVzLgo=",
    "solution_code": "import torch\nfrom typing import Union\n\nclass Solution:\n    def make_diagonal(self, x: Union[torch.Tensor, list, \"np.ndarray\"]) -> torch.Tensor:\n        # Convert to a 1-D float tensor\n        x_t = torch.as_tensor(x, dtype=torch.float).flatten()\n        # diag_embed(x_t) \u2192 (N) -> (N,N) diagonal matrix\n        return torch.diag_embed(x_t)\n",
    "test_cases": [
      {
        "test": [
          [
            1,
            2,
            3
          ]
        ],
        "expected_output": [
          [
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            2.0,
            0.0
          ],
          [
            0.0,
            0.0,
            3.0
          ]
        ]
      },
      {
        "test": [
          [
            4,
            5,
            6,
            7
          ]
        ],
        "expected_output": [
          [
            4.0,
            0.0,
            0.0,
            0.0
          ],
          [
            0.0,
            5.0,
            0.0,
            0.0
          ],
          [
            0.0,
            0.0,
            6.0,
            0.0
          ],
          [
            0.0,
            0.0,
            0.0,
            7.0
          ]
        ]
      }
    ],
    "function_name": "make_diagonal"
  },
  {
    "id": 36,
    "title": "Calculate Accuracy Score",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBhY2N1cmFjeSBzY29yZSBvZiBhIG1vZGVsJ3MgcHJlZGljdGlvbnMuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBpbiB0d28gMUQgbnVtcHkgYXJyYXlzOiB5X3RydWUsIHdoaWNoIGNvbnRhaW5zIHRoZSB0cnVlIGxhYmVscywgYW5kIHlfcHJlZCwgd2hpY2ggY29udGFpbnMgdGhlIHByZWRpY3RlZCBsYWJlbHMuIEl0IHNob3VsZCByZXR1cm4gdGhlIGFjY3VyYWN5IHNjb3JlIGFzIGEgZmxvYXQu",
    "starter_code": "import torch\nfrom typing import Union\n\nclass Solution:\n    def accuracy_score(self, y_true: Union[torch.Tensor, list, \"np.ndarray\"],\n                       y_pred: Union[torch.Tensor, list, \"np.ndarray\"]) -> float:\n        \"\"\"\n        Compute the accuracy: fraction of matching elements in y_true and y_pred.\n        Both inputs may be torch.Tensor, list, or numpy.ndarray.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "y_true = np.array([1, 0, 1, 1, 0, 1])\n    y_pred = np.array([1, 0, 0, 1, 0, 1])\n    output = accuracy_score(y_true, y_pred)\n    print(output)",
    "example_output": "# 0.8333333333333334",
    "example_reasoning": "The function compares the true labels with the predicted labels and calculates the ratio of correct predictions to the total number of predictions. In this example, there are 5 correct predictions out of 6, resulting in an accuracy score of 0.8333333333333334.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgQWNjdXJhY3kgU2NvcmUKCkFjY3VyYWN5IGlzIGEgbWV0cmljIHVzZWQgdG8gZXZhbHVhdGUgdGhlIHBlcmZvcm1hbmNlIG9mIGEgY2xhc3NpZmljYXRpb24gbW9kZWwuIEl0IGlzIGRlZmluZWQgYXMgdGhlIHJhdGlvIG9mIHRoZSBudW1iZXIgb2YgY29ycmVjdCBwcmVkaWN0aW9ucyB0byB0aGUgdG90YWwgbnVtYmVyIG9mIHByZWRpY3Rpb25zIG1hZGUuIE1hdGhlbWF0aWNhbGx5LCBhY2N1cmFjeSBpcyBnaXZlbiBieToKJCQKXHRleHR7QWNjdXJhY3l9ID0gXGZyYWN7XHRleHR7TnVtYmVyIG9mIENvcnJlY3QgUHJlZGljdGlvbnN9fXtcdGV4dHtUb3RhbCBOdW1iZXIgb2YgUHJlZGljdGlvbnN9fQokJAoKIyMjIFByb2JsZW0gT3ZlcnZpZXcKSW4gdGhpcyBwcm9ibGVtLCB5b3Ugd2lsbCB3cml0ZSBhIGZ1bmN0aW9uIHRvIGNhbGN1bGF0ZSB0aGUgYWNjdXJhY3kgc2NvcmUgZ2l2ZW4gdGhlIHRydWUgbGFiZWxzIGFuZCB0aGUgcHJlZGljdGVkIGxhYmVscy4gVGhlIGZ1bmN0aW9uIHdpbGwgY29tcGFyZSB0aGUgdHdvIGFycmF5cyBhbmQgY29tcHV0ZSB0aGUgYWNjdXJhY3kgYXMgdGhlIHByb3BvcnRpb24gb2YgbWF0Y2hpbmcgZWxlbWVudHMuCgojIyMgSW1wb3J0YW5jZQpBY2N1cmFjeSBpcyBhIHN0cmFpZ2h0Zm9yd2FyZCBhbmQgY29tbW9ubHkgdXNlZCBtZXRyaWMgZm9yIGNsYXNzaWZpY2F0aW9uIHRhc2tzLiBJdCBwcm92aWRlcyBhIHF1aWNrIHdheSB0byB1bmRlcnN0YW5kIGhvdyB3ZWxsIGEgbW9kZWwgaXMgcGVyZm9ybWluZywgYnV0IGl0IG1heSBub3QgYWx3YXlzIGJlIHRoZSBiZXN0IG1ldHJpYywgZXNwZWNpYWxseSBmb3IgaW1iYWxhbmNlZCBkYXRhc2V0cy4K",
    "solution_code": "import torch\nfrom typing import Union\n\nclass Solution:\n    def accuracy_score(self, y_true: Union[torch.Tensor, list, \"np.ndarray\"],\n                       y_pred: Union[torch.Tensor, list, \"np.ndarray\"]) -> float:\n        t_true = torch.as_tensor(y_true)\n        t_pred = torch.as_tensor(y_pred)\n        return (t_true == t_pred).float().mean().item()\n",
    "test_cases": [
      {
        "test": [
          [
            1,
            0,
            1,
            1,
            0,
            1
          ],
          [
            1,
            0,
            0,
            1,
            0,
            1
          ]
        ],
        "expected_output": 0.8333333333333334
      },
      {
        "test": [
          [
            1,
            1,
            1,
            1
          ],
          [
            1,
            0,
            1,
            0
          ]
        ],
        "expected_output": 0.5
      }
    ],
    "function_name": "accuracy_score"
  },
  {
    "id": 37,
    "title": "Calculate Correlation Matrix",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBjb3JyZWxhdGlvbiBtYXRyaXggZm9yIGEgZ2l2ZW4gZGF0YXNldC4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGluIGEgMkQgbnVtcHkgYXJyYXkgWCBhbmQgYW4gb3B0aW9uYWwgMkQgbnVtcHkgYXJyYXkgWS4gSWYgWSBpcyBub3QgcHJvdmlkZWQsIHRoZSBmdW5jdGlvbiBzaG91bGQgY2FsY3VsYXRlIHRoZSBjb3JyZWxhdGlvbiBtYXRyaXggb2YgWCB3aXRoIGl0c2VsZi4gSXQgc2hvdWxkIHJldHVybiB0aGUgY29ycmVsYXRpb24gbWF0cml4IGFzIGEgMkQgbnVtcHkgYXJyYXku",
    "starter_code": "import torch\nfrom typing import Optional, Union\n\nclass Solution:\n    def calculate_correlation_matrix(self, \n        X: Union[torch.Tensor, list, \"np.ndarray\"],\n        Y: Optional[Union[torch.Tensor, list, \"np.ndarray\"]] = None\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute the correlation matrix of X (and optionally Y) using PyTorch.\n        If Y is None, returns the correlation matrix of X with itself.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "X = np.array([[1, 2],\n                  [3, 4],\n                  [5, 6]])\n    output = calculate_correlation_matrix(X)\n    print(output)",
    "example_output": "# [[1. 1.]\n    #  [1. 1.]]",
    "example_reasoning": "The function calculates the correlation matrix for the dataset X. In this example, the correlation between the two features is 1, indicating a perfect linear relationship.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgQ29ycmVsYXRpb24gTWF0cml4CgpBIGNvcnJlbGF0aW9uIG1hdHJpeCBpcyBhIHRhYmxlIHNob3dpbmcgdGhlIGNvcnJlbGF0aW9uIGNvZWZmaWNpZW50cyBiZXR3ZWVuIHZhcmlhYmxlcy4gRWFjaCBjZWxsIGluIHRoZSB0YWJsZSBzaG93cyB0aGUgY29ycmVsYXRpb24gYmV0d2VlbiB0d28gdmFyaWFibGVzLCB3aXRoIHZhbHVlcyByYW5naW5nIGZyb20gLTEgdG8gMS4gVGhlc2UgdmFsdWVzIGluZGljYXRlIHRoZSBzdHJlbmd0aCBhbmQgZGlyZWN0aW9uIG9mIHRoZSBsaW5lYXIgcmVsYXRpb25zaGlwIGJldHdlZW4gdGhlIHZhcmlhYmxlcy4KCiMjIyBNYXRoZW1hdGljYWwgRGVmaW5pdGlvbgpUaGUgY29ycmVsYXRpb24gY29lZmZpY2llbnQgYmV0d2VlbiB0d28gdmFyaWFibGVzIFwoIFggXCkgYW5kIFwoIFkgXCkgaXMgZ2l2ZW4gYnk6CiQkClx0ZXh0e2NvcnJ9KFgsIFkpID0gXGZyYWN7XHRleHR7Y292fShYLCBZKX17XHNpZ21hX1ggXHNpZ21hX1l9CiQkCgojIyMjIFdoZXJlOgotICQkIFx0ZXh0e2Nvdn0oWCwgWSkgJCQgaXMgdGhlIGNvdmFyaWFuY2UgYmV0d2VlbiBcKCBYIFwpIGFuZCBcKCBZIFwpLgotICQkIFxzaWdtYV9YICQkIGFuZCAkJCBcc2lnbWFfWSAkJCBhcmUgdGhlIHN0YW5kYXJkIGRldmlhdGlvbnMgb2YgXCggWCBcKSBhbmQgXCggWSBcKSwgcmVzcGVjdGl2ZWx5LgoKIyMjIFByb2JsZW0gT3ZlcnZpZXcKSW4gdGhpcyBwcm9ibGVtLCB5b3Ugd2lsbCB3cml0ZSBhIGZ1bmN0aW9uIHRvIGNhbGN1bGF0ZSB0aGUgY29ycmVsYXRpb24gbWF0cml4IGZvciBhIGdpdmVuIGRhdGFzZXQuIFRoZSBmdW5jdGlvbiB3aWxsIHRha2UgaW4gYSAyRCBudW1weSBhcnJheSBcKCBYIFwpIGFuZCBhbiBvcHRpb25hbCAyRCBudW1weSBhcnJheSBcKCBZIFwpLiBJZiBcKCBZIFwpIGlzIG5vdCBwcm92aWRlZCwgdGhlIGZ1bmN0aW9uIHdpbGwgY2FsY3VsYXRlIHRoZSBjb3JyZWxhdGlvbiBtYXRyaXggb2YgXCggWCBcKSB3aXRoIGl0c2VsZi4=",
    "solution_code": "import torch\nfrom typing import Optional, Union\n\nclass Solution:\n    def calculate_correlation_matrix(self, \n        X: Union[torch.Tensor, list, \"np.ndarray\"],\n        Y: Optional[Union[torch.Tensor, list, \"np.ndarray\"]] = None\n    ) -> torch.Tensor:\n        X_t = torch.as_tensor(X, dtype=torch.float32)\n        if Y is None:\n            Y_t = X_t\n        else:\n            Y_t = torch.as_tensor(Y, dtype=torch.float32)\n        # center\n        Xc = X_t - X_t.mean(dim=0, keepdim=True)\n        Yc = Y_t - Y_t.mean(dim=0, keepdim=True)\n        # covariance\n        n = X_t.shape[0]\n        cov = Xc.T @ Yc / (n - 1)\n        # standard deviations\n        stdX = Xc.std(dim=0, unbiased=True)\n        stdY = Yc.std(dim=0, unbiased=True)\n        # correlation\n        corr = cov / (stdX.unsqueeze(1) * stdY.unsqueeze(0))\n        return corr\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ],
            [
              5,
              6
            ]
          ]
        ],
        "expected_output": [
          [
            1.0,
            1.0
          ],
          [
            1.0,
            1.0
          ]
        ]
      },
      {
        "test": [
          [
            [
              1,
              2,
              3
            ],
            [
              7,
              15,
              6
            ],
            [
              7,
              8,
              9
            ]
          ]
        ],
        "expected_output": [
          [
            1.0,
            0.84298868,
            0.8660254
          ],
          [
            0.84298868,
            1.0,
            0.46108397
          ],
          [
            0.8660254,
            0.46108397,
            1.0
          ]
        ]
      }
    ],
    "function_name": "calculate_correlation_matrix"
  },
  {
    "id": 38,
    "title": "Implement AdaBoost Fit Method",
    "difficulty": "hard",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYGFkYWJvb3N0X2ZpdGAgdGhhdCBpbXBsZW1lbnRzIHRoZSBmaXQgbWV0aG9kIGZvciBhbiBBZGFCb29zdCBjbGFzc2lmaWVyLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgaW4gYSAyRCBudW1weSBhcnJheSBgWGAgb2Ygc2hhcGUgYChuX3NhbXBsZXMsIG5fZmVhdHVyZXMpYCByZXByZXNlbnRpbmcgdGhlIGRhdGFzZXQsIGEgMUQgbnVtcHkgYXJyYXkgYHlgIG9mIHNoYXBlIGAobl9zYW1wbGVzLClgIHJlcHJlc2VudGluZyB0aGUgbGFiZWxzLCBhbmQgYW4gaW50ZWdlciBgbl9jbGZgIHJlcHJlc2VudGluZyB0aGUgbnVtYmVyIG9mIGNsYXNzaWZpZXJzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIGluaXRpYWxpemUgc2FtcGxlIHdlaWdodHMsIGZpbmQgdGhlIGJlc3QgdGhyZXNob2xkcyBmb3IgZWFjaCBmZWF0dXJlLCBjYWxjdWxhdGUgdGhlIGVycm9yLCB1cGRhdGUgd2VpZ2h0cywgYW5kIHJldHVybiBhIGxpc3Qgb2YgY2xhc3NpZmllcnMgd2l0aCB0aGVpciBwYXJhbWV0ZXJzLg==",
    "starter_code": "import torch\nimport math\nfrom typing import List, Dict\n\nclass Solution:\n    def adaboost_fit(self, X, y, n_clf) -> List[Dict]:\n        \"\"\"\n        Fit an AdaBoost classifier using PyTorch tensors (no sklearn).\n        Args:\n            X: torch.Tensor or array-like, shape (n_samples, n_features)\n            y: torch.Tensor or array-like, shape (n_samples,) with labels (+1/-1)\n            n_clf: int, number of weak classifiers\n        Returns:\n            List of dictionaries with classifier params: 'polarity', 'threshold', 'feature_index', 'alpha'.\n        \"\"\"\n        # Your implementation here\n        pass\n",
    "example_input": "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([1, 1, -1, -1])\n    n_clf = 3\n\n    clfs = adaboost_fit(X, y, n_clf)\n    print(clfs)",
    "example_output": "(example format, actual values may vary):\n    # [{'polarity': 1, 'threshold': 2, 'feature_index': 0, 'alpha': 0.5},\n    #  {'polarity': -1, 'threshold': 3, 'feature_index': 1, 'alpha': 0.3},\n    #  {'polarity': 1, 'threshold': 4, 'feature_index': 0, 'alpha': 0.2}]",
    "example_reasoning": "The function fits an AdaBoost classifier on the dataset X with the given labels y and number of classifiers n_clf. It returns a list of classifiers with their parameters, including the polarity, threshold, feature index, and alpha values",
    "learn_content": "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",
    "solution_code": "import torch\nimport math\nfrom typing import List, Dict\n\nclass Solution:\n    def adaboost_fit(self, X, y, n_clf) -> List[Dict]:\n        X = torch.as_tensor(X, dtype=torch.float32)\n        y = torch.as_tensor(y, dtype=torch.float32)\n        n_samples, n_features = X.shape\n        w = torch.full((n_samples,), 1.0 / n_samples)\n        clfs = []\n        for _ in range(n_clf):\n            clf = {}\n            min_error = float('inf')\n            # Find best decision stump\n            for feature_i in range(n_features):\n                feature_values = X[:, feature_i]\n                thresholds = torch.unique(feature_values)\n                for threshold in thresholds:\n                    for polarity in [1, -1]:\n                        preds = torch.ones(n_samples)\n                        preds[polarity * feature_values < polarity * threshold] = -1\n                        error = torch.sum(w * (preds != y))\n                        if error > 0.5:\n                            error = 1 - error\n                            polarity = -polarity\n                        if error < min_error:\n                            min_error = error.item()\n                            clf['polarity'] = polarity\n                            clf['threshold'] = threshold.item()\n                            clf['feature_index'] = feature_i\n            # Compute alpha\n            clf['alpha'] = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n            # Update weights\n            feature_values = X[:, clf['feature_index']]\n            preds = torch.ones(n_samples)\n            polarity = clf['polarity']\n            preds[polarity * feature_values < polarity * clf['threshold']] = -1\n            w = w * torch.exp(-clf['alpha'] * y * preds)\n            w = w / torch.sum(w)\n            clfs.append(clf.copy())\n        return clfs\n",
    "test_cases": [
      {
        "test": [
          "X = np.array(torch.tensor([[1, 2], [2, 3], [3, 4], [4, 5]]))\ny = np.array(torch.tensor([1, 1, -1, -1]))\nn_clf = 3\nclfs = adaboost_fit(X, y, n_clf)\nprint(clfs)"
        ],
        "expected_output": [
          {
            "polarity": -1,
            "threshold": 3,
            "feature_index": 0,
            "alpha": 11.512925464970229
          },
          {
            "polarity": -1,
            "threshold": 3,
            "feature_index": 0,
            "alpha": 11.512925464970229
          },
          {
            "polarity": -1,
            "threshold": 3,
            "feature_index": 0,
            "alpha": 11.512925464970229
          }
        ]
      },
      {
        "test": [
          "X = np.array(torch.tensor([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]))\ny = np.array(torch.tensor([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]))\nn_clf = 2\nclfs = adaboost_fit(X, y, n_clf)\nprint(clfs)"
        ],
        "expected_output": [
          {
            "polarity": 1,
            "threshold": 5,
            "feature_index": 0,
            "alpha": 0.6931471803099453
          },
          {
            "polarity": -1,
            "threshold": 3,
            "feature_index": 0,
            "alpha": 0.5493061439673882
          }
        ]
      }
    ],
    "function_name": "adaboost_fit"
  },
  {
    "id": 39,
    "title": "Implementation of Log Softmax Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW4gbWFjaGluZSBsZWFybmluZyBhbmQgc3RhdGlzdGljcywgdGhlIHNvZnRtYXggZnVuY3Rpb24gaXMgYSBnZW5lcmFsaXphdGlvbiBvZiB0aGUgbG9naXN0aWMgZnVuY3Rpb24gdGhhdCBjb252ZXJ0cyBhIHZlY3RvciBvZiBzY29yZXMgaW50byBwcm9iYWJpbGl0aWVzLiBUaGUgbG9nLXNvZnRtYXggZnVuY3Rpb24gaXMgdGhlIGxvZ2FyaXRobSBvZiB0aGUgc29mdG1heCBmdW5jdGlvbiwgYW5kIGl0IGlzIG9mdGVuIHVzZWQgZm9yIG51bWVyaWNhbCBzdGFiaWxpdHkgd2hlbiBjb21wdXRpbmcgdGhlIHNvZnRtYXggb2YgbGFyZ2UgbnVtYmVycy4KCkdpdmVuIGEgMUQgbnVtcHkgYXJyYXkgb2Ygc2NvcmVzLCBpbXBsZW1lbnQgYSBQeXRob24gZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgbG9nLXNvZnRtYXggb2YgdGhlIGFycmF5Lg==",
    "starter_code": "import torch\nfrom typing import List\n\nclass Solution:\n    def log_softmax(self, scores: List[float]) -> torch.Tensor:\n        \"\"\"\n        Compute the log-softmax of a 1D list of scores using PyTorch.\n        Args:\n            scores: list of floats\n        Returns:\n            torch.Tensor of log-softmax values\n        \"\"\"\n        # Your code here\n        pass\n",
    "example_input": "A = np.array([1, 2, 3])\nprint(log_softmax(A))",
    "example_output": "array([-2.4076, -1.4076, -0.4076])",
    "example_reasoning": "The log-softmax function is applied to the input array [1, 2, 3]. The output array contains the log-softmax values for each element.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgTG9nIFNvZnRtYXggRnVuY3Rpb24KClRoZSBsb2cgc29mdG1heCBmdW5jdGlvbiBpcyBhIG51bWVyaWNhbGx5IHN0YWJsZSB3YXkgb2YgY2FsY3VsYXRpbmcgdGhlIGxvZ2FyaXRobSBvZiB0aGUgc29mdG1heCBmdW5jdGlvbi4gVGhlIHNvZnRtYXggZnVuY3Rpb24gY29udmVydHMgYSB2ZWN0b3Igb2YgYXJiaXRyYXJ5IHZhbHVlcyAobG9naXRzKSBpbnRvIGEgdmVjdG9yIG9mIHByb2JhYmlsaXRpZXMsIHdoZXJlIGVhY2ggdmFsdWUgbGllcyBiZXR3ZWVuIDAgYW5kIDEsIGFuZCB0aGUgdmFsdWVzIHN1bSB0byAxLgoKIyMjIFNvZnRtYXggRnVuY3Rpb24KVGhlIHNvZnRtYXggZnVuY3Rpb24gaXMgZ2l2ZW4gYnk6CiQkClx0ZXh0e3NvZnRtYXh9KHhfaSkgPSBcZnJhY3tlXnt4X2l9fXtcc3VtX3tqPTF9Xm4gZV57eF9qfX0KJCQKCiMjIyBMb2cgU29mdG1heCBGdW5jdGlvbgpEaXJlY3RseSBhcHBseWluZyB0aGUgbG9nYXJpdGhtIHRvIHRoZSBzb2Z0bWF4IGZ1bmN0aW9uIGNhbiBsZWFkIHRvIG51bWVyaWNhbCBpbnN0YWJpbGl0eSwgZXNwZWNpYWxseSB3aGVuIGRlYWxpbmcgd2l0aCBsYXJnZSBudW1iZXJzLiBUbyBwcmV2ZW50IHRoaXMsIHdlIHVzZSB0aGUgbG9nLXNvZnRtYXggZnVuY3Rpb24sIHdoaWNoIGluY29ycG9yYXRlcyBhIHNoaWZ0IGJ5IHN1YnRyYWN0aW5nIHRoZSBtYXhpbXVtIHZhbHVlIGZyb20gdGhlIGlucHV0IHZlY3RvcjoKJCQKXHRleHR7bG9nIHNvZnRtYXh9KHhfaSkgPSB4X2kgLSBcbWF4KHgpIC0gXGxvZ1xsZWZ0KFxzdW1fe2o9MX1ebiBlXnt4X2ogLSBcbWF4KHgpfVxyaWdodCkKJCQKClRoaXMgZm9ybXVsYXRpb24gaGVscHMgdG8gYXZvaWQgb3ZlcmZsb3cgaXNzdWVzIHRoYXQgY2FuIG9jY3VyIHdoZW4gZXhwb25lbnRpYXRpbmcgbGFyZ2UgbnVtYmVycy4gVGhlIGxvZy1zb2Z0bWF4IGZ1bmN0aW9uIGlzIHBhcnRpY3VsYXJseSB1c2VmdWwgaW4gbWFjaGluZSBsZWFybmluZyBmb3IgY2FsY3VsYXRpbmcgcHJvYmFiaWxpdGllcyBpbiBhIHN0YWJsZSBtYW5uZXIsIGVzcGVjaWFsbHkgd2hlbiB1c2VkIHdpdGggY3Jvc3MtZW50cm9weSBsb3NzIGZ1bmN0aW9ucy4K",
    "solution_code": "import torch\nfrom typing import List\n\nclass Solution:\n    def log_softmax(self, scores: List[float]) -> torch.Tensor:\n        scores = torch.tensor(scores, dtype=torch.float32)\n        scores = scores - torch.max(scores)\n        log_probs = scores - torch.log(torch.sum(torch.exp(scores)))\n        return torch.round(log_probs * 1e4) / 1e4  # rounding to 4 decimals for test case\n",
    "test_cases": [
      {
        "test": [
          [
            1,
            2,
            3
          ]
        ],
        "expected_output": [
          -2.4076,
          -1.4076,
          -0.4076
        ]
      },
      {
        "test": [
          [
            1,
            1,
            1
          ]
        ],
        "expected_output": [
          -1.0986,
          -1.0986,
          -1.0986
        ]
      }
    ],
    "function_name": "log_softmax"
  },
  {
    "id": 40,
    "title": "Implementing a Custom Dense Layer in Python",
    "difficulty": "hard",
    "category": "deep learning",
    "problem_description": "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",
    "starter_code": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom typing import Tuple\n\nclass Solution:\n    # DO NOT CHANGE SEED (used for deterministic init)\n    np.random.seed(42)\n\n    def _to_tensor(self, x: torch.Tensor | np.ndarray) -> torch.Tensor:\n        return x if isinstance(x, torch.Tensor) else torch.as_tensor(x, dtype=torch.float32)\n\n    class Layer:\n        def set_input_shape(self, self, shape: Tuple[int, ...]):\n            self.input_shape = shape\n\n        def layer_name(self) -> str:\n            return self.__class__.__name__\n\n        def parameters(self) -> int:\n            return 0\n\n        def forward_pass(self, self, X, training: bool = True):\n            raise NotImplementedError\n\n        def backward_pass(self, self, accum_grad):\n            raise NotImplementedError\n\n        def output_shape(self) -> Tuple[int, ...]:\n            raise NotImplementedError\n\n    class Dense(Layer):\n        def __init__(self, self, n_units: int, input_shape: Tuple[int, ...] | None = None):\n            self.layer_input = None\n            self.input_shape = input_shape\n            self.n_units = n_units\n            self.trainable = True\n            self.W = None  # torch.Tensor with requires_grad=True\n            self.b = None  # torch.Tensor with requires_grad=True\n            self._opt = None\n\n        def initialize(self, self, optimizer) -> None:\n            \"\"\"Initialize weights with a uniform distribution and biases with zeros.\n            Hint: use numpy to create deterministic arrays, then convert to torch tensors.\n            Also store the provided optimizer for updates.\n            \"\"\"\n            pass\n\n        def number_of_parameters(self) -> int:\n            \"\"\"Return total number of trainable parameters in W and b.\"\"\"\n            pass\n\n        def forward_pass(self, self, X, training: bool = True):\n            \"\"\"Use torch.nn.functional.linear for the forward pass (X @ W + b).\"\"\"\n            pass\n\n        def backward_pass(self, self, accum_grad):\n            \"\"\"Use torch.autograd.grad to get dL/dW, dL/db given upstream gradient.\n            Then update params with the provided optimizer and return grad w.r.t. input.\n            Hint: grad_input can be computed with a matmul against W.T.\n            \"\"\"\n            pass\n",
    "example_input": "# Initialize a Dense layer with 3 neurons and input shape (2,)\ndense_layer = Dense(n_units=3, input_shape=(2,))\n\n# Define a mock optimizer with a simple update rule\nclass MockOptimizer:\n    def update(self, weights, grad):\n        return weights - 0.01 * grad\n\noptimizer = MockOptimizer()\n\n# Initialize the Dense layer with the mock optimizer\ndense_layer.initialize(optimizer)\n\n# Perform a forward pass with sample input data\nX = np.array([[1, 2]])\noutput = dense_layer.forward_pass(X)\nprint(\"Forward pass output:\", output)\n\n# Perform a backward pass with sample gradient\naccum_grad = np.array([[0.1, 0.2, 0.3]])\nback_output = dense_layer.backward_pass(accum_grad)\nprint(\"Backward pass output:\", back_output)",
    "example_output": "Forward pass output: [[-0.00655782  0.01429615  0.00905812]]\nBackward pass output: [[ 0.00129588  0.00953634]]",
    "example_reasoning": "The code initializes a Dense layer with 3 neurons and input shape (2,). It then performs a forward pass with sample input data and a backward pass with sample gradients. The output demonstrates the forward and backward pass results.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyB0aGUgRGVuc2UgTGF5ZXIKClRoZSBEZW5zZSBsYXllciwgYWxzbyBrbm93biBhcyBhIGZ1bGx5IGNvbm5lY3RlZCBsYXllciwgaXMgYSBmdW5kYW1lbnRhbCBidWlsZGluZyBibG9jayBpbiBuZXVyYWwgbmV0d29ya3MuIEl0IGNvbm5lY3RzIGVhY2ggaW5wdXQgbmV1cm9uIHRvIGVhY2ggb3V0cHV0IG5ldXJvbiwgaGVuY2UgdGhlIHRlcm0gImZ1bGx5IGNvbm5lY3RlZC4iCgojIyMgMS4gV2VpZ2h0IEluaXRpYWxpemF0aW9uCkluIHRoZSBgaW5pdGlhbGl6ZWAgbWV0aG9kLCB3ZWlnaHRzIGFyZSB0eXBpY2FsbHkgaW5pdGlhbGl6ZWQgdXNpbmcgYSB1bmlmb3JtIGRpc3RyaWJ1dGlvbiB3aXRoaW4gYSBjZXJ0YWluIHJhbmdlLiBGb3IgYSBEZW5zZSBsYXllciwgYSBjb21tb24gcHJhY3RpY2UgaXMgdG8gc2V0IHRoaXMgcmFuZ2UgYXM6CiQkClx0ZXh0e2xpbWl0fSA9IFxmcmFjezF9e1xzcXJ0e1x0ZXh0e2lucHV0XF9zaGFwZX19fQokJApUaGlzIGluaXRpYWxpemF0aW9uIGhlbHBzIGluIG1haW50YWluaW5nIGEgYmFsYW5jZSBpbiB0aGUgZGlzdHJpYnV0aW9uIG9mIHdlaWdodHMsIHByZXZlbnRpbmcgaXNzdWVzIGxpa2UgdmFuaXNoaW5nIG9yIGV4cGxvZGluZyBncmFkaWVudHMgZHVyaW5nIHRyYWluaW5nLgoKIyMjIDIuIEZvcndhcmQgUGFzcwpEdXJpbmcgdGhlIGZvcndhcmQgcGFzcywgdGhlIGlucHV0IGRhdGEgXCggWCBcKSBpcyBtdWx0aXBsaWVkIGJ5IHRoZSB3ZWlnaHQgbWF0cml4IFwoIFcgXCkgYW5kIGFkZGVkIHRvIHRoZSBiaWFzIFwoIHdfMCBcKSB0byBwcm9kdWNlIHRoZSBvdXRwdXQ6CiQkClx0ZXh0e291dHB1dH0gPSBYIFxjZG90IFcgKyB3XzAKJCQKCiMjIyAzLiBCYWNrd2FyZCBQYXNzClRoZSBiYWNrd2FyZCBwYXNzIGNvbXB1dGVzIHRoZSBncmFkaWVudHMgb2YgdGhlIGxvc3MgZnVuY3Rpb24gd2l0aCByZXNwZWN0IHRvIHRoZSBpbnB1dCBkYXRhLCB3ZWlnaHRzLCBhbmQgYmlhc2VzLiBJZiB0aGUgbGF5ZXIgaXMgdHJhaW5hYmxlLCBpdCB1cGRhdGVzIHRoZSB3ZWlnaHRzIGFuZCBiaWFzZXMgdXNpbmcgdGhlIG9wdGltaXplcidzIHVwZGF0ZSBydWxlOgokJApXID0gVyAtIFxldGEgXGNkb3QgXHRleHR7Z3JhZH1fVwokJAokJAp3XzAgPSB3XzAgLSBcZXRhIFxjZG90IFx0ZXh0e2dyYWR9X3t3XzB9CiQkCndoZXJlIFwoIFxldGEgXCkgaXMgdGhlIGxlYXJuaW5nIHJhdGUgYW5kIFwoIFx0ZXh0e2dyYWR9X1cgXCkgYW5kIFwoIFx0ZXh0e2dyYWR9X3t3XzB9IFwpIGFyZSB0aGUgZ3JhZGllbnRzIG9mIHRoZSB3ZWlnaHRzIGFuZCBiaWFzZXMsIHJlc3BlY3RpdmVseS4KCiMjIyA0LiBPdXRwdXQgU2hhcGUKVGhlIHNoYXBlIG9mIHRoZSBvdXRwdXQgZnJvbSBhIERlbnNlIGxheWVyIGlzIGRldGVybWluZWQgYnkgdGhlIG51bWJlciBvZiBuZXVyb25zIGluIHRoZSBsYXllci4gSWYgYSBsYXllciBoYXMgYG5fdW5pdHNgIG5ldXJvbnMsIHRoZSBvdXRwdXQgc2hhcGUgd2lsbCBiZSBcKCAoblxfdW5pdHMsKSBcKS4KCiMjIyBSZXNvdXJjZXMKLSBbQ1MyMzFuOiBGdWxseSBDb25uZWN0ZWQgTGF5ZXJdKGh0dHBzOi8vY3MyMzFuLmdpdGh1Yi5pby9uZXVyYWwtbmV0d29ya3MtMi8jZmMpCgogICAg",
    "solution_code": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom typing import Tuple\n\nclass Solution:\n    # DO NOT CHANGE SEED (used for deterministic init)\n    np.random.seed(42)\n\n    def _to_tensor(self, x: torch.Tensor | np.ndarray) -> torch.Tensor:\n        return x if isinstance(x, torch.Tensor) else torch.as_tensor(x, dtype=torch.float32)\n\n    class Layer:\n        def set_input_shape(self, self, shape: Tuple[int, ...]):\n            self.input_shape = shape\n\n        def layer_name(self) -> str:\n            return self.__class__.__name__\n\n        def parameters(self) -> int:\n            return 0\n\n        def forward_pass(self, self, X, training: bool = True):\n            raise NotImplementedError\n\n        def backward_pass(self, self, accum_grad):\n            raise NotImplementedError\n\n        def output_shape(self) -> Tuple[int, ...]:\n            raise NotImplementedError\n\n    class Dense(Layer):\n        def __init__(self, self, n_units: int, input_shape: Tuple[int, ...] | None = None):\n            self.layer_input = None\n            self.input_shape = input_shape\n            self.n_units = n_units\n            self.trainable = True\n            self.W: torch.Tensor | None = None\n            self.b: torch.Tensor | None = None\n            self._opt = None\n\n        def initialize(self, self, optimizer) -> None:\n            in_features = self.input_shape[0]\n            limit = 1.0 / np.sqrt(in_features)\n            # deterministic init via numpy, then convert to torch and enable grads\n            W_np = np.random.uniform(-limit, limit, size=(in_features, self.n_units)).astype(np.float32)\n            b_np = np.zeros((1, self.n_units), dtype=np.float32)\n            self.W = torch.tensor(W_np, requires_grad=True)\n            self.b = torch.tensor(b_np, requires_grad=True)\n            self._opt = optimizer\n\n        def number_of_parameters(self) -> int:\n            return int(self.W.numel() + self.b.numel())\n\n        def forward_pass(self, self, X, training: bool = True):\n            X = _to_tensor(X)\n            self.layer_input = X\n            # built-in linear: Y = X @ W + b\n            return F.linear(X, self.W.t(), self.b.squeeze(0))\n\n        def backward_pass(self, self, accum_grad):\n            accum_grad = _to_tensor(accum_grad)\n            # Use autograd to compute gradients wrt W and b for the given upstream gradient\n            # Forward again to set graph, then backprop with custom grad_outputs\n            out = F.linear(self.layer_input, self.W.t(), self.b.squeeze(0))\n            dW, db = torch.autograd.grad(\n                outputs=out,\n                inputs=(self.W, self.b),\n                grad_outputs=accum_grad,\n                retain_graph=False,\n                create_graph=False,\n                allow_unused=False,\n            )\n            # Update parameters via provided optimizer using its built-in rule\n            if self.trainable and self._opt is not None:\n                with torch.no_grad():\n                    self.W.copy_(self._opt.update(self.W, dW))\n                    self.b.copy_(self._opt.update(self.b, db))\n            # Gradient w.r.t. input: use built-in matmul\n            grad_input = accum_grad @ self.W.t()\n            return grad_input\n",
    "test_cases": [
      {
        "test": [
          "dense_layer = Dense(n_units=3, input_shape=(2,))\n\nclass MockOptimizer:\n    def update(self, weights, grad):\n        return weights - 0.01 * grad\n\noptimizer = MockOptimizer()\ndense_layer.initialize(optimizer)\n\nX = np.array(torch.tensor([[1, 2]]))\noutput = dense_layer.forward_pass(X)\n\naccum_grad = np.array(torch.tensor([[0.1, 0.2, 0.3]]))\nback_output = dense_layer.backward_pass(accum_grad)\nprint(back_output)"
        ],
        "expected_output": [
          [
            0.20816524,
            -0.22928937
          ]
        ]
      }
    ],
    "function_name": "_to_tensor"
  },
  {
    "id": 41,
    "title": "Simple Convolutional 2D Layer",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW4gdGhpcyBwcm9ibGVtLCB5b3UgbmVlZCB0byBpbXBsZW1lbnQgYSAyRCBjb252b2x1dGlvbmFsIGxheWVyIGluIFB5dGhvbi4gVGhpcyBmdW5jdGlvbiB3aWxsIHByb2Nlc3MgYW4gaW5wdXQgbWF0cml4IHVzaW5nIGEgc3BlY2lmaWVkIGNvbnZvbHV0aW9uYWwga2VybmVsLCBwYWRkaW5nLCBhbmQgc3RyaWRlLgo=",
    "starter_code": "import torch\nimport torch.nn.functional as F\n\nclass Solution:\n    def simple_conv2d(self, input_matrix: torch.Tensor, kernel: torch.Tensor, padding: int, stride: int) -> torch.Tensor:\n        \"\"\"\n        Perform a 2D convolution on a single-channel input using PyTorch's built-in conv2d.\n        input_matrix: 2D tensor (H, W)\n        kernel: 2D tensor (kH, kW)\n        padding: int, zero-padding on all sides\n        stride: int, stride of the convolution\n        \"\"\"\n        # Hint: conv2d expects input of shape (N, C, H, W) and weight of shape (out_channels, in_channels, kH, kW)\n        pass\n",
    "example_input": "import numpy as np\n\ninput_matrix = np.array([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n])\n\nkernel = np.array([\n    [1, 0],\n    [-1, 1]\n])\n\npadding = 1\nstride = 2\n\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)",
    "example_output": "[[ 1.  1. -4.],[ 9.  7. -4.],[ 0. 14. 16.]]",
    "example_reasoning": "The function performs a 2D convolution operation on the input matrix using the specified kernel, padding, and stride. The output matrix contains the results of the convolution operation.",
    "learn_content": "IyMgU2ltcGxlIENvbnZvbHV0aW9uYWwgMkQgTGF5ZXIKClRoZSBDb252b2x1dGlvbmFsIGxheWVyIGlzIGEgZnVuZGFtZW50YWwgY29tcG9uZW50IHVzZWQgZXh0ZW5zaXZlbHkgaW4gQ29tcHV0ZXIgVmlzaW9uIHRhc2tzLiBIZXJlIGFyZSB0aGUgY3J1Y2lhbCBwYXJhbWV0ZXJzOgoKIyMjIFBhcmFtZXRlcnMKMS4gKippbnB1dF9tYXRyaXgqKjogIAogICBBIDJEIE51bVB5IGFycmF5IHJlcHJlc2VudGluZyB0aGUgaW5wdXQgZGF0YSwgc3VjaCBhcyBhbiBpbWFnZS4gRWFjaCBlbGVtZW50IGluIHRoaXMgYXJyYXkgY29ycmVzcG9uZHMgdG8gYSBwaXhlbCBvciBhIGZlYXR1cmUgdmFsdWUgaW4gdGhlIGlucHV0IHNwYWNlLiBUaGUgZGltZW5zaW9ucyBvZiB0aGUgaW5wdXQgbWF0cml4IGFyZSB0eXBpY2FsbHkgcmVwcmVzZW50ZWQgYXMgJCBcdGV4dHtoZWlnaHR9IFx0aW1lcyBcdGV4dHt3aWR0aH0gJC4KCjIuICoqa2VybmVsKio6ICAKICAgQW5vdGhlciAyRCBOdW1QeSBhcnJheSByZXByZXNlbnRpbmcgdGhlIGNvbnZvbHV0aW9uYWwgZmlsdGVyLiBUaGUga2VybmVsIGlzIHNtYWxsZXIgdGhhbiB0aGUgaW5wdXQgbWF0cml4IGFuZCBzbGlkZXMgb3ZlciBpdCB0byBwZXJmb3JtIHRoZSBjb252b2x1dGlvbiBvcGVyYXRpb24uIEVhY2ggZWxlbWVudCBpbiB0aGUga2VybmVsIHNlcnZlcyBhcyBhIHdlaWdodCB0aGF0IG1vZGlmaWVzIHRoZSBpbnB1dCBkdXJpbmcgY29udm9sdXRpb24uIFRoZSBrZXJuZWwgc2l6ZSBpcyBkZW5vdGVkIGFzICQgXHRleHR7a2VybmVsXF9oZWlnaHR9IFx0aW1lcyBcdGV4dHtrZXJuZWxcX3dpZHRofSAkLgoKMy4gKipwYWRkaW5nKio6ICAKICAgQW4gaW50ZWdlciBzcGVjaWZ5aW5nIHRoZSBudW1iZXIgb2Ygcm93cyBhbmQgY29sdW1ucyBvZiB6ZXJvcyBhZGRlZCBhcm91bmQgdGhlIGlucHV0IG1hdHJpeC4gUGFkZGluZyBjb250cm9scyB0aGUgc3BhdGlhbCBkaW1lbnNpb25zIG9mIHRoZSBvdXRwdXQsIGFsbG93aW5nIHRoZSBrZXJuZWwgdG8gcHJvY2VzcyBlZGdlIGVsZW1lbnRzIGVmZmVjdGl2ZWx5IG9yIHRvIG1haW50YWluIHRoZSBvcmlnaW5hbCBpbnB1dCBzaXplLgoKNC4gKipzdHJpZGUqKjogIAogICBBbiBpbnRlZ2VyIHRoYXQgcmVwcmVzZW50cyB0aGUgbnVtYmVyIG9mIHN0ZXBzIHRoZSBrZXJuZWwgbW92ZXMgYWNyb3NzIHRoZSBpbnB1dCBtYXRyaXggZm9yIGVhY2ggY29udm9sdXRpb24uIEEgc3RyaWRlIGdyZWF0ZXIgdGhhbiBvbmUgcmVkdWNlcyB0aGUgb3V0cHV0IHNpemUsIGFzIHRoZSBrZXJuZWwgc2tpcHMgb3ZlciBlbGVtZW50cy4KCiMjIyBJbXBsZW1lbnRhdGlvbgoxLiAqKlBhZGRpbmcgdGhlIElucHV0Kio6ICAKICAgVGhlIGlucHV0IG1hdHJpeCBpcyBwYWRkZWQgd2l0aCB6ZXJvcyBiYXNlZCBvbiB0aGUgc3BlY2lmaWVkIGBwYWRkaW5nYCB2YWx1ZS4gVGhpcyBpbmNyZWFzZXMgdGhlIGlucHV0IHNpemUgYW5kIGVuYWJsZXMgdGhlIGtlcm5lbCB0byBjb3ZlciBlbGVtZW50cyBhdCB0aGUgYm9yZGVycyBhbmQgY29ybmVycy4KCjIuICoqQ2FsY3VsYXRpbmcgT3V0cHV0IERpbWVuc2lvbnMqKjogIAogICBUaGUgaGVpZ2h0IGFuZCB3aWR0aCBvZiB0aGUgb3V0cHV0IG1hdHJpeCBhcmUgY2FsY3VsYXRlZCB1c2luZyB0aGUgZm9sbG93aW5nIGZvcm11bGFzOgogICAkJAogICBcdGV4dHtvdXRwdXRcX2hlaWdodH0gPSBcbGVmdCggXGZyYWN7XHRleHR7aW5wdXRcX2hlaWdodCwgcGFkZGVkfSAtIFx0ZXh0e2tlcm5lbFxfaGVpZ2h0fX17XHRleHR7c3RyaWRlfX0gXHJpZ2h0KSArIDEKICAgJCQKICAgJCQKICAgXHRleHR7b3V0cHV0XF93aWR0aH0gPSBcbGVmdCggXGZyYWN7XHRleHR7aW5wdXRcX3dpZHRoLCBwYWRkZWR9IC0gXHRleHR7a2VybmVsXF93aWR0aH19e1x0ZXh0e3N0cmlkZX19IFxyaWdodCkgKyAxCiAgICQkCgozLiAqKlBlcmZvcm1pbmcgQ29udm9sdXRpb24qKjoKICAgLSBBIG5lc3RlZCBsb29wIGl0ZXJhdGVzIG92ZXIgZWFjaCBwb3NpdGlvbiB3aGVyZSB0aGUga2VybmVsIGNhbiBiZSBhcHBsaWVkIHRvIHRoZSBwYWRkZWQgaW5wdXQgbWF0cml4LgogICAtIEF0IGVhY2ggcG9zaXRpb24sIGEgcmVnaW9uIG9mIHRoZSBpbnB1dCBtYXRyaXgsIG1hdGNoaW5nIHRoZSBzaXplIG9mIHRoZSBrZXJuZWwsIGlzIHNlbGVjdGVkLgogICAtIEVsZW1lbnQtd2lzZSBtdWx0aXBsaWNhdGlvbiBiZXR3ZWVuIHRoZSBrZXJuZWwgYW5kIHRoZSBpbnB1dCByZWdpb24gaXMgcGVyZm9ybWVkLCBmb2xsb3dlZCBieSBzdW1taW5nIHRoZSByZXN1bHRzIHRvIHByb2R1Y2UgYSBzaW5nbGUgdmFsdWUuIFRoaXMgdmFsdWUgaXMgdGhlbiBzdG9yZWQgaW4gdGhlIGNvcnJlc3BvbmRpbmcgcG9zaXRpb24gb2YgdGhlIG91dHB1dCBtYXRyaXguCgo0LiAqKk91dHB1dCoqOiAgCiAgIFRoZSBmdW5jdGlvbiByZXR1cm5zIHRoZSBvdXRwdXQgbWF0cml4LCB3aGljaCBjb250YWlucyB0aGUgcmVzdWx0cyBvZiB0aGUgY29udm9sdXRpb24gb3BlcmF0aW9uIHBlcmZvcm1lZCBhY3Jvc3MgdGhlIGVudGlyZSBpbnB1dC4KCg==",
    "solution_code": "import torch\nimport torch.nn.functional as F\n\nclass Solution:\n    def simple_conv2d(self, input_matrix: torch.Tensor, kernel: torch.Tensor, padding: int, stride: int) -> torch.Tensor:\n        # Ensure tensors are float32 and have correct shape for conv2d\n        if input_matrix.dim() == 2:\n            input_matrix = input_matrix.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n        if kernel.dim() == 2:\n            kernel = kernel.unsqueeze(0).unsqueeze(0)  # (1, 1, kH, kW)\n        input_matrix = input_matrix.to(dtype=torch.float32)\n        kernel = kernel.to(dtype=torch.float32)\n        # Use conv2d\n        output = F.conv2d(input_matrix, kernel, padding=padding, stride=stride)\n        return output.squeeze(0).squeeze(0)\n",
    "test_cases": [
      {
        "test": [
          "input_matrix = np.array([\n    torch.tensor([1., 2., 3., 4., 5.]),\n    torch.tensor([6., 7., 8., 9., 10.]),\n    torch.tensor([11., 12., 13., 14., 15.]),\n    torch.tensor([16., 17., 18., 19., 20.]),\n    torch.tensor([21., 22., 23., 24., 25.]),\n])\nkernel = np.array([\n    torch.tensor([1., 2.]),\n    torch.tensor([3., -1.]),\n])\npadding, stride = 0, 1\nexpected = np.array([\n    [ 16., 21., 26., 31.],\n    [ 41., 46., 51., 56.],\n    [ 66., 71., 76., 81.],\n    [ 91., 96., 101., 106.],\n])\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)"
        ],
        "expected_output": [
          [
            16.0,
            21.0,
            26.0,
            31.0
          ],
          [
            41.0,
            46.0,
            51.0,
            56.0
          ],
          [
            66.0,
            71.0,
            76.0,
            81.0
          ],
          [
            91.0,
            96.0,
            101.0,
            106.0
          ]
        ]
      },
      {
        "test": [
          "input_matrix = np.array([\n    torch.tensor([1., 2., 3., 4., 5.]),\n    torch.tensor([6., 7., 8., 9., 10.]),\n    torch.tensor([11., 12., 13., 14., 15.]),\n    torch.tensor([16., 17., 18., 19., 20.]),\n    torch.tensor([21., 22., 23., 24., 25.]),\n])\nkernel = np.array([\n    [.5, 3.2],\n    torch.tensor([1., -1.]),\n])\npadding, stride = 2, 2\nexpected = np.array([\n        [ -1., 1., 3., 5., 7., 15.],\n        [ -4., 16., 21., 26., 31., 35.],\n        [  1., 41., 46., 51., 56., 55.],\n        [  6., 66., 71., 76., 81., 75.],\n        [ 11., 91., 96., 101., 106., 95.],\n        [ 42., 65., 68., 71., 74.,  25.],\n    ])\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)"
        ],
        "expected_output": [
          [
            0.0,
            0.0,
            0.0,
            0.0
          ],
          [
            0.0,
            5.9,
            13.3,
            12.5
          ],
          [
            0.0,
            42.9,
            50.3,
            27.5
          ],
          [
            0.0,
            80.9,
            88.3,
            12.5
          ]
        ]
      }
    ],
    "function_name": "simple_conv2d"
  },
  {
    "id": 42,
    "title": "Implement ReLU Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYHJlbHVgIHRoYXQgaW1wbGVtZW50cyB0aGUgUmVjdGlmaWVkIExpbmVhciBVbml0IChSZUxVKSBhY3RpdmF0aW9uIGZ1bmN0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBzaW5nbGUgZmxvYXQgYXMgaW5wdXQgYW5kIHJldHVybiB0aGUgdmFsdWUgYWZ0ZXIgYXBwbHlpbmcgdGhlIFJlTFUgZnVuY3Rpb24uIFRoZSBSZUxVIGZ1bmN0aW9uIHJldHVybnMgdGhlIGlucHV0IGlmIGl0J3MgZ3JlYXRlciB0aGFuIDAsIG90aGVyd2lzZSwgaXQgcmV0dXJucyAwLg==",
    "starter_code": "\n\nclass Solution:\n    def relu(self, z: float) -> float:\n    \t# Your code here\n    \tpass\n",
    "example_input": "print(relu(0)) \nprint(relu(1)) \nprint(relu(-1))",
    "example_output": "0\n1\n0",
    "example_reasoning": "The ReLU function is applied to the input values 0, 1, and -1. The output is 0 for negative values and the input value for non-negative values.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgdGhlIFJlTFUgQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIFJlTFUgKFJlY3RpZmllZCBMaW5lYXIgVW5pdCkgYWN0aXZhdGlvbiBmdW5jdGlvbiBpcyB3aWRlbHkgdXNlZCBpbiBuZXVyYWwgbmV0d29ya3MsIHBhcnRpY3VsYXJseSBpbiBoaWRkZW4gbGF5ZXJzIG9mIGRlZXAgbGVhcm5pbmcgbW9kZWxzLiBJdCBtYXBzIGFueSByZWFsLXZhbHVlZCBudW1iZXIgdG8gdGhlIG5vbi1uZWdhdGl2ZSByYW5nZSAkWzAsIFxpbmZ0eSkkLCB3aGljaCBoZWxwcyBpbnRyb2R1Y2Ugbm9uLWxpbmVhcml0eSBpbnRvIHRoZSBtb2RlbCB3aGlsZSBtYWludGFpbmluZyBjb21wdXRhdGlvbmFsIGVmZmljaWVuY3kuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KVGhlIFJlTFUgZnVuY3Rpb24gaXMgbWF0aGVtYXRpY2FsbHkgZGVmaW5lZCBhczoKJCQKZih6KSA9IFxtYXgoMCwgeikKJCQKd2hlcmUgJHokIGlzIHRoZSBpbnB1dCB0byB0aGUgZnVuY3Rpb24uCgojIyMgQ2hhcmFjdGVyaXN0aWNzCi0gKipPdXRwdXQgUmFuZ2UqKjogVGhlIG91dHB1dCBpcyBhbHdheXMgaW4gdGhlIHJhbmdlICRbMCwgXGluZnR5KSQuIFZhbHVlcyBiZWxvdyAwIGFyZSBtYXBwZWQgdG8gMCwgd2hpbGUgcG9zaXRpdmUgdmFsdWVzIGFyZSByZXRhaW5lZC4KLSAqKlNoYXBlKio6IFRoZSBmdW5jdGlvbiBoYXMgYW4gIkwiIHNoYXBlZCBjdXJ2ZSB3aXRoIGEgaG9yaXpvbnRhbCBheGlzIGF0ICR5ID0gMCQgYW5kIGEgbGluZWFyIGluY3JlYXNlIGZvciBwb3NpdGl2ZSAkeiQuCi0gKipHcmFkaWVudCoqOiBUaGUgZ3JhZGllbnQgaXMgMSBmb3IgcG9zaXRpdmUgdmFsdWVzIG9mICR6JCBhbmQgMCBmb3Igbm9uLXBvc2l0aXZlIHZhbHVlcy4gVGhpcyBtZWFucyB0aGUgZnVuY3Rpb24gaXMgbGluZWFyIGZvciBwb3NpdGl2ZSBpbnB1dHMgYW5kIGZsYXQgKHplcm8gZ3JhZGllbnQpIGZvciBuZWdhdGl2ZSBpbnB1dHMuCgpUaGlzIGZ1bmN0aW9uIGlzIHBhcnRpY3VsYXJseSB1c2VmdWwgaW4gZGVlcCBsZWFybmluZyBtb2RlbHMgYXMgaXQgaW50cm9kdWNlcyBub24tbGluZWFyaXR5IHdoaWxlIGJlaW5nIGNvbXB1dGF0aW9uYWxseSBlZmZpY2llbnQsIGhlbHBpbmcgdG8gY2FwdHVyZSBjb21wbGV4IHBhdHRlcm5zIGluIHRoZSBkYXRhLgo=",
    "solution_code": "\n\nclass Solution:\n    def relu(self, z: float) -> float:\n        return max(0, z)\n",
    "test_cases": [
      {
        "test": [
          0
        ],
        "expected_output": 0
      },
      {
        "test": [
          1
        ],
        "expected_output": 1
      }
    ],
    "function_name": "relu"
  },
  {
    "id": 43,
    "title": "Implement Ridge Regression Loss Function",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYHJpZGdlX2xvc3NgIHRoYXQgaW1wbGVtZW50cyB0aGUgUmlkZ2UgUmVncmVzc2lvbiBsb3NzIGZ1bmN0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSAyRCBudW1weSBhcnJheSBgWGAgcmVwcmVzZW50aW5nIHRoZSBmZWF0dXJlIG1hdHJpeCwgYSAxRCBudW1weSBhcnJheSBgd2AgcmVwcmVzZW50aW5nIHRoZSBjb2VmZmljaWVudHMsIGEgMUQgbnVtcHkgYXJyYXkgYHlfdHJ1ZWAgcmVwcmVzZW50aW5nIHRoZSB0cnVlIGxhYmVscywgYW5kIGEgZmxvYXQgYGFscGhhYCByZXByZXNlbnRpbmcgdGhlIHJlZ3VsYXJpemF0aW9uIHBhcmFtZXRlci4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gdGhlIFJpZGdlIGxvc3MsIHdoaWNoIGNvbWJpbmVzIHRoZSBNZWFuIFNxdWFyZWQgRXJyb3IgKE1TRSkgYW5kIGEgcmVndWxhcml6YXRpb24gdGVybS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def ridge_loss(self, X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \t# Your code here\n    \tpass\n",
    "example_input": "import numpy as np\n\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\nw = np.array([0.2, 2])\ny_true = np.array([2, 3, 4, 5])\nalpha = 0.1\n\nloss = ridge_loss(X, w, y_true, alpha)\nprint(loss)",
    "example_output": "2.204",
    "example_reasoning": "The Ridge loss is calculated using the Mean Squared Error (MSE) and a regularization term. The output represents the combined loss value.",
    "learn_content": "CiMjIFJpZGdlIFJlZ3Jlc3Npb24gTG9zcwoKUmlkZ2UgUmVncmVzc2lvbiBpcyBhIGxpbmVhciByZWdyZXNzaW9uIG1ldGhvZCB3aXRoIGEgcmVndWxhcml6YXRpb24gdGVybSB0byBwcmV2ZW50IG92ZXJmaXR0aW5nIGJ5IGNvbnRyb2xsaW5nIHRoZSBzaXplIG9mIHRoZSBjb2VmZmljaWVudHMuCgojIyMgS2V5IENvbmNlcHRzOgoxLiAqKlJlZ3VsYXJpemF0aW9uKio6ICAKICAgQWRkcyBhIHBlbmFsdHkgdG8gdGhlIGxvc3MgZnVuY3Rpb24gdG8gZGlzY291cmFnZSBsYXJnZSBjb2VmZmljaWVudHMsIGhlbHBpbmcgdG8gZ2VuZXJhbGl6ZSB0aGUgbW9kZWwuCgoyLiAqKk1lYW4gU3F1YXJlZCBFcnJvciAoTVNFKSoqOiAgCiAgIE1lYXN1cmVzIHRoZSBhdmVyYWdlIHNxdWFyZWQgZGlmZmVyZW5jZSBiZXR3ZWVuIGFjdHVhbCBhbmQgcHJlZGljdGVkIHZhbHVlcy4KCjMuICoqUGVuYWx0eSBUZXJtKio6ICAKICAgVGhlIHN1bSBvZiB0aGUgc3F1YXJlZCBjb2VmZmljaWVudHMsIHNjYWxlZCBieSB0aGUgcmVndWxhcml6YXRpb24gcGFyYW1ldGVyICQgXGxhbWJkYSAkLCB3aGljaCBjb250cm9scyB0aGUgc3RyZW5ndGggb2YgdGhlIHJlZ3VsYXJpemF0aW9uLgoKIyMjIFJpZGdlIExvc3MgRnVuY3Rpb24KVGhlIFJpZGdlIExvc3MgZnVuY3Rpb24gY29tYmluZXMgTVNFIGFuZCB0aGUgcGVuYWx0eSB0ZXJtOgokJApMKFxiZXRhKSA9IFxmcmFjezF9e259IFxzdW1fe2k9MX1ee259ICh5X2kgLSBcaGF0e3l9X2kpXjIgKyBcbGFtYmRhIFxzdW1fe2o9MX1ee3B9IFxiZXRhX2peMgokJAoKIyMjIEltcGxlbWVudGF0aW9uIFN0ZXBzOgoxLiAqKkNhbGN1bGF0ZSBNU0UqKjogIAogICBDb21wdXRlIHRoZSBhdmVyYWdlIHNxdWFyZWQgZGlmZmVyZW5jZSBiZXR3ZWVuIGFjdHVhbCBhbmQgcHJlZGljdGVkIHZhbHVlcy4KCjIuICoqQWRkIFJlZ3VsYXJpemF0aW9uIFRlcm0qKjogIAogICBDb21wdXRlIHRoZSBzdW0gb2Ygc3F1YXJlZCBjb2VmZmljaWVudHMgbXVsdGlwbGllZCBieSAkIFxsYW1iZGEgJC4KCjMuICoqQ29tYmluZSBhbmQgTWluaW1pemUqKjogIAogICBTdW0gTVNFIGFuZCB0aGUgcmVndWxhcml6YXRpb24gdGVybSB0byBmb3JtIHRoZSBSaWRnZSBsb3NzLCB0aGVuIG1pbmltaXplIHRoaXMgbG9zcyB0byBmaW5kIHRoZSBvcHRpbWFsIGNvZWZmaWNpZW50cy4KCg==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def ridge_loss(self, X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n        loss = np.mean((y_true - X @ w)**2) + alpha * np.sum(w**2)\n        return loss\n",
    "test_cases": [
      {
        "test": [
          "X = np.array([[1,1],[2,1],[3,1],[4,1]])\nW = np.array([.2,2])\ny = np.array([2,3,4,5])\nalpha = 0.1\noutput = ridge_loss(X, W, y, alpha)\nprint(output)"
        ],
        "expected_output": 2.204
      },
      {
        "test": [
          "X = np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]])\nW = np.array([.2,2,5])\ny = np.array([2,3,4,5,2])\nalpha = 0.1\noutput = ridge_loss(X, W, y, alpha)\nprint(output)"
        ],
        "expected_output": 164.402
      }
    ],
    "function_name": "ridge_loss"
  },
  {
    "id": 44,
    "title": "Leaky ReLU Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYGxlYWt5X3JlbHVgIHRoYXQgaW1wbGVtZW50cyB0aGUgTGVha3kgUmVjdGlmaWVkIExpbmVhciBVbml0IChMZWFreSBSZUxVKSBhY3RpdmF0aW9uIGZ1bmN0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBmbG9hdCBgemAgYXMgaW5wdXQgYW5kIGFuIG9wdGlvbmFsIGZsb2F0IGBhbHBoYWAsIHdpdGggYSBkZWZhdWx0IHZhbHVlIG9mIDAuMDEsIGFzIHRoZSBzbG9wZSBmb3IgbmVnYXRpdmUgaW5wdXRzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgdmFsdWUgYWZ0ZXIgYXBwbHlpbmcgdGhlIExlYWt5IFJlTFUgZnVuY3Rpb24u",
    "starter_code": "\n\nclass Solution:\n    def leaky_relu(self, z: float, alpha: float = 0.01) -> float|int:\n    \t# Your code here\n    \tpass\n",
    "example_input": "print(leaky_relu(0)) \nprint(leaky_relu(1))\nprint(leaky_relu(-1)) \nprint(leaky_relu(-2, alpha=0.1))",
    "example_output": "0\n1\n-0.01\n-0.2",
    "example_reasoning": "- For z = 0, the output is 0.\n- For z = 1, the output is 1.\n- For z = -1, the output is -0.01 (0.01 * -1).\n- For z = -2 with alpha = 0.1, the output is -0.2 (0.1 * -2).",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgdGhlIExlYWt5IFJlTFUgQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIExlYWt5IFJlTFUgKExlYWt5IFJlY3RpZmllZCBMaW5lYXIgVW5pdCkgYWN0aXZhdGlvbiBmdW5jdGlvbiBpcyBhIHZhcmlhbnQgb2YgdGhlIFJlTFUgZnVuY3Rpb24gdXNlZCBpbiBuZXVyYWwgbmV0d29ya3MuIEl0IGFkZHJlc3NlcyB0aGUgImR5aW5nIFJlTFUiIHByb2JsZW0gYnkgYWxsb3dpbmcgYSBzbWFsbCwgbm9uLXplcm8gZ3JhZGllbnQgd2hlbiB0aGUgaW5wdXQgaXMgbmVnYXRpdmUuIFRoaXMgc21hbGwgc2xvcGUgZm9yIG5lZ2F0aXZlIGlucHV0cyBoZWxwcyBrZWVwIHRoZSBmdW5jdGlvbiBhY3RpdmUgYW5kIHByZXZlbnRzIG5ldXJvbnMgZnJvbSBiZWNvbWluZyBpbmFjdGl2ZS4KCiMjIyBNYXRoZW1hdGljYWwgRGVmaW5pdGlvbgpUaGUgTGVha3kgUmVMVSBmdW5jdGlvbiBpcyBtYXRoZW1hdGljYWxseSBkZWZpbmVkIGFzOgokJApmKHopID0gXGJlZ2lue2Nhc2VzfSAKeiAmIFx0ZXh0e2lmIH0geiA+IDAgXFwgClxhbHBoYSB6ICYgXHRleHR7aWYgfSB6IFxsZXEgMCAKXGVuZHtjYXNlc30KJCQKd2hlcmUgJHokIGlzIHRoZSBpbnB1dCB0byB0aGUgZnVuY3Rpb24gYW5kICRcYWxwaGEkIGlzIGEgc21hbGwgcG9zaXRpdmUgY29uc3RhbnQsIHR5cGljYWxseSAkXGFscGhhID0gMC4wMSQuCgpJbiB0aGlzIGRlZmluaXRpb24sIHRoZSBmdW5jdGlvbiByZXR1cm5zICR6JCBmb3IgcG9zaXRpdmUgdmFsdWVzLCBhbmQgZm9yIG5lZ2F0aXZlIHZhbHVlcywgaXQgcmV0dXJucyAkXGFscGhhIHokLCBhbGxvd2luZyBhIHNtYWxsIGdyYWRpZW50IHRvIHBhc3MgdGhyb3VnaC4KCiMjIyBDaGFyYWN0ZXJpc3RpY3MKLSAqKk91dHB1dCBSYW5nZSoqOiBUaGUgb3V0cHV0IGlzIGluIHRoZSByYW5nZSAkKC1caW5mdHksIFxpbmZ0eSkkLiBQb3NpdGl2ZSB2YWx1ZXMgYXJlIHJldGFpbmVkLCB3aGlsZSBuZWdhdGl2ZSB2YWx1ZXMgYXJlIHNjYWxlZCBieSB0aGUgZmFjdG9yICRcYWxwaGEkLCBhbGxvd2luZyB0aGVtIHRvIGJlIHNsaWdodGx5IG5lZ2F0aXZlLgotICoqU2hhcGUqKjogVGhlIGZ1bmN0aW9uIGhhcyBhIHNpbWlsYXIgIkwiIHNoYXBlZCBjdXJ2ZSBhcyBSZUxVLCBidXQgd2l0aCBhIHNtYWxsIG5lZ2F0aXZlIHNsb3BlIG9uIHRoZSBsZWZ0IHNpZGUgZm9yIG5lZ2F0aXZlICR6JCwgY3JlYXRpbmcgYSBzbWFsbCBncmFkaWVudCBmb3IgbmVnYXRpdmUgaW5wdXRzLgotICoqR3JhZGllbnQqKjogVGhlIGdyYWRpZW50IGlzIDEgZm9yIHBvc2l0aXZlIHZhbHVlcyBvZiAkeiQgYW5kICRcYWxwaGEkIGZvciBub24tcG9zaXRpdmUgdmFsdWVzLiBUaGlzIGFsbG93cyB0aGUgZnVuY3Rpb24gdG8gcmVtYWluIGFjdGl2ZSBldmVuIGZvciBuZWdhdGl2ZSBpbnB1dHMsIHVubGlrZSBSZUxVLCB3aGVyZSB0aGUgZ3JhZGllbnQgaXMgemVybyBmb3IgbmVnYXRpdmUgaW5wdXRzLgoKVGhpcyBmdW5jdGlvbiBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIGluIGRlZXAgbGVhcm5pbmcgbW9kZWxzIGFzIGl0IG1pdGlnYXRlcyB0aGUgaXNzdWUgb2YgImRlYWQgbmV1cm9ucyIgaW4gUmVMVSBieSBlbnN1cmluZyB0aGF0IG5ldXJvbnMgY2FuIHN0aWxsIHByb3BhZ2F0ZSBhIGdyYWRpZW50IGV2ZW4gd2hlbiB0aGUgaW5wdXQgaXMgbmVnYXRpdmUsIGhlbHBpbmcgdG8gaW1wcm92ZSBsZWFybmluZyBkeW5hbWljcyBpbiB0aGUgbmV0d29yay4K",
    "solution_code": "\n\nclass Solution:\n    def leaky_relu(self, z: float, alpha: float = 0.01) -> float|int:\n        return z if z > 0 else alpha * z\n",
    "test_cases": [
      {
        "test": [
          5
        ],
        "expected_output": 5
      },
      {
        "test": [
          1
        ],
        "expected_output": 1
      }
    ],
    "function_name": "leaky_relu"
  },
  {
    "id": 45,
    "title": "Linear Kernel Function",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYGtlcm5lbF9mdW5jdGlvbmAgdGhhdCBjb21wdXRlcyB0aGUgbGluZWFyIGtlcm5lbCBiZXR3ZWVuIHR3byBpbnB1dCB2ZWN0b3JzIGB4MWAgYW5kIGB4MmAuIFRoZSBsaW5lYXIga2VybmVsIGlzIGRlZmluZWQgYXMgdGhlIGRvdCBwcm9kdWN0IChpbm5lciBwcm9kdWN0KSBvZiB0d28gdmVjdG9ycy4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def kernel_function(self, x1, x2):\n    \t# Your code here\n    \tpass\n",
    "example_input": "import numpy as np\n\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\n\nresult = kernel_function(x1, x2)\nprint(result)",
    "example_output": "32",
    "example_reasoning": "The linear kernel between x1 and x2 is computed as:1\\*4 + 2\\*5 + 3\\*6 = 32",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def kernel_function(self, x1, x2):\n        return np.inner(x1, x2)\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\nresult = kernel_function(x1, x2)\nprint(result)"
        ],
        "expected_output": 32
      },
      {
        "test": [
          "import numpy as np\nx1 = np.array([0, 1, 2])\nx2 = np.array([3, 4, 5])\nresult = kernel_function(x1, x2)\nprint(result)"
        ],
        "expected_output": 14
      }
    ],
    "function_name": "kernel_function"
  },
  {
    "id": 46,
    "title": "Implement Precision Metric",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYHByZWNpc2lvbmAgdGhhdCBjYWxjdWxhdGVzIHRoZSBwcmVjaXNpb24gbWV0cmljIGdpdmVuIHR3byBudW1weSBhcnJheXM6IGB5X3RydWVgIGFuZCBgeV9wcmVkYC4gVGhlIGB5X3RydWVgIGFycmF5IGNvbnRhaW5zIHRoZSB0cnVlIGJpbmFyeSBsYWJlbHMsIGFuZCB0aGUgYHlfcHJlZGAgYXJyYXkgY29udGFpbnMgdGhlIHByZWRpY3RlZCBiaW5hcnkgbGFiZWxzLiBQcmVjaXNpb24gaXMgZGVmaW5lZCBhcyB0aGUgcmF0aW8gb2YgdHJ1ZSBwb3NpdGl2ZXMgdG8gdGhlIHN1bSBvZiB0cnVlIHBvc2l0aXZlcyBhbmQgZmFsc2UgcG9zaXRpdmVzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def precision(self, y_true, y_pred):\n    \t# Your code here\n    \tpass\n",
    "example_input": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\n\nresult = precision(y_true, y_pred)\nprint(result)",
    "example_output": "1.0",
    "example_reasoning": "- True Positives (TP) = 3\n- False Positives (FP) = 0\n- Precision = TP / (TP + FP) = 3 / (3 + 0) = 1.0",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgUHJlY2lzaW9uIGluIENsYXNzaWZpY2F0aW9uCgpQcmVjaXNpb24gaXMgYSBrZXkgbWV0cmljIHVzZWQgaW4gdGhlIGV2YWx1YXRpb24gb2YgY2xhc3NpZmljYXRpb24gbW9kZWxzLCBwYXJ0aWN1bGFybHkgaW4gYmluYXJ5IGNsYXNzaWZpY2F0aW9uLiBJdCBwcm92aWRlcyBpbnNpZ2h0IGludG8gdGhlIGFjY3VyYWN5IG9mIHRoZSBwb3NpdGl2ZSBwcmVkaWN0aW9ucyBtYWRlIGJ5IHRoZSBtb2RlbC4KCiMjIyBNYXRoZW1hdGljYWwgRGVmaW5pdGlvbgpQcmVjaXNpb24gaXMgZGVmaW5lZCBhcyB0aGUgcmF0aW8gb2YgdHJ1ZSBwb3NpdGl2ZXMgKFRQKSB0byB0aGUgc3VtIG9mIHRydWUgcG9zaXRpdmVzIGFuZCBmYWxzZSBwb3NpdGl2ZXMgKEZQKToKJCQKXHRleHR7UHJlY2lzaW9ufSA9IFxmcmFje1x0ZXh0e1RQfX17XHRleHR7VFB9ICsgXHRleHR7RlB9fQokJAoKV2hlcmU6Ci0gKipUcnVlIFBvc2l0aXZlcyAoVFApKio6IFRoZSBudW1iZXIgb2YgcG9zaXRpdmUgc2FtcGxlcyB0aGF0IGFyZSBjb3JyZWN0bHkgaWRlbnRpZmllZCBhcyBwb3NpdGl2ZS4KLSAqKkZhbHNlIFBvc2l0aXZlcyAoRlApKio6IFRoZSBudW1iZXIgb2YgbmVnYXRpdmUgc2FtcGxlcyB0aGF0IGFyZSBpbmNvcnJlY3RseSBpZGVudGlmaWVkIGFzIHBvc2l0aXZlLgoKIyMjIENoYXJhY3RlcmlzdGljcyBvZiBQcmVjaXNpb24KLSAqKlJhbmdlKio6IFByZWNpc2lvbiByYW5nZXMgZnJvbSAwIHRvIDEsIHdoZXJlIDEgaW5kaWNhdGVzIHBlcmZlY3QgcHJlY2lzaW9uIChubyBmYWxzZSBwb3NpdGl2ZXMpIGFuZCAwIGluZGljYXRlcyBubyB0cnVlIHBvc2l0aXZlcy4KLSAqKkludGVycHJldGF0aW9uKio6IEhpZ2ggcHJlY2lzaW9uIG1lYW5zIHRoYXQgdGhlIG1vZGVsIGhhcyBhIGxvdyBmYWxzZSBwb3NpdGl2ZSByYXRlLCBtZWFuaW5nIGl0IHJhcmVseSBsYWJlbHMgbmVnYXRpdmUgc2FtcGxlcyBhcyBwb3NpdGl2ZS4KLSAqKlVzZSBDYXNlKio6IFByZWNpc2lvbiBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIHdoZW4gdGhlIGNvc3Qgb2YgZmFsc2UgcG9zaXRpdmVzIGlzIGhpZ2gsIHN1Y2ggYXMgaW4gbWVkaWNhbCBkaWFnbm9zaXMgb3IgZnJhdWQgZGV0ZWN0aW9uLgoKSW4gdGhpcyBwcm9ibGVtLCB5b3Ugd2lsbCBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0byBjYWxjdWxhdGUgcHJlY2lzaW9uIGdpdmVuIHRoZSB0cnVlIGxhYmVscyBhbmQgcHJlZGljdGVkIGxhYmVscyBvZiBhIGJpbmFyeSBjbGFzc2lmaWNhdGlvbiB0YXNrLgo=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def precision(self, y_true, y_pred):\n        true_positives = np.sum((y_true == 1) & (y_pred == 1))\n        false_positives = np.sum((y_true == 0) & (y_pred == 1))\n        return true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nresult = precision(y_true, y_pred)\nprint(result)"
        ],
        "expected_output": 1.0
      },
      {
        "test": [
          "import numpy as np\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([1, 0, 0, 0, 0, 1])\nresult = precision(y_true, y_pred)\nprint(result)"
        ],
        "expected_output": 0.5
      }
    ],
    "function_name": "precision"
  },
  {
    "id": 47,
    "title": "Implement Gradient Descent Variants with MSE Loss",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SW4gdGhpcyBwcm9ibGVtLCB5b3UgbmVlZCB0byBpbXBsZW1lbnQgYSBzaW5nbGUgZnVuY3Rpb24gdGhhdCBjYW4gcGVyZm9ybSB0aHJlZSB2YXJpYW50cyBvZiBncmFkaWVudCBkZXNjZW50IFN0b2NoYXN0aWMgR3JhZGllbnQgRGVzY2VudCAoU0dEKSwgQmF0Y2ggR3JhZGllbnQgRGVzY2VudCwgYW5kIE1pbmkgQmF0Y2ggR3JhZGllbnQgRGVzY2VudCB1c2luZyBNZWFuIFNxdWFyZWQgRXJyb3IgKE1TRSkgYXMgdGhlIGxvc3MgZnVuY3Rpb24uIFRoZSBmdW5jdGlvbiB3aWxsIHRha2UgYW4gYWRkaXRpb25hbCBwYXJhbWV0ZXIgdG8gc3BlY2lmeSB3aGljaCB2YXJpYW50IHRvIHVzZS4gTm90ZTogRG8gbm90IHNodWZmbGUgdGhlIGRhdGEK",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def gradient_descent(self, X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \t# Your code here\n    \tpass\n",
    "example_input": "import numpy as np\n\n# Sample data\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\ny = np.array([2, 3, 4, 5])\n\n# Parameters\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2\n\n# Initialize weights\nweights = np.zeros(X.shape[1])\n\n# Test Batch Gradient Descent\nfinal_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n# Test Stochastic Gradient Descent\nfinal_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n# Test Mini-Batch Gradient Descent\nfinal_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')",
    "example_output": "[float,float]\n[float, float]\n[float, float]",
    "example_reasoning": "The function should return the final weights after performing the specified variant of gradient descent.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def gradient_descent(self, X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n        m = len(y)\n\n        for _ in range(n_iterations):\n            if method == 'batch':\n                # Calculate the gradient using all data points\n                predictions = X.dot(weights)\n                errors = predictions - y\n                gradient = 2 * X.T.dot(errors) / m\n                weights = weights - learning_rate * gradient\n\n            elif method == 'stochastic':\n                # Update weights for each data point individually\n                for i in range(m):\n                    prediction = X[i].dot(weights)\n                    error = prediction - y[i]\n                    gradient = 2 * X[i].T.dot(error)\n                    weights = weights - learning_rate * gradient\n\n            elif method == 'mini_batch':\n                # Update weights using sequential batches of data points without shuffling\n                for i in range(0, m, batch_size):\n                    X_batch = X[i:i+batch_size]\n                    y_batch = y[i:i+batch_size]\n                    predictions = X_batch.dot(weights)\n                    errors = predictions - y_batch\n                    gradient = 2 * X_batch.T.dot(errors) / batch_size\n                    weights = weights - learning_rate * gradient\n\n        return weights\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\ny = np.array([2, 3, 4, 5])\nweights = np.zeros(X.shape[1])\nlearning_rate = 0.01\nn_iterations = 100\n\n# Test Batch Gradient Descent\noutput = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\nprint(output)"
        ],
        "expected_output": "[1.14905239 0.56176776]"
      },
      {
        "test": [
          "import numpy as np\n\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\ny = np.array([2, 3, 4, 5])\nweights = np.zeros(X.shape[1])\nlearning_rate = 0.01\nn_iterations = 100\n\n# Test Stochastic Gradient Descent\noutput = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\nprint(output)"
        ],
        "expected_output": "[1.0507814  0.83659454]"
      }
    ],
    "function_name": "gradient_descent"
  },
  {
    "id": 48,
    "title": "Implement Reduced Row Echelon Form (RREF) Function",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "SW4gdGhpcyBwcm9ibGVtLCB5b3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjb252ZXJ0cyBhIGdpdmVuIG1hdHJpeCBpbnRvIGl0cyBSZWR1Y2VkIFJvdyBFY2hlbG9uIEZvcm0gKFJSRUYpLiBUaGUgUlJFRiBvZiBhIG1hdHJpeCBpcyBhIHNwZWNpYWwgZm9ybSB3aGVyZSBlYWNoIGxlYWRpbmcgZW50cnkgaW4gYSByb3cgaXMgMSwgYW5kIGFsbCBvdGhlciBlbGVtZW50cyBpbiB0aGUgY29sdW1uIGNvbnRhaW5pbmcgdGhlIGxlYWRpbmcgMSBhcmUgemVyb3MsIGV4Y2VwdCBmb3IgdGhlIGxlYWRpbmcgMSBpdHNlbGYuCgpIb3dldmVyLCB0aGVyZSBhcmUgc29tZSBhZGRpdGlvbmFsIGRldGFpbHMgdG8ga2VlcCBpbiBtaW5kOgoKLSBEaWFnb25hbCBlbnRyaWVzIGNhbiBiZSAwIGlmIHRoZSBtYXRyaXggaXMgcmVkdWNpYmxlIChpLmUuLCB0aGUgcm93IGNvcnJlc3BvbmRpbmcgdG8gdGhhdCBwb3NpdGlvbiBjYW4gYmUgZWxpbWluYXRlZCBlbnRpcmVseSkuCi0gU29tZSByb3dzIG1heSBjb25zaXN0IGVudGlyZWx5IG9mIHplcm9zLgotIElmIGEgY29sdW1uIGNvbnRhaW5zIGEgcGl2b3QgKGEgbGVhZGluZyAxKSwgYWxsIG90aGVyIGVudHJpZXMgaW4gdGhhdCBjb2x1bW4gc2hvdWxkIGJlIHplcm8uCgpZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IHRoZSBSUkVGIGFsZ29yaXRobSwgd2hpY2ggbXVzdCBoYW5kbGUgdGhlc2UgY2FzZXMgYW5kIGNvbnZlcnQgYW55IGdpdmVuIG1hdHJpeCBpbnRvIGl0cyBSUkVGLgoK",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def rref(self, matrix):\n    \tYour code here\n    \tpass\n",
    "example_input": "import numpy as np\n\nmatrix = np.array([\n    [1, 2, -1, -4],\n    [2, 3, -1, -11],\n    [-2, 0, -3, 22]\n])\n\nrref_matrix = rref(matrix)\nprint(rref_matrix)",
    "example_output": "# array([\n#    [ 1.  0.  0. -8.],\n#    [ 0.  1.  0.  1.],\n#    [-0. -0.  1. -2.]\n# ])",
    "example_reasoning": "The given matrix is converted to its Reduced Row Echelon Form (RREF) where each leading entry is 1, and all other entries in the leading columns are zero.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def rref(self, matrix):\n        # Convert to float for division operations\n        A = matrix.astype(np.float32)\n        n, m = A.shape\n        row = 0  # Current row index for pivot placement\n\n        # Iterate over columns, up to the number of columns m\n        for col in range(m):\n            if row >= n:  # No more rows to process\n                break\n\n            # Find a row with a non-zero entry in the current column\n            nonzero_rel_id = np.nonzero(A[row:, col])[0]\n            if len(nonzero_rel_id) == 0:  # No pivot in this column\n                continue\n\n            # Swap the current row with the row containing the non-zero entry\n            k = nonzero_rel_id[0] + row\n            A[[row, k]] = A[[k, row]]\n\n            # Normalize the pivot row to make the pivot 1\n            A[row] = A[row] / A[row, col]\n\n            # Eliminate all other entries in this column\n            for j in range(n):\n                if j != row:\n                    A[j] -= A[j, col] * A[row]\n\n            row += 1  # Move to the next row for the next pivot\n\n        return A",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nmatrix = np.array([\n    [1, 2, -1, -4],\n    [2, 3, -1, -11],\n    [-2, 0, -3, 22]\n])\n\noutput = rref(matrix)\nprint(output)"
        ],
        "expected_output": [
          [
            1.0,
            0.0,
            0.0,
            -8.0
          ],
          [
            0.0,
            1.0,
            0.0,
            1.0
          ],
          [
            -0.0,
            -0.0,
            1.0,
            -2.0
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\n\nmatrix = np.array([\n    [2, 4, -2],\n    [4, 9, -3],\n    [-2, -3, 7]\n])\n\noutput = rref(matrix)\nprint(output)"
        ],
        "expected_output": [
          [
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            1.0,
            0.0
          ],
          [
            0.0,
            0.0,
            1.0
          ]
        ]
      }
    ],
    "function_name": "rref"
  },
  {
    "id": 49,
    "title": "Implement Adam Optimization Algorithm",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBBZGFtIChBZGFwdGl2ZSBNb21lbnQgRXN0aW1hdGlvbikgb3B0aW1pemF0aW9uIGFsZ29yaXRobSBpbiBQeXRob24uIEFkYW0gaXMgYW4gb3B0aW1pemF0aW9uIGFsZ29yaXRobSB0aGF0IGFkYXB0cyB0aGUgbGVhcm5pbmcgcmF0ZSBmb3IgZWFjaCBwYXJhbWV0ZXIuIFlvdXIgdGFzayBpcyB0byB3cml0ZSBhIGZ1bmN0aW9uIGBhZGFtX29wdGltaXplcmAgdGhhdCB1cGRhdGVzIHRoZSBwYXJhbWV0ZXJzIG9mIGEgZ2l2ZW4gZnVuY3Rpb24gdXNpbmcgdGhlIEFkYW0gYWxnb3JpdGhtLgoKVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIHRoZSBmb2xsb3dpbmcgcGFyYW1ldGVyczoKCi0gYGZgOiBUaGUgb2JqZWN0aXZlIGZ1bmN0aW9uIHRvIGJlIG9wdGltaXplZAotIGBncmFkYDogQSBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIHRoZSBncmFkaWVudCBvZiBgZmAKLSBgeDBgOiBJbml0aWFsIHBhcmFtZXRlciB2YWx1ZXMKLSBgbGVhcm5pbmdfcmF0ZWA6IFRoZSBzdGVwIHNpemUgKGRlZmF1bHQ6IDAuMDAxKQotIGBiZXRhMWA6IEV4cG9uZW50aWFsIGRlY2F5IHJhdGUgZm9yIHRoZSBmaXJzdCBtb21lbnQgZXN0aW1hdGVzIChkZWZhdWx0OiAwLjkpCi0gYGJldGEyYDogRXhwb25lbnRpYWwgZGVjYXkgcmF0ZSBmb3IgdGhlIHNlY29uZCBtb21lbnQgZXN0aW1hdGVzIChkZWZhdWx0OiAwLjk5OSkKLSBgZXBzaWxvbmA6IEEgc21hbGwgY29uc3RhbnQgZm9yIG51bWVyaWNhbCBzdGFiaWxpdHkgKGRlZmF1bHQ6IDFlLTgpCi0gYG51bV9pdGVyYXRpb25zYDogTnVtYmVyIG9mIGl0ZXJhdGlvbnMgdG8gcnVuIHRoZSBvcHRpbWl6ZXIgKGRlZmF1bHQ6IDEwMDApCgpUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgb3B0aW1pemVkIHBhcmFtZXRlcnMuCgo=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def adam_optimizer(self, f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    \t# Your code here\n    \tpass\n",
    "example_input": "import numpy as np\n\ndef objective_function(x):\n    return x[0]**2 + x[1]**2\n\ndef gradient(x):\n    return np.array([2*x[0], 2*x[1]])\n\nx0 = np.array([1.0, 1.0])\nx_opt = adam_optimizer(objective_function, gradient, x0)\n\nprint(\"Optimized parameters:\", x_opt)",
    "example_output": "# Optimized parameters: [0.99000325 0.99000325]",
    "example_reasoning": "The Adam optimizer updates the parameters to minimize the objective function. In this case, the objective function is the sum of squares of the parameters, and the optimizer finds the optimal values for the parameters.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def adam_optimizer(self, f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n        x = x0\n        m = np.zeros_like(x)\n        v = np.zeros_like(x)\n\n        for t in range(1, num_iterations + 1):\n            g = grad(x)\n            m = beta1 * m + (1 - beta1) * g\n            v = beta2 * v + (1 - beta2) * g**2\n            m_hat = m / (1 - beta1**t)\n            v_hat = v / (1 - beta2**t)\n            x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n        return x\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\ndef objective_function(x):\n    return x[0]**2 + x[1]**2\n\ndef gradient(x):\n    return np.array([2*x[0], 2*x[1]])\n\nx0 = np.array([1.0, 1.0])\nx_opt = adam_optimizer(objective_function, gradient, x0)\n\nprint(x_opt)"
        ],
        "expected_output": "[0.99000325 0.99000325]"
      },
      {
        "test": [
          "import numpy as np\n\ndef objective_function(x):\n    return x[0]**2 + x[1]**2\n\ndef gradient(x):\n    return np.array([2*x[0], 2*x[1]])\n\nx0 = np.array([0.2, 12.3])\nx_opt = adam_optimizer(objective_function, gradient, x0)\n\nprint(x_opt)"
        ],
        "expected_output": "[ 0.19001678 12.29000026]"
      }
    ],
    "function_name": "adam_optimizer"
  },
  {
    "id": 50,
    "title": "Implement Lasso Regression using Gradient Descent",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SW4gdGhpcyBwcm9ibGVtLCB5b3UgbmVlZCB0byBpbXBsZW1lbnQgdGhlIExhc3NvIFJlZ3Jlc3Npb24gYWxnb3JpdGhtIHVzaW5nIEdyYWRpZW50IERlc2NlbnQuIExhc3NvIFJlZ3Jlc3Npb24gKEwxIFJlZ3VsYXJpemF0aW9uKSBhZGRzIGEgcGVuYWx0eSBlcXVhbCB0byB0aGUgYWJzb2x1dGUgdmFsdWUgb2YgdGhlIGNvZWZmaWNpZW50cyB0byB0aGUgbG9zcyBmdW5jdGlvbi4gWW91ciB0YXNrIGlzIHRvIHVwZGF0ZSB0aGUgd2VpZ2h0cyBhbmQgYmlhcyBpdGVyYXRpdmVseSB1c2luZyB0aGUgZ3JhZGllbnQgb2YgdGhlIGxvc3MgZnVuY3Rpb24gYW5kIHRoZSBMMSBwZW5hbHR5LgoKVGhlIG9iamVjdGl2ZSBmdW5jdGlvbiBvZiBMYXNzbyBSZWdyZXNzaW9uIGlzOgokJApKKHcsIGIpID0gXGZyYWN7MX17Mm59IFxzdW1fe2k9MX1ee259IFxsZWZ0KCB5X2kgLSBcbGVmdCggXHN1bV97aj0xfV57cH0gWF97aWp9IHdfaiArIGIgXHJpZ2h0KSBccmlnaHQpXjIgKyBcYWxwaGEgXHN1bV97aj0xfV57cH0gfCB3X2ogfAokJAoKV2hlcmU6Ci0gJCB5X2kgJCBpcyB0aGUgYWN0dWFsIHZhbHVlIGZvciB0aGUgJCBpICQtdGggc2FtcGxlCi0gJCBcaGF0e3l9X2kgPSBcc3VtX3tqPTF9XntwfSBYX3tpan0gd19qICsgYiAkIGlzIHRoZSBwcmVkaWN0ZWQgdmFsdWUgZm9yIHRoZSAkIGkgJC10aCBzYW1wbGUKLSAkIHdfaiAkIGlzIHRoZSB3ZWlnaHQgYXNzb2NpYXRlZCB3aXRoIHRoZSAkIGogJC10aCBmZWF0dXJlCi0gJCBcYWxwaGEgJCBpcyB0aGUgcmVndWxhcml6YXRpb24gcGFyYW1ldGVyCi0gJCBiICQgaXMgdGhlIGJpYXMKCllvdXIgdGFzayBpcyB0byB1c2UgdGhlIEwxIHBlbmFsdHkgdG8gc2hyaW5rIHNvbWUgb2YgdGhlIGZlYXR1cmUgY29lZmZpY2llbnRzIHRvIHplcm8gZHVyaW5nIGdyYWRpZW50IGRlc2NlbnQsIHRoZXJlYnkgaGVscGluZyB3aXRoIGZlYXR1cmUgc2VsZWN0aW9uLgoK",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def l1_regularization_gradient_descent(self, X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \tn_samples, n_features = X.shape\n\n    \tweights = np.zeros(n_features)\n    \tbias = 0\n    \t# Your code here\n    \tpass\n",
    "example_input": "import numpy as np\n\nX = np.array([[0, 0], [1, 1], [2, 2]])\ny = np.array([0, 1, 2])\n\nalpha = 0.1\nweights, bias = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)",
    "example_output": "(weights,bias)\n(array([float, float]), float)",
    "example_reasoning": "The Lasso Regression algorithm is used to optimize the weights and bias for the given data. The weights are adjusted to minimize the loss function with the L1 penalty.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def l1_regularization_gradient_descent(self, X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n        n_samples, n_features = X.shape\n        # Zero out weights and bias\n        weights = np.zeros(n_features)\n        bias = 0\n\n        for iteration in range(max_iter):\n            # Predict values\n            y_pred = np.dot(X, weights) + bias\n            # Calculate error\n            error = y_pred - y\n            # Gradient for weights with L1 penalty\n            grad_w = (1 / n_samples) * np.dot(X.T, error) + alpha * np.sign(weights)\n            # Gradient for bias (no penalty for bias)\n            grad_b = (1 / n_samples) * np.sum(error)\n\n            # Update weights and bias\n            weights -= learning_rate * grad_w\n            bias -= learning_rate * grad_b\n\n            # Check for convergence\n            if np.linalg.norm(grad_w, ord=1) < tol:\n                break\n\n        return weights, bias\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nX = np.array([[0, 0], [1, 1], [2, 2]])\ny = np.array([0, 1, 2])\n\nalpha = 0.1\noutput = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\nprint(output)"
        ],
        "expected_output": "(array([0.42371644, 0.42371644]), 0.15385068459377865)"
      },
      {
        "test": [
          "import numpy as np\n\nX = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 2, 3, 4, 5])\n\nalpha = 0.1\noutput = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\nprint(output)"
        ],
        "expected_output": "(array([0.27280148, 0.68108784]), 0.4082863608718005)"
      }
    ],
    "function_name": "l1_regularization_gradient_descent"
  },
  {
    "id": 51,
    "title": "Optimal String Alignment Distance",
    "difficulty": "medium",
    "category": "nlp",
    "problem_description": "SW4gdGhpcyBwcm9ibGVtLCB5b3UgbmVlZCB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0aGF0IGNhbGN1bGF0ZXMgdGhlIFtPcHRpbWFsIFN0cmluZyBBbGlnbm1lbnQgKE9TQSldKGh0dHBzOi8vZW4ud2lraXBlZGlhLm9yZy93aWtpL0RhbWVyYXXigJNMZXZlbnNodGVpbl9kaXN0YW5jZSkgZGlzdGFuY2UgYmV0d2VlbiB0d28gZ2l2ZW4gc3RyaW5ncy4gVGhlIE9TQSBkaXN0YW5jZSByZXByZXNlbnRzIHRoZSBtaW5pbXVtIG51bWJlciBvZiBlZGl0cyByZXF1aXJlZCB0byB0cmFuc2Zvcm0gb25lIHN0cmluZyBpbnRvIGFub3RoZXIuIFRoZSBhbGxvd2VkIGVkaXQgb3BlcmF0aW9ucyBhcmU6CgotIEluc2VydCBhIGNoYXJhY3RlcgotIERlbGV0ZSBhIGNoYXJhY3RlcgotIFN1YnN0aXR1dGUgYSBjaGFyYWN0ZXIKLSBUcmFuc3Bvc2UgdHdvIGFkamFjZW50IGNoYXJhY3RlcnMKCkVhY2ggb2YgdGhlc2Ugb3BlcmF0aW9ucyBjb3N0cyAxIHVuaXQuCgpZb3VyIHRhc2sgaXMgdG8gZmluZCB0aGUgbWluaW11bSBudW1iZXIgb2YgZWRpdHMgbmVlZGVkIHRvIGNvbnZlcnQgdGhlIGZpcnN0IHN0cmluZyAoczEpIGludG8gdGhlIHNlY29uZCBzdHJpbmcgKHMyKS4KCkZvciBleGFtcGxlLCB0aGUgT1NBIGRpc3RhbmNlIGJldHdlZW4gdGhlIHN0cmluZ3MgYGNhcGVyYCBhbmQgYGFjZXJgIGlzIDI6IG9uZSBkZWxldGlvbiAocmVtb3ZpbmcgInAiKSBhbmQgb25lIHRyYW5zcG9zaXRpb24gKHN3YXBwaW5nICJhIiBhbmQgImMiKS4KCg==",
    "starter_code": "\n\nclass Solution:\n    def OSA(self, source: str, target: str) -> int:\n    \t# Your code here\n    \tpass\n",
    "example_input": "source = \"butterfly\"\ntarget = \"dragonfly\"\n\ndistance = OSA(source, target)\nprint(distance)",
    "example_output": "6",
    "example_reasoning": "The OSA distance between the strings \"butterfly\" and \"dragonfly\" is 6. The minimum number of edits required to transform the source string into the target string is 6.",
    "learn_content": "CiMjIE9wdGltYWwgU3RyaW5nIEFsaWdubWVudCBEaXN0YW5jZQoKR2l2ZW4gdHdvIHN0cmluZ3MgXCggc18xIFwpIGFuZCBcKCBzXzIgXCksIGZpbmQgdGhlIE9wdGltYWwgU3RyaW5nIEFsaWdubWVudCAoT1NBKSBkaXN0YW5jZSBiZXR3ZWVuIHRoZW0uCgpUaGUgT1NBIGRpc3RhbmNlIGdpdmVzIHRoZSBtaW5pbXVtIG51bWJlciBvZiBlZGl0cyBuZWVkZWQgdG8gdHJhbnNmb3JtIHN0cmluZyBcKCBzXzEgXCkgaW50byBcKCBzXzIgXCkuIEhlcmUgYXJlIHRoZSBhbGxvd2VkIGVkaXQgb3BlcmF0aW9uczoKCjEuICoqSW5zZXJ0IGEgY2hhcmFjdGVyKioKMi4gKipEZWxldGUgYSBjaGFyYWN0ZXIqKgozLiAqKlN1YnN0aXR1dGUgYSBjaGFyYWN0ZXIqKgo0LiAqKlRyYW5zcG9zZSB0d28gYWRqYWNlbnQgY2hhcmFjdGVycyoqCgpFYWNoIG9wZXJhdGlvbiBoYXMgYSBjb3N0IG9mIDEgdW5pdC4KCkZvciBleGFtcGxlLCB0aGUgT1NBIGRpc3RhbmNlIGJldHdlZW4gdGhlIHN0cmluZ3MgImNhcGVyIiBhbmQgImFjZXIiIGlzIDI6Ci0gT25lIGRlbGV0aW9uIChyZW1vdmluZyB0aGUgbGV0dGVyICdwJykKLSBPbmUgdHJhbnNwb3NpdGlvbiAoc3dhcHBpbmcgdGhlIGxldHRlcnMgJ2EnIGFuZCAnYycpCgo=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def OSA(self, source: str, target: str) -> int:\n        source_len, target_len = len(source), len(target)\n\n        # Initialize matrix with zeros\n        osa_matrix = [[0] * (target_len + 1) for _ in range(source_len + 1)]\n\n        # Fill the first row and first column with index values\n        for j in range(1, target_len + 1):\n            osa_matrix[0][j] = j\n        for i in range(1, source_len + 1):\n            osa_matrix[i][0] = i\n\n        # Compute the OSA distance\n        for i in range(1, source_len + 1):\n            for j in range(1, target_len + 1):\n                osa_matrix[i][j] = min(\n                    osa_matrix[i - 1][j] + 1,  # Deletion\n                    osa_matrix[i][j - 1] + 1,  # Insertion\n                    osa_matrix[i - 1][j - 1] + (1 if source[i - 1] != target[j - 1] else 0)  # Substitution\n                )\n                if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                    osa_matrix[i][j] = min(osa_matrix[i][j], osa_matrix[i - 2][j - 2] + 1)  # Transposition\n\n        return osa_matrix[-1][-1]\n",
    "test_cases": [
      {
        "test": [
          "source = \"butterfly\"\ntarget = \"dragonfly\"\noutput = OSA(source, target)\nprint(output)"
        ],
        "expected_output": 6
      },
      {
        "test": [
          "source = \"caper\"\ntarget = \"acer\"\noutput = OSA(source, target)\nprint(output)"
        ],
        "expected_output": 2
      }
    ],
    "function_name": "OSA"
  },
  {
    "id": 52,
    "title": "Implement Recall Metric in Binary Classification",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IFJlY2FsbCBpbiBCaW5hcnkgQ2xhc3NpZmljYXRpb24KCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgdGhlICoqcmVjYWxsKiogbWV0cmljIGluIGEgYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHNldHRpbmcuIFJlY2FsbCBpcyBhIHBlcmZvcm1hbmNlIG1lYXN1cmUgdGhhdCBldmFsdWF0ZXMgaG93IGVmZmVjdGl2ZWx5IGEgbWFjaGluZSBsZWFybmluZyBtb2RlbCBpZGVudGlmaWVzIHBvc2l0aXZlIGluc3RhbmNlcyBmcm9tIGFsbCB0aGUgYWN0dWFsIHBvc2l0aXZlIGNhc2VzIGluIGEgZGF0YXNldC4KCllvdSBuZWVkIHRvIHdyaXRlIGEgZnVuY3Rpb24gYHJlY2FsbCh5X3RydWUsIHlfcHJlZClgIHRoYXQgY2FsY3VsYXRlcyB0aGUgcmVjYWxsIG1ldHJpYy4gVGhlIGZ1bmN0aW9uIHNob3VsZCBhY2NlcHQgdHdvIGlucHV0czoKCi0gYHlfdHJ1ZWA6IEEgbGlzdCBvZiB0cnVlIGJpbmFyeSBsYWJlbHMgKDAgb3IgMSkgZm9yIHRoZSBkYXRhc2V0LgotIGB5X3ByZWRgOiBBIGxpc3Qgb2YgcHJlZGljdGVkIGJpbmFyeSBsYWJlbHMgKDAgb3IgMSkgZnJvbSB0aGUgbW9kZWwuCgpZb3VyIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gdGhlIHJlY2FsbCB2YWx1ZSByb3VuZGVkIHRvIHRocmVlIGRlY2ltYWwgcGxhY2VzLiBJZiB0aGUgZGVub21pbmF0b3IgKFRQICsgRk4pIGlzIHplcm8sIHRoZSByZWNhbGwgc2hvdWxkIGJlIDAuMCB0byBhdm9pZCBkaXZpc2lvbiBieSB6ZXJvLgoKICAgIA==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def recall(self, y_true, y_pred):\n\n",
    "example_input": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\n\nprint(recall(y_true, y_pred))",
    "example_output": "# 0.75",
    "example_reasoning": "The recall value for the given true labels and predicted labels is 0.75. The model correctly identified 3 out of 4 positive instances in the dataset.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgUmVjYWxsIGluIENsYXNzaWZpY2F0aW9uCgpSZWNhbGwgaXMgYSBtZXRyaWMgdGhhdCBtZWFzdXJlcyBob3cgb2Z0ZW4gYSBtYWNoaW5lIGxlYXJuaW5nIG1vZGVsIGNvcnJlY3RseSBpZGVudGlmaWVzIHBvc2l0aXZlIGluc3RhbmNlcywgYWxzbyBrbm93biBhcyB0cnVlIHBvc2l0aXZlcywgZnJvbSBhbGwgdGhlIGFjdHVhbCBwb3NpdGl2ZSBzYW1wbGVzIGluIHRoZSBkYXRhc2V0LgoKIyMjIE1hdGhlbWF0aWNhbCBEZWZpbml0aW9uCgpSZWNhbGwsIGFsc28ga25vd24gYXMgc2Vuc2l0aXZpdHksIGlzIHRoZSBmcmFjdGlvbiBvZiByZWxldmFudCBpbnN0YW5jZXMgdGhhdCB3ZXJlIHJldHJpZXZlZC4gSXQgaXMgY2FsY3VsYXRlZCB1c2luZyB0aGUgZm9sbG93aW5nIGVxdWF0aW9uOgokJApcdGV4dHtSZWNhbGx9ID0gXGZyYWN7XHRleHR7VFB9fXtcdGV4dHtUUH0gKyBcdGV4dHtGTn19CiQkCgpXaGVyZToKMS4gKipUcnVlIFBvc2l0aXZlcyAoVFApKio6IFRoZSBudW1iZXIgb2YgcG9zaXRpdmUgc2FtcGxlcyB0aGF0IGFyZSBjb3JyZWN0bHkgaWRlbnRpZmllZCBhcyBwb3NpdGl2ZS4KMi4gKipGYWxzZSBOZWdhdGl2ZXMgKEZOKSoqOiBUaGUgbnVtYmVyIG9mIHBvc2l0aXZlIHNhbXBsZXMgdGhhdCBhcmUgaW5jb3JyZWN0bHkgaWRlbnRpZmllZCBhcyBuZWdhdGl2ZS4KCiMjIyBUYXNrCgpJbiB0aGlzIHByb2JsZW0sIHlvdSB3aWxsIGltcGxlbWVudCBhIGZ1bmN0aW9uIHRvIGNhbGN1bGF0ZSByZWNhbGwgZ2l2ZW4gdGhlIHRydWUgbGFiZWxzIGFuZCBwcmVkaWN0ZWQgbGFiZWxzIG9mIGEgYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHRhc2suIFRoZSByZXN1bHRzIHNob3VsZCBiZSByb3VuZGVkIHRvIHRocmVlIGRlY2ltYWwgcGxhY2VzLgoK",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def recall(self, y_true, y_pred):\n        tp = np.sum((y_true == 1) & (y_pred == 1))\n        fn = np.sum((y_true == 1) & (y_pred == 0))\n\n        try:\n            return round(tp / (tp + fn), 3)\n        except ZeroDivisionError:\n            return 0.0\n\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nprint(recall(y_true, y_pred))"
        ],
        "expected_output": 0.75
      },
      {
        "test": [
          "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([1, 0, 0, 0, 0, 1])\nprint(recall(y_true, y_pred))"
        ],
        "expected_output": 0.333
      }
    ],
    "function_name": "recall"
  },
  {
    "id": 53,
    "title": "Implement Self-Attention Mechanism",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IHRoZSBTZWxmLUF0dGVudGlvbiBNZWNoYW5pc20KCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgdGhlICoqc2VsZi1hdHRlbnRpb24qKiBtZWNoYW5pc20sIHdoaWNoIGlzIGEgZnVuZGFtZW50YWwgY29tcG9uZW50IG9mIHRyYW5zZm9ybWVyIG1vZGVscywgd2lkZWx5IHVzZWQgaW4gbmF0dXJhbCBsYW5ndWFnZSBwcm9jZXNzaW5nIGFuZCBjb21wdXRlciB2aXNpb24gdGFza3MuIFRoZSBzZWxmLWF0dGVudGlvbiBtZWNoYW5pc20gYWxsb3dzIGEgbW9kZWwgdG8gZHluYW1pY2FsbHkgZm9jdXMgb24gZGlmZmVyZW50IHBhcnRzIG9mIHRoZSBpbnB1dCBzZXF1ZW5jZSB3aGVuIGdlbmVyYXRpbmcgYSBjb250ZXh0dWFsaXplZCByZXByZXNlbnRhdGlvbi4KCllvdXIgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgc2VsZi1hdHRlbnRpb24gb3V0cHV0IGFzIGEgbnVtcHkgYXJyYXkuCgogICAg",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def self_attention(self, Q, K, V):\n\n    \treturn attention_output\n",
    "example_input": "import numpy as np\n\nX = np.array([[1, 0], [0, 1]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 2], [3, 4]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = self_attention(Q, K, V)\n\nprint(output)",
    "example_output": "# [[1.660477 2.660477]\n#  [2.339523 3.339523]]",
    "example_reasoning": "The self-attention mechanism calculates the attention scores for each input, determining how much focus to put on other inputs when generating a contextualized representation. The output is the weighted sum of the values based on the attention scores.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def compute_qkv(self, X, W_q, W_k, W_v):\n        Q = np.dot(X, W_q)\n        K = np.dot(X, W_k)\n        V = np.dot(X, W_v)\n        return Q, K, V\n\n    def self_attention(self, Q, K, V):\n        d_k = Q.shape[1]\n        scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n        attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n        attention_output = np.matmul(attention_weights, V)\n        return attention_output\n\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nX = np.array([[1, 0], [0, 1]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 2], [3, 4]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = self_attention(Q, K, V)\nprint(output)"
        ],
        "expected_output": [
          [
            1.660477,
            2.660477
          ],
          [
            2.339523,
            3.339523
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\n\nX = np.array([[1, 1], [1, 0]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 2], [3, 4]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = self_attention(Q, K, V)\nprint(output)"
        ],
        "expected_output": [
          [
            3.00928465,
            4.6790462
          ],
          [
            2.5,
            4.0
          ]
        ]
      }
    ],
    "function_name": "self_attention"
  },
  {
    "id": 54,
    "title": "Implementing a Simple RNN",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBpbXBsZW1lbnRzIGEgc2ltcGxlIFJlY3VycmVudCBOZXVyYWwgTmV0d29yayAoUk5OKSBjZWxsLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHByb2Nlc3MgYSBzZXF1ZW5jZSBvZiBpbnB1dCB2ZWN0b3JzIGFuZCBwcm9kdWNlIHRoZSBmaW5hbCBoaWRkZW4gc3RhdGUuIFVzZSB0aGUgdGFuaCBhY3RpdmF0aW9uIGZ1bmN0aW9uIGZvciB0aGUgaGlkZGVuIHN0YXRlIHVwZGF0ZXMuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBhcyBpbnB1dHMgdGhlIHNlcXVlbmNlIG9mIGlucHV0IHZlY3RvcnMsIHRoZSBpbml0aWFsIGhpZGRlbiBzdGF0ZSwgdGhlIHdlaWdodCBtYXRyaWNlcyBmb3IgaW5wdXQtdG8taGlkZGVuIGFuZCBoaWRkZW4tdG8taGlkZGVuIGNvbm5lY3Rpb25zLCBhbmQgdGhlIGJpYXMgdmVjdG9yLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgZmluYWwgaGlkZGVuIHN0YXRlIGFmdGVyIHByb2Nlc3NpbmcgdGhlIGVudGlyZSBzZXF1ZW5jZSwgcm91bmRlZCB0byBmb3VyIGRlY2ltYWwgcGxhY2VzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def rnn_forward(self, input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \t# Your code here\n    \treturn final_hidden_state",
    "example_input": "input_sequence = [[1.0], [2.0], [3.0]]\n    initial_hidden_state = [0.0]\n    Wx = [[0.5]]  # Input to hidden weights\n    Wh = [[0.8]]  # Hidden to hidden weights\n    b = [0.0]     # Bias",
    "example_output": "final_hidden_state = [0.9993]",
    "example_reasoning": "The RNN processes each input in the sequence, updating the hidden state at each step using the tanh activation function.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def rnn_forward(self, input_sequence, initial_hidden_state, Wx, Wh, b):\n        h = np.array(initial_hidden_state)\n        Wx = np.array(Wx)\n        Wh = np.array(Wh)\n        b = np.array(b)\n        for x in input_sequence:\n            x = np.array(x)\n            h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n        final_hidden_state = np.round(h, 4)\n        return final_hidden_state.tolist()\n",
    "test_cases": [
      {
        "test": [
          [
            [
              1.0
            ],
            [
              2.0
            ],
            [
              3.0
            ]
          ],
          [
            0.0
          ],
          [
            [
              0.5
            ]
          ],
          [
            [
              0.8
            ]
          ],
          [
            0.0
          ]
        ],
        "expected_output": [
          0.9759
        ]
      },
      {
        "test": [
          [
            [
              0.5
            ],
            [
              0.1
            ],
            [
              -0.2
            ]
          ],
          [
            0.0
          ],
          [
            [
              1.0
            ]
          ],
          [
            [
              0.5
            ]
          ],
          [
            0.1
          ]
        ],
        "expected_output": [
          0.118
        ]
      }
    ],
    "function_name": "rnn_forward"
  },
  {
    "id": 55,
    "title": "2D Translation Matrix Implementation",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IGEgMkQgVHJhbnNsYXRpb24gTWF0cml4CgpZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBhcHBsaWVzIGEgMkQgdHJhbnNsYXRpb24gbWF0cml4IHRvIGEgc2V0IG9mIHBvaW50cy4gQSB0cmFuc2xhdGlvbiBtYXRyaXggaXMgdXNlZCB0byBtb3ZlIHBvaW50cyBpbiAyRCBzcGFjZSBieSBhIHNwZWNpZmllZCBkaXN0YW5jZSBpbiB0aGUgeCBhbmQgeSBkaXJlY3Rpb25zLgoKV3JpdGUgYSBmdW5jdGlvbiBgdHJhbnNsYXRlX29iamVjdChwb2ludHMsIHR4LCB0eSlgIHdoZXJlIGBwb2ludHNgIGlzIGEgbGlzdCBvZiBbeCwgeV0gY29vcmRpbmF0ZXMgYW5kIGB0eGAgYW5kIGB0eWAgYXJlIHRoZSB0cmFuc2xhdGlvbiBkaXN0YW5jZXMgaW4gdGhlIHggYW5kIHkgZGlyZWN0aW9ucywgcmVzcGVjdGl2ZWx5LgoKVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gYSBuZXcgbGlzdCBvZiBwb2ludHMgYWZ0ZXIgYXBwbHlpbmcgdGhlIHRyYW5zbGF0aW9uIG1hdHJpeC4KCiAgICA=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def translate_object(self, points, tx, ty):\n    \treturn translated_points\n",
    "example_input": "points = [[0, 0], [1, 0], [0.5, 1]]\ntx, ty = 2, 3\n\nprint(translate_object(points, tx, ty))",
    "example_output": "[[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]",
    "example_reasoning": "The translation matrix moves the points by 2 units in the x-direction and 3 units in the y-direction. The resulting points are [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]].",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def translate_object(self, points, tx, ty):\n        translation_matrix = np.array([\n            [1, 0, tx],\n            [0, 1, ty],\n            [0, 0, 1]\n        ])\n\n        homogeneous_points = np.hstack([np.array(points), np.ones((len(points), 1))])\n\n        translated_points = np.dot(homogeneous_points, translation_matrix.T)\n\n        return translated_points[:, :2].tolist()\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\ntriangle = [[0, 0], [1, 0], [0.5, 1]]\ntx, ty = 2, 3\nprint(translate_object(triangle, tx, ty))"
        ],
        "expected_output": [
          [
            2.0,
            3.0
          ],
          [
            3.0,
            3.0
          ],
          [
            2.5,
            4.0
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\n\nsquare = [[0, 0], [1, 0], [1, 1], [0, 1]]\ntx, ty = -1, 2\nprint(translate_object(square, tx, ty))"
        ],
        "expected_output": [
          [
            -1.0,
            2.0
          ],
          [
            0.0,
            2.0
          ],
          [
            0.0,
            3.0
          ],
          [
            -1.0,
            3.0
          ]
        ]
      }
    ],
    "function_name": "translate_object"
  },
  {
    "id": 56,
    "title": "KL Divergence Between Two Normal Distributions",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IEtMIERpdmVyZ2VuY2UgQmV0d2VlbiBUd28gTm9ybWFsIERpc3RyaWJ1dGlvbnMKCllvdXIgdGFzayBpcyB0byBjb21wdXRlIHRoZSBLdWxsYmFjayBMZWlibGVyIChLTCkgZGl2ZXJnZW5jZSBiZXR3ZWVuIHR3byBub3JtYWwgZGlzdHJpYnV0aW9ucy4gS0wgZGl2ZXJnZW5jZSBtZWFzdXJlcyBob3cgb25lIHByb2JhYmlsaXR5IGRpc3RyaWJ1dGlvbiBkaWZmZXJzIGZyb20gYSBzZWNvbmQsIHJlZmVyZW5jZSBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb24uCgpXcml0ZSBhIGZ1bmN0aW9uIGtsX2RpdmVyZ2VuY2Vfbm9ybWFsKG11X3AsIHNpZ21hX3AsIG11X3EsIHNpZ21hX3EpIHRoYXQgY2FsY3VsYXRlcyB0aGUgS0wgZGl2ZXJnZW5jZSBiZXR3ZWVuIHR3byBub3JtYWwgZGlzdHJpYnV0aW9ucy4KClRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIHRoZSBLTCBkaXZlcmdlbmNlIGFzIGEgZmxvYXRpbmcgcG9pbnQgbnVtYmVyLgoKICAgIA==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def kl_divergence_normal(self, mu_p, sigma_p, mu_q, sigma_q):\n    \treturn 0.0\n",
    "example_input": "mu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 1.0\n\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
    "example_output": "0.5",
    "example_reasoning": "The KL divergence between the normal distributions \\( P \\) and \\( Q \\) with parameters \\( \\mu_P = 0.0 \\), \\( \\sigma_P = 1.0 \\) and \\( \\mu_Q = 1.0 \\), \\( \\sigma_Q = 1.0 \\) is 0.5.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def kl_divergence_normal(self, mu_p, sigma_p, mu_q, sigma_q):\n        term1 = np.log(sigma_q / sigma_p)\n        term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n        kl_div = term1 + term2 - 0.5\n        return kl_div\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 0.0\nsigma_q = 1.0\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))"
        ],
        "expected_output": 0.0
      },
      {
        "test": [
          "import numpy as np\n\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 1.0\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))"
        ],
        "expected_output": 0.5
      }
    ],
    "function_name": "kl_divergence_normal"
  },
  {
    "id": 57,
    "title": "Gauss-Seidel Method for Solving Linear Systems",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IHRoZSBHYXVzcy1TZWlkZWwgTWV0aG9kCgpZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IHRoZSBHYXVzcy1TZWlkZWwgbWV0aG9kLCBhbiBpdGVyYXRpdmUgdGVjaG5pcXVlIGZvciBzb2x2aW5nIGEgc3lzdGVtIG9mIGxpbmVhciBlcXVhdGlvbnMgXChBeCA9IGJcKS4KClRoZSBmdW5jdGlvbiBzaG91bGQgaXRlcmF0aXZlbHkgdXBkYXRlIHRoZSBzb2x1dGlvbiB2ZWN0b3IgXCh4XCkgYnkgdXNpbmcgdGhlIG1vc3QgcmVjZW50IHZhbHVlcyBhdmFpbGFibGUgZHVyaW5nIHRoZSBpdGVyYXRpb24gcHJvY2Vzcy4KCldyaXRlIGEgZnVuY3Rpb24gYGdhdXNzX3NlaWRlbChBLCBiLCBuLCB4X2luaT1Ob25lKWAgd2hlcmU6CgotIGBBYCBpcyBhIHNxdWFyZSBtYXRyaXggb2YgY29lZmZpY2llbnRzLAotIGBiYCBpcyB0aGUgcmlnaHQtaGFuZCBzaWRlIHZlY3RvciwKLSBgbmAgaXMgdGhlIG51bWJlciBvZiBpdGVyYXRpb25zLAotIGB4X2luaWAgaXMgYW4gb3B0aW9uYWwgaW5pdGlhbCBndWVzcyBmb3IgXCh4XCkgKGlmIG5vdCBwcm92aWRlZCwgYXNzdW1lIGEgdmVjdG9yIG9mIHplcm9zKS4KClRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIHRoZSBhcHByb3hpbWF0ZWQgc29sdXRpb24gdmVjdG9yIFwoeFwpIGFmdGVyIHBlcmZvcm1pbmcgdGhlIHNwZWNpZmllZCBudW1iZXIgb2YgaXRlcmF0aW9ucy4KCiAgICA=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def gauss_seidel(self, A, b, n, x_ini=None):\n    \treturn np.zeros_like(b)\n",
    "example_input": "A = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float)\nb = np.array([4, 7, 3], dtype=float)\n\nn = 100\nprint(gauss_seidel(A, b, n))",
    "example_output": "# [0.2, 1.4, 0.8]  (Approximate, values may vary depending on iterations)",
    "example_reasoning": "The Gauss-Seidel method iteratively updates the solution vector \\(x\\) until convergence. The output is an approximate solution to the linear system.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def gauss_seidel_it(self, A, b, x):\n        rows, cols = A.shape\n        for i in range(rows):\n            x_new = b[i]\n            for j in range(cols):\n                if i != j:\n                    x_new -= A[i, j] * x[j]\n            x[i] = x_new / A[i, i]\n        return x\n\n    def gauss_seidel(self, A, b, n, x_ini=None):\n        x = x_ini or np.zeros_like(b)\n        for _ in range(n):\n            x = gauss_seidel_it(A, b, x)\n        return x\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float)\nb = np.array([4, 7, 3], dtype=float)\nn = 5\nprint(gauss_seidel(A, b, n))"
        ],
        "expected_output": [
          0.5008,
          0.99968,
          0.49984
        ]
      },
      {
        "test": [
          "import numpy as np\n\nA = np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float)\nb = np.array([15, 10, 10, 15], dtype=float)\nn = 1\nprint(gauss_seidel(A, b, n))"
        ],
        "expected_output": [
          3.75,
          3.4375,
          3.359375,
          3.65234375
        ]
      }
    ],
    "function_name": "gauss_seidel"
  },
  {
    "id": 58,
    "title": "Gaussian Elimination for Solving Linear Systems",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IHRoZSBHYXVzc2lhbiBFbGltaW5hdGlvbiBNZXRob2QKCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgdGhlIEdhdXNzaWFuIEVsaW1pbmF0aW9uIG1ldGhvZCwgd2hpY2ggdHJhbnNmb3JtcyBhIHN5c3RlbSBvZiBsaW5lYXIgZXF1YXRpb25zIGludG8gYW4gdXBwZXIgdHJpYW5ndWxhciBtYXRyaXguIFRoaXMgbWV0aG9kIGNhbiB0aGVuIGJlIHVzZWQgdG8gc29sdmUgZm9yIHRoZSB2YXJpYWJsZXMgdXNpbmcgYmFja3dhcmQgc3Vic3RpdHV0aW9uLgoKV3JpdGUgYSBmdW5jdGlvbiBgZ2F1c3NpYW5fZWxpbWluYXRpb24oQSwgYilgIHRoYXQgcGVyZm9ybXMgR2F1c3NpYW4gRWxpbWluYXRpb24gd2l0aCBwYXJ0aWFsIHBpdm90aW5nIHRvIHNvbHZlIHRoZSBzeXN0ZW0gXChBeCA9IGJcKS4KClRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIHRoZSBzb2x1dGlvbiB2ZWN0b3IgXCh4XCkuCgogICAg",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def gaussian_elimination(self, A, b):\n    \t\"\"\"\n    \tSolves the system Ax = b using Gaussian Elimination with partial pivoting.\n\n    \t:param A: Coefficient matrix\n    \t:param b: Right-hand side vector\n    \t:return: Solution vector x\n    \t\"\"\"\n    \treturn np.zeros_like(b)\n",
    "example_input": "A = np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float)\nb = np.array([2,5,1], dtype=float)\n\nprint(gaussian_elimination(A, b))",
    "example_output": "[11.0, -4.0, 3.0]",
    "example_reasoning": "The Gaussian Elimination method transforms the system of equations into an upper triangular matrix and then uses backward substitution to solve for the variables.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def partial_pivoting(self, A_aug, row_num, col_num):\n        rows, cols = A_aug.shape\n        max_row = row_num\n        max_val = abs(A_aug[row_num, col_num])\n        for i in range(row_num, rows):\n            current_val = abs(A_aug[i, col_num])\n            if current_val > max_val:\n                max_val = current_val\n                max_row = i\n        if max_row != row_num:\n            A_aug[[row_num, max_row]] = A_aug[[max_row, row_num]]\n        return A_aug\n\n    def gaussian_elimination(self, A, b):\n        rows, cols = A.shape\n        A_aug = np.hstack((A, b.reshape(-1, 1)))\n\n        for i in range(rows-1):\n            A_aug = partial_pivoting(A_aug, i, i)\n            for j in range(i+1, rows):\n                A_aug[j, i:] -= (A_aug[j, i] / A_aug[i, i]) * A_aug[i, i:]\n\n        x = np.zeros_like(b, dtype=float)\n        for i in range(rows-1, -1, -1):\n            x[i] = (A_aug[i, -1] - np.dot(A_aug[i, i+1:cols], x[i+1:])) / A_aug[i, i]\n        return x\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nA = np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float)\nb = np.array([2,5,1], dtype=float)\nprint(gaussian_elimination(A, b))"
        ],
        "expected_output": [
          11.0,
          -4.0,
          3.0
        ]
      },
      {
        "test": [
          "import numpy as np\n\nA = np.array([\n    [0, 2, 1, 0, 0, 0, 0],\n    [2, 6, 2, 1, 0, 0, 0],\n    [1, 2, 7, 2, 1, 0, 0],\n    [0, 1, 2, 8, 2, 1, 0],\n    [0, 0, 1, 2, 9, 2, 1],\n    [0, 0, 0, 1, 2, 10, 2],\n    [0, 0, 0, 0, 1, 2, 11]\n], dtype=float)\nb = np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)\nprint(gaussian_elimination(A, b))"
        ],
        "expected_output": [
          -0.4894027,
          0.36169985,
          0.2766003,
          0.25540569,
          0.31898951,
          0.40387497,
          0.53393278
        ]
      }
    ],
    "function_name": "gaussian_elimination"
  },
  {
    "id": 59,
    "title": "Implement Long Short-Term Memory (LSTM) Network",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IExvbmcgU2hvcnQtVGVybSBNZW1vcnkgKExTVE0pIE5ldHdvcmsKCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYW4gTFNUTSBuZXR3b3JrIHRoYXQgcHJvY2Vzc2VzIGEgc2VxdWVuY2Ugb2YgaW5wdXRzIGFuZCBwcm9kdWNlcyB0aGUgZmluYWwgaGlkZGVuIHN0YXRlIGFuZCBjZWxsIHN0YXRlIGFmdGVyIHByb2Nlc3NpbmcgYWxsIGlucHV0cy4KCldyaXRlIGEgY2xhc3MgYExTVE1gIHdpdGggdGhlIGZvbGxvd2luZyBtZXRob2RzOgoKLSBgX19pbml0X18oc2VsZiwgaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUpYDogSW5pdGlhbGl6ZXMgdGhlIExTVE0gd2l0aCByYW5kb20gd2VpZ2h0cyBhbmQgemVybyBiaWFzZXMuCi0gYGZvcndhcmQoc2VsZiwgeCwgaW5pdGlhbF9oaWRkZW5fc3RhdGUsIGluaXRpYWxfY2VsbF9zdGF0ZSlgOiBQcm9jZXNzZXMgYSBzZXF1ZW5jZSBvZiBpbnB1dHMgYW5kIHJldHVybnMgdGhlIGhpZGRlbiBzdGF0ZXMgYXQgZWFjaCB0aW1lIHN0ZXAsIGFzIHdlbGwgYXMgdGhlIGZpbmFsIGhpZGRlbiBzdGF0ZSBhbmQgY2VsbCBzdGF0ZS4KClRoZSBMU1RNIHNob3VsZCBjb21wdXRlIHRoZSBmb3JnZXQgZ2F0ZSwgaW5wdXQgZ2F0ZSwgY2FuZGlkYXRlIGNlbGwgc3RhdGUsIGFuZCBvdXRwdXQgZ2F0ZSBhdCBlYWNoIHRpbWUgc3RlcCB0byB1cGRhdGUgdGhlIGhpZGRlbiBzdGF0ZSBhbmQgY2VsbCBzdGF0ZS4KCiAgICA=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    class LSTM:\n    def __init__(self, self, input_size, hidden_size):\n    \tself.input_size = input_size\n    \tself.hidden_size = hidden_size\n\n    \t# Initialize weights and biases\n    \tself.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n    \tself.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n    \tself.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n    \tself.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n    \tself.bf = np.zeros((hidden_size, 1))\n    \tself.bi = np.zeros((hidden_size, 1))\n    \tself.bc = np.zeros((hidden_size, 1))\n    \tself.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, self, x, initial_hidden_state, initial_cell_state):\n    \t\"\"\"\n    \tProcesses a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n    \t\"\"\"\n    \tpass\n",
    "example_input": "input_sequence = np.array([[1.0], [2.0], [3.0]])\ninitial_hidden_state = np.zeros((1, 1))\ninitial_cell_state = np.zeros((1, 1))\n\nlstm = LSTM(input_size=1, hidden_size=1)\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n\nprint(final_h)",
    "example_output": "[[0.73698596]] (approximate)",
    "example_reasoning": "The LSTM processes the input sequence [1.0, 2.0, 3.0] and produces the final hidden state [0.73698596].",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    class LSTM:\n    def __init__(self, self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, self, x, initial_hidden_state, initial_cell_state):\n        h = initial_hidden_state\n        c = initial_cell_state\n        outputs = []\n\n        for t in range(len(x)):\n            xt = x[t].reshape(-1, 1)\n            concat = np.vstack((h, xt))\n\n            # Forget gate\n            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n\n            # Input gate\n            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n\n            # Cell state update\n            c = ft * c + it * c_tilde\n\n            # Output gate\n            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n\n            # Hidden state update\n            h = ot * np.tanh(c)\n\n            outputs.append(h)\n\n        return np.array(outputs), h, c\n\n    def sigmoid(self, self, x):\n        return 1 / (1 + np.exp(-x))\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\ninput_sequence = np.array([[1.0], [2.0], [3.0]])\ninitial_hidden_state = np.zeros((1, 1))\ninitial_cell_state = np.zeros((1, 1))\n\nlstm = LSTM(input_size=1, hidden_size=1)\n# Set weights and biases for reproducibility\nlstm.Wf = np.array([[0.5, 0.5]])\nlstm.Wi = np.array([[0.5, 0.5]])\nlstm.Wc = np.array([[0.3, 0.3]])\nlstm.Wo = np.array([[0.5, 0.5]])\nlstm.bf = np.array([[0.1]])\nlstm.bi = np.array([[0.1]])\nlstm.bc = np.array([[0.1]])\nlstm.bo = np.array([[0.1]])\n\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n\nprint(final_h)"
        ],
        "expected_output": [
          [
            0.73698596
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\n\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])\ninitial_hidden_state = np.zeros((2, 1))\ninitial_cell_state = np.zeros((2, 1))\n\nlstm = LSTM(input_size=2, hidden_size=2)\n# Set weights and biases for reproducibility\nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\nlstm.bf = np.array([[0.1], [0.2]])\nlstm.bi = np.array([[0.1], [0.2]])\nlstm.bc = np.array([[0.1], [0.2]])\nlstm.bo = np.array([[0.1], [0.2]])\n\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n\nprint(final_h)"
        ],
        "expected_output": [
          [
            0.16613133
          ],
          [
            0.40299449
          ]
        ]
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 60,
    "title": "Implement TF-IDF (Term Frequency-Inverse Document Frequency)",
    "difficulty": "medium",
    "category": "nlp",
    "problem_description": "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",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def compute_tf_idf(self, corpus, query):\n    \t\"\"\"\n    \tCompute TF-IDF scores for a query against a corpus of documents.\n\n    \t:param corpus: List of documents, where each document is a list of words\n    \t:param query: List of words in the query\n    \t:return: List of lists containing TF-IDF scores for the query words in each document\n    \t\"\"\"\n    \tpass\n",
    "example_input": "corpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n]\nquery = [\"cat\"]\n\nprint(compute_tf_idf(corpus, query))",
    "example_output": "[[0.21461], [0.25754], [0.0]]",
    "example_reasoning": "The TF-IDF scores for the word \"cat\" in each document are computed and rounded to five decimal places.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def compute_tf_idf(self, corpus, query):\n        \"\"\"\n        Compute TF-IDF scores for a query against a corpus of documents using only NumPy.\n        The output TF-IDF scores retain five decimal places.\n        \"\"\"\n        vocab = sorted(set(word for document in corpus for word in document).union(query))\n        word_to_index = {word: idx for idx, word in enumerate(vocab)}\n\n        tf = np.zeros((len(corpus), len(vocab)))\n\n        for doc_idx, document in enumerate(corpus):\n            for word in document:\n                word_idx = word_to_index[word]\n                tf[doc_idx, word_idx] += 1\n            tf[doc_idx, :] /= len(document)\n\n        df = np.count_nonzero(tf > 0, axis=0)\n\n        num_docs = len(corpus)\n        idf = np.log((num_docs + 1) / (df + 1)) + 1\n\n        tf_idf = tf * idf\n\n        query_indices = [word_to_index[word] for word in query]\n        tf_idf_scores = tf_idf[:, query_indices]\n\n        tf_idf_scores = np.round(tf_idf_scores, 5)\n\n        return tf_idf_scores.tolist()\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\ncorpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n]\nquery = [\"cat\"]\n\nprint(compute_tf_idf(corpus, query))"
        ],
        "expected_output": [
          [
            0.21461
          ],
          [
            0.25754
          ],
          [
            0.0
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\n\ncorpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n]\nquery = [\"cat\", \"mat\"]\n\nprint(compute_tf_idf(corpus, query))"
        ],
        "expected_output": [
          [
            0.21461,
            0.21461
          ],
          [
            0.25754,
            0.0
          ],
          [
            0.0,
            0.21461
          ]
        ]
      }
    ],
    "function_name": "compute_tf_idf"
  },
  {
    "id": 61,
    "title": "Implement F-Score Calculation for Binary Classification",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IEYtU2NvcmUgQ2FsY3VsYXRpb24gZm9yIEJpbmFyeSBDbGFzc2lmaWNhdGlvbgoKWW91ciB0YXNrIGlzIHRvIGltcGxlbWVudCBhIGZ1bmN0aW9uIHRoYXQgY2FsY3VsYXRlcyB0aGUgRi1TY29yZSBmb3IgYSBiaW5hcnkgY2xhc3NpZmljYXRpb24gdGFzay4gVGhlIEYtU2NvcmUgY29tYmluZXMgYm90aCBQcmVjaXNpb24gYW5kIFJlY2FsbCBpbnRvIGEgc2luZ2xlIG1ldHJpYywgcHJvdmlkaW5nIGEgYmFsYW5jZWQgbWVhc3VyZSBvZiBhIG1vZGVsJ3MgcGVyZm9ybWFuY2UuCgpXcml0ZSBhIGZ1bmN0aW9uIGBmX3Njb3JlKHlfdHJ1ZSwgeV9wcmVkLCBiZXRhKWAgd2hlcmU6CgotIGB5X3RydWVgOiBBIG51bXB5IGFycmF5IG9mIHRydWUgbGFiZWxzIChiaW5hcnkpLgotIGB5X3ByZWRgOiBBIG51bXB5IGFycmF5IG9mIHByZWRpY3RlZCBsYWJlbHMgKGJpbmFyeSkuCi0gYGJldGFgOiBBIGZsb2F0IHZhbHVlIHRoYXQgYWRqdXN0cyB0aGUgaW1wb3J0YW5jZSBvZiBQcmVjaXNpb24gYW5kIFJlY2FsbC4gV2hlbiBgYmV0YT0xYCwgaXQgY29tcHV0ZXMgdGhlIEYxLVNjb3JlLCBhIGJhbGFuY2VkIG1lYXN1cmUgb2YgYm90aCBQcmVjaXNpb24gYW5kIFJlY2FsbC4KClRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIHRoZSBGLVNjb3JlIHJvdW5kZWQgdG8gdGhyZWUgZGVjaW1hbCBwbGFjZXMuCgogICAg",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def f_score(self, y_true, y_pred, beta):\n    \t\"\"\"\n    \tCalculate F-Score for a binary classification task.\n\n    \t:param y_true: Numpy array of true labels\n    \t:param y_pred: Numpy array of predicted labels\n    \t:param beta: The weight of precision in the harmonic mean\n    \t:return: F-Score rounded to three decimal places\n    \t\"\"\"\n    \tpass\n",
    "example_input": "y_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nbeta = 1\n\nprint(f_score(y_true, y_pred, beta))",
    "example_output": "0.857",
    "example_reasoning": "The F-Score for the binary classification task is calculated using the true labels, predicted labels, and beta value.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgRi1TY29yZSBpbiBDbGFzc2lmaWNhdGlvbgoKRi1TY29yZSwgYWxzbyBjYWxsZWQgRi1tZWFzdXJlLCBpcyBhIG1lYXN1cmUgb2YgcHJlZGljdGl2ZSBwZXJmb3JtYW5jZSB0aGF0J3MgY2FsY3VsYXRlZCBmcm9tIHRoZSBQcmVjaXNpb24gYW5kIFJlY2FsbCBtZXRyaWNzLgoKIyMjIE1hdGhlbWF0aWNhbCBEZWZpbml0aW9uCgpUaGUgJEZfe1xiZXRhfSQgc2NvcmUgYXBwbGllcyBhZGRpdGlvbmFsIHdlaWdodHMsIHZhbHVpbmcgb25lIG9mIHByZWNpc2lvbiBvciByZWNhbGwgbW9yZSB0aGFuIHRoZSBvdGhlci4gV2hlbiAkXGJldGEkIGVxdWFscyAxLCBhbHNvIGtub3duIGFzIHRoZSAqKkYxLVNjb3JlKiosIGl0IHN5bW1ldHJpY2FsbHkgcmVwcmVzZW50cyBib3RoIHByZWNpc2lvbiBhbmQgcmVjYWxsIGluIG9uZSBtZXRyaWMuIFRoZSBGLVNjb3JlIGNhbiBiZSBjYWxjdWxhdGVkIHVzaW5nIHRoZSBmb2xsb3dpbmcgZm9ybXVsYToKCiQkCkZfe1xiZXRhfSA9ICgxICsgXGJldGFeMikgXHRpbWVzIFxmcmFje1x0ZXh0e3ByZWNpc2lvbn0gXHRpbWVzIFx0ZXh0e3JlY2FsbH19eyhcYmV0YV4yIFx0aW1lcyBcdGV4dHtwcmVjaXNpb259KSArIFx0ZXh0e3JlY2FsbH19CiQkCgpXaGVyZToKCi0gKipSZWNhbGwqKjogVGhlIG51bWJlciBvZiB0cnVlIHBvc2l0aXZlIHJlc3VsdHMgZGl2aWRlZCBieSB0aGUgbnVtYmVyIG9mIGFsbCBzYW1wbGVzIHRoYXQgc2hvdWxkIGhhdmUgYmVlbiBpZGVudGlmaWVkIGFzIHBvc2l0aXZlLgotICoqUHJlY2lzaW9uKio6IFRoZSBudW1iZXIgb2YgdHJ1ZSBwb3NpdGl2ZSByZXN1bHRzIGRpdmlkZWQgYnkgdGhlIG51bWJlciBvZiBhbGwgc2FtcGxlcyBwcmVkaWN0ZWQgdG8gYmUgcG9zaXRpdmUsIGluY2x1ZGluZyB0aG9zZSBub3QgaWRlbnRpZmllZCBjb3JyZWN0bHkuCgojIyMgSW1wbGVtZW50YXRpb24gSW5zdHJ1Y3Rpb25zCgpJbiB0aGlzIHByb2JsZW0sIHlvdSB3aWxsIGltcGxlbWVudCBhIGZ1bmN0aW9uIHRvIGNhbGN1bGF0ZSB0aGUgKipGLVNjb3JlKiogZ2l2ZW4gdGhlIHRydWUgbGFiZWxzLCBwcmVkaWN0ZWQgbGFiZWxzLCBhbmQgdGhlIEJldGEgdmFsdWUgb2YgYSBiaW5hcnkgY2xhc3NpZmljYXRpb24gdGFzay4gVGhlIHJlc3VsdHMgc2hvdWxkIGJlIHJvdW5kZWQgdG8gdGhyZWUgZGVjaW1hbCBwbGFjZXMuCgojIyMjIFNwZWNpYWwgQ2FzZToKSWYgdGhlIGRlbm9taW5hdG9yIGlzIHplcm8sIHRoZSBGLVNjb3JlIHNob3VsZCBiZSBzZXQgdG8gKiowLjAqKiB0byBhdm9pZCBkaXZpc2lvbiBieSB6ZXJvLgoK",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def f_score(self, y_true, y_pred, beta):\n        tp = np.sum((y_true == 1) & (y_pred == 1))\n        fn = np.sum((y_true == 1) & (y_pred == 0))\n        fp = np.sum((y_true == 0) & (y_pred == 1))\n\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n\n        op = precision * recall\n        div = ((beta**2) * precision) + recall\n\n        if div == 0 or op == 0:\n            return 0.0\n\n        score = (1 + (beta ** 2)) * op / div\n        return round(score, 3)\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nbeta = 1\nprint(f_score(y_true, y_pred, beta))"
        ],
        "expected_output": 0.857
      },
      {
        "test": [
          "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([1, 0, 0, 0, 0, 1])\nbeta = 1\nprint(f_score(y_true, y_pred, beta))"
        ],
        "expected_output": 0.4
      }
    ],
    "function_name": "f_score"
  },
  {
    "id": 62,
    "title": "Implement a Simple RNN with Backpropagation Through Time (BPTT)",
    "difficulty": "hard",
    "category": "deep learning",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IGEgU2ltcGxlIFJOTiB3aXRoIEJhY2twcm9wYWdhdGlvbiBUaHJvdWdoIFRpbWUgKEJQVFQpCgpZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IGEgc2ltcGxlIFJlY3VycmVudCBOZXVyYWwgTmV0d29yayAoUk5OKSBhbmQgYmFja3Byb3BhZ2F0aW9uIHRocm91Z2ggdGltZSAoQlBUVCkgdG8gbGVhcm4gZnJvbSBzZXF1ZW50aWFsIGRhdGEuIFRoZSBSTk4gd2lsbCBwcm9jZXNzIGlucHV0IHNlcXVlbmNlcywgdXBkYXRlIGhpZGRlbiBzdGF0ZXMsIGFuZCBwZXJmb3JtIGJhY2twcm9wYWdhdGlvbiB0byBhZGp1c3Qgd2VpZ2h0cyBiYXNlZCBvbiB0aGUgZXJyb3IgZ3JhZGllbnQuCgpXcml0ZSBhIGNsYXNzIGBTaW1wbGVSTk5gIHdpdGggdGhlIGZvbGxvd2luZyBtZXRob2RzOgoKLSBgX19pbml0X18oc2VsZiwgaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKWA6IEluaXRpYWxpemVzIHRoZSBSTk4gd2l0aCByYW5kb20gd2VpZ2h0cyBhbmQgemVybyBiaWFzZXMuCi0gYGZvcndhcmQoc2VsZiwgeClgOiBQcm9jZXNzZXMgYSBzZXF1ZW5jZSBvZiBpbnB1dHMgYW5kIHJldHVybnMgdGhlIGhpZGRlbiBzdGF0ZXMgYW5kIG91dHB1dC4KLSBgYmFja3dhcmQoc2VsZiwgeCwgeSwgbGVhcm5pbmdfcmF0ZSlgOiBQZXJmb3JtcyBiYWNrcHJvcGFnYXRpb24gdGhyb3VnaCB0aW1lIChCUFRUKSB0byBhZGp1c3QgdGhlIHdlaWdodHMgYmFzZWQgb24gdGhlIGxvc3MuCgpJbiB0aGlzIHRhc2ssIHRoZSBSTk4gd2lsbCBiZSB0cmFpbmVkIG9uIHNlcXVlbmNlIHByZWRpY3Rpb24sIHdoZXJlIHRoZSBuZXR3b3JrIHdpbGwgbGVhcm4gdG8gcHJlZGljdCB0aGUgbmV4dCBpdGVtIGluIGEgc2VxdWVuY2UuIFlvdSBzaG91bGQgdXNlIDEvMiAqIE1lYW4gU3F1YXJlZCBFcnJvciAoTVNFKSBhcyB0aGUgbG9zcyBmdW5jdGlvbiBhbmQgbWFrZSBzdXJlIHRvIGFnZ3JlZ2F0ZSB0aGUgbG9zc2VzIGF0IGVhY2ggdGltZSBzdGVwIGJ5IHN1bW1pbmcuCg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    class SimpleRNN:\n    def __init__(self, self, input_size, hidden_size, output_size):\n    \"\"\"\n    Initializes the RNN with random weights and zero biases.\n    \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n    def forward(self, self, x):\n    \"\"\"\n    Forward pass through the RNN for a given sequence of inputs.\n    \"\"\"\n    pass\n\n    def backward(self, self, x, y, learning_rate):\n    \"\"\"\n    Backpropagation through time to adjust weights based on error gradient.\n    \"\"\"\n    pass\n",
    "example_input": "import numpy as np\n    input_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\n    expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n    # Initialize RNN\n    rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n    \n    # Forward pass\n    output = rnn.forward(input_sequence)\n    \n    # Backward pass\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    \n    print(output)\n    \n    # The output should show the RNN predictions for each step of the input sequence.",
    "example_output": "[[x1], [x2], [x3], [x4]]",
    "example_reasoning": "The RNN processes the input sequence [1.0, 2.0, 3.0, 4.0] and predicts the next item in the sequence at each step.",
    "learn_content": "# Understanding Recurrent Neural Networks (RNNs) and Backpropagation Through Time (BPTT)

Recurrent Neural Networks (RNNs) are designed to handle sequential data by maintaining a hidden state that captures information from previous inputs. They are particularly useful for tasks where context or sequential order is important, such as language modeling, time series forecasting, and sequence prediction.

## RNN Architecture

An RNN processes inputs one at a time while maintaining a hidden state that gets updated at each time step. The core equations governing the forward pass of an RNN are:

### 1) Hidden State Update

$$
h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
$$

### 2) Output Computation

$$
y_t = W_{hy} h_t + b_y
$$

Where:

1. $x_t$ is the input at time step $t$.
2. $h_t$ is the hidden state at time step $t$.
3. $W_{xh}$ is the weight matrix for input to hidden state.
4. $W_{hh}$ is the weight matrix for hidden state to hidden state.
5. $W_{hy}$ is the weight matrix for hidden state to output.
6. $b_h$ and $b_y$ are the bias terms for the hidden state and output, respectively.
7. $\tanh$ is the hyperbolic tangent activation function applied element-wise.

## Forward Pass Implementation

In the forward pass, we iterate over each element in the input sequence, updating the hidden state and computing the output:

1. **Initialize the hidden state** $h_0$ to zeros.
2. For each time step $t$:
   - Compute $h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$.
   - Compute $y_t = W_{hy} h_t + b_y$.
   - Store $h_t$ and $y_t$ for use in backpropagation.

## Loss Function

The loss function measures the discrepancy between the predicted outputs and the actual target values. For sequence prediction tasks, we often use the **Mean Squared Error (MSE)** loss:

$$
\text{Loss} = \frac{1}{T} \sum_{t=1}^{T} (\hat{y}_t - y_t)^2
$$

Where $T$ is the length of the sequence, $\hat{y}_t$ is the predicted output, and $y_t$ is the actual target at time step $t$.

## Backpropagation Through Time (BPTT)

BPTT is the process of training RNNs by unrolling them through time and applying backpropagation to compute gradients for each time step. The key steps in BPTT are:

1. Compute the gradient of the loss with respect to the outputs:

$$
\frac{dL}{dy_t} = \hat{y}_t - y_t
$$

2. Compute the gradients for the output layer weights and biases:

$$
dW_{hy} += \frac{dL}{dy_t} \cdot h_t^T
$$

$$
db_y += \frac{dL}{dy_t}
$$

3. Backpropagate the gradients through the hidden layers:

$$
dh_t = W_{hy}^T \cdot \frac{dL}{dy_t} + dh_{t+1}
$$

$$
dh_{\text{raw}} = dh_t \circ (1 - h_t^2)
$$

Here, $\circ$ denotes element-wise multiplication, and $(1 - h_t^2)$ is the derivative of the $\tanh$ activation function.

4. Compute the gradients for the hidden layer weights and biases:

$$
dW_{xh} += dh_{\text{raw}} \cdot x_t^T
$$

$$
dW_{hh} += dh_{\text{raw}} \cdot h_{t-1}^T
$$

$$
db_h += dh_{\text{raw}}
$$

We repeat steps 1-4 for each time step $t$ in reverse order (from $T$ to 1), accumulating the gradients. The term $dh_{t+1}$ represents the gradient flowing from the next time step, initialized to zeros at the last time step.

## Updating Weights

After computing the gradients, we update the weights and biases using gradient descent:

$$
W_{xh} -= \text{learning\_rate} \times dW_{xh}
$$

$$
W_{hh} -= \text{learning\_rate} \times dW_{hh}
$$

$$
W_{hy} -= \text{learning\_rate} \times dW_{hy}
$$

$$
b_h -= \text{learning\_rate} \times db_h
$$

$$
b_y -= \text{learning\_rate} \times db_y
$$

## Implementing the RNN

To implement the RNN with BPTT, follow these steps:

1. **Initialization**: Initialize the weight matrices $W_{xh}, W_{hh}, W_{hy}$ with small random values and biases $b_h, b_y$ with zeros.
2. **Forward Pass**: Implement the forward method to process the input sequence, updating the hidden states and computing the outputs at each time step. Store the inputs, hidden states, and outputs for use in backpropagation.
3. **Backward Pass**: Implement the backward method to perform BPTT. Compute the gradients at each time step in reverse order, accumulate them, and update the weights and biases.
4. **Training Loop**: Train the RNN over multiple epochs by repeatedly performing forward and backward passes and updating the weights.

## Tips for Implementation

1. **Gradient Clipping**: To prevent exploding gradients, consider applying gradient clipping, which scales down gradients if they exceed a certain threshold.
2. **Learning Rate**: Choose an appropriate learning rate. If the learning rate is too high, the training may become unstable.
3. **Debugging**: Check the dimensions of all matrices and vectors to ensure they align correctly during matrix multiplication.
4. **Testing**: Start with small sequences and hidden sizes to test your implementation before scaling up.

## Example Calculation

Suppose we have an input sequence $x = [x_1, x_2]$ and target sequence $y = [y_1, y_2]$. Here's how you would compute the forward and backward passes:

### 1) Forward Pass

- At $t = 1$:
  - Compute $h_1 = \tanh(W_{xh} x_1 + W_{hh} h_0 + b_h)$.
  - Compute $\hat{y}_1 = W_{hy} h_1 + b_y$.

- At $t = 2$:
  - Compute $h_2 = \tanh(W_{xh} x_2 + W_{hh} h_1 + b_h)$.
  - Compute $\hat{y}_2 = W_{hy} h_2 + b_y$.

### 2) Compute Loss

$$
L = \frac{1}{2} \left[ (\hat{y}_1 - y_1)^2 + (\hat{y}_2 - y_2)^2 \right]
$$

### 3) Backward Pass

Starting from $t = 2$ to $t = 1$:

- At $t = 2$:
  - Compute $\frac{dL}{d\hat{y}_2} = \hat{y}_2 - y_2$.
  - Backpropagate to find $dh_2$, $dW_{hy}$, $db_y$.

- At $t = 1$:
  - Use the chain rule to compute gradients with respect to inputs and weights.

## Conclusion

RNNs with BPTT are powerful for modeling sequences, but they come with challenges such as vanishing and exploding gradients. Techniques like gradient clipping, long short-term memory (LSTM) cells, or gated recurrent units (GRUs) can help mitigate these issues. Implementing an RNN from scratch provides a deeper understanding of the underlying mechanisms and prepares you for working with more complex architectures.
",
    "solution_code": "import numpy as np\n\nclass Solution:\n    class SimpleRNN:\n    def __init__(self, self, input_size, hidden_size, output_size):\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, self, x):\n        h = np.zeros((self.hidden_size, 1))  # Initialize hidden state\n        outputs = []\n        self.last_inputs = []\n        self.last_hiddens = [h]\n\n        for t in range(len(x)):\n            self.last_inputs.append(x[t].reshape(-1, 1))\n            h = np.tanh(np.dot(self.W_xh, self.last_inputs[t]) + np.dot(self.W_hh, h) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            self.last_hiddens.append(h)\n\n        self.last_outputs = outputs\n        return np.array(outputs)\n\n    def backward(self, self, x, y, learning_rate):\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        dh_next = np.zeros((self.hidden_size, 1))\n\n        for t in reversed(range(len(x))):\n            dy = self.last_outputs[t] - y[t].reshape(-1, 1)  # (Predicted - Actual)\n            dW_hy += np.dot(dy, self.last_hiddens[t+1].T)\n            db_y += dy\n\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - self.last_hiddens[t+1] ** 2) * dh  # Derivative of tanh\n\n            dW_xh += np.dot(dh_raw, self.last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, self.last_hiddens[t].T)\n            db_h += dh_raw\n\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n\n        # Update weights and biases\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nnp.random.seed(42) \n\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n\nprint(output)"
        ],
        "expected_output": [
          [
            [
              2.24143915
            ]
          ],
          [
            [
              3.18450265
            ]
          ],
          [
            [
              4.04305928
            ]
          ],
          [
            [
              4.57419398
            ]
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\n\nnp.random.seed(42) \n\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n\nprint(output)"
        ],
        "expected_output": [
          [
            [
              2.42201379
            ]
          ],
          [
            [
              3.44167595
            ]
          ],
          [
            [
              3.6129965
            ]
          ],
          [
            [
              4.50660152
            ]
          ]
        ]
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 63,
    "title": "Implement the Conjugate Gradient Method for Solving Linear Systems",
    "difficulty": "hard",
    "category": "linear algebra",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IHRoZSBDb25qdWdhdGUgR3JhZGllbnQgTWV0aG9kIGZvciBTb2x2aW5nIExpbmVhciBTeXN0ZW1zCgpZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IHRoZSBDb25qdWdhdGUgR3JhZGllbnQgKENHKSBtZXRob2QsIGFuIGVmZmljaWVudCBpdGVyYXRpdmUgYWxnb3JpdGhtIGZvciBzb2x2aW5nIGxhcmdlLCBzcGFyc2UsIHN5bW1ldHJpYywgcG9zaXRpdmUtZGVmaW5pdGUgbGluZWFyIHN5c3RlbXMuIEdpdmVuIGEgbWF0cml4IGBBYCBhbmQgYSB2ZWN0b3IgYGJgLCB0aGUgYWxnb3JpdGhtIHdpbGwgc29sdmUgZm9yIGB4YCBpbiB0aGUgc3lzdGVtIFwoIEF4ID0gYiBcKS4KCldyaXRlIGEgZnVuY3Rpb24gYGNvbmp1Z2F0ZV9ncmFkaWVudChBLCBiLCBuLCB4MD1Ob25lLCB0b2w9MWUtOClgIHRoYXQgcGVyZm9ybXMgdGhlIENvbmp1Z2F0ZSBHcmFkaWVudCBtZXRob2QgYXMgZm9sbG93czoKCi0gYEFgOiBBIHN5bW1ldHJpYywgcG9zaXRpdmUtZGVmaW5pdGUgbWF0cml4IHJlcHJlc2VudGluZyB0aGUgbGluZWFyIHN5c3RlbS4KLSBgYmA6IFRoZSB2ZWN0b3Igb24gdGhlIHJpZ2h0IHNpZGUgb2YgdGhlIGVxdWF0aW9uLgotIGBuYDogTWF4aW11bSBudW1iZXIgb2YgaXRlcmF0aW9ucy4KLSBgeDBgOiBJbml0aWFsIGd1ZXNzIGZvciB0aGUgc29sdXRpb24gdmVjdG9yLgotIGB0b2xgOiBUb2xlcmFuY2UgZm9yIHN0b3BwaW5nIGNyaXRlcmlhLgoKVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gdGhlIHNvbHV0aW9uIHZlY3RvciBgeGAuCgogICAg",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def conjugate_gradient(self, A, b, n, x0=None, tol=1e-8):\n    \t\"\"\"\n    \tSolve the system Ax = b using the Conjugate Gradient method.\n\n    \t:param A: Symmetric positive-definite matrix\n    \t:param b: Right-hand side vector\n    \t:param n: Maximum number of iterations\n    \t:param x0: Initial guess for solution (default is zero vector)\n    \t:param tol: Convergence tolerance\n    \t:return: Solution vector x\n    \t\"\"\"\n    \t# calculate initial residual vector\n    \tx = np.zeros_like(b)\n",
    "example_input": "A = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 5\n\nprint(conjugate_gradient(A, b, n))",
    "example_output": "[0.09090909, 0.63636364]",
    "example_reasoning": "The Conjugate Gradient method is applied to the linear system Ax = b with the given matrix A and vector b. The algorithm iteratively refines the solution to converge to the exact solution.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def conjugate_gradient(self, A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8) -> np.array:\n\n        # calculate initial residual vector\n        x = np.zeros_like(b)\n        r = residual(A, b, x) # residual vector\n        rPlus1 = r\n        p = r # search direction vector\n\n        for i in range(n):\n\n            # line search step value - this minimizes the error along the current search direction\n            alp = alpha(A, r, p)\n\n            # new x and r based on current p (the search direction vector)\n            x = x + alp * p\n            rPlus1 = r - alp * (A@p)\n\n            # calculate beta - this ensures that all vectors are A-orthogonal to each other\n            bet = beta(r, rPlus1)\n\n            # update x and r\n            # using a othogonal search direction ensures we get all the information we need in more direction and then don't have to search in that direction again\n            p = rPlus1 + bet * p\n\n            # update residual vector\n            r = rPlus1\n\n            # break if less than tolerance\n            if np.linalg.norm(residual(A,b,x)) < tol:\n                break\n\n        return x\n\n    def residual(self, A: np.array, b: np.array, x: np.array) -> np.array:\n        # calculate linear system residuals\n        return b - A @ x\n\n    def alpha(self, A: np.array, r: np.array, p: np.array) -> float:\n\n        # calculate step size\n        alpha_num = np.dot(r, r)\n        alpha_den = np.dot(p @ A, p)\n\n        return alpha_num/alpha_den\n\n    def beta(self, r: np.array, r_plus1: np.array) -> float:\n\n        # calculate direction scaling\n        beta_num = np.dot(r_plus1, r_plus1)\n        beta_den = np.dot(r, r)\n\n        return beta_num/beta_den\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 5\nprint(conjugate_gradient(A, b, n))"
        ],
        "expected_output": [
          0.09090909,
          0.63636364
        ]
      },
      {
        "test": [
          "import numpy as np\n\nA = np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]])\nb = np.array([7, 8, 5])\nn = 1\nprint(conjugate_gradient(A, b, n))"
        ],
        "expected_output": [
          1.2627451,
          1.44313725,
          0.90196078
        ]
      }
    ],
    "function_name": "conjugate_gradient"
  },
  {
    "id": 64,
    "title": "Implement Gini Impurity Calculation for a Set of Classes",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "IyMgVGFzazogSW1wbGVtZW50IEdpbmkgSW1wdXJpdHkgQ2FsY3VsYXRpb24KCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0aGF0IGNhbGN1bGF0ZXMgdGhlIEdpbmkgSW1wdXJpdHkgZm9yIGEgc2V0IG9mIGNsYXNzZXMuIEdpbmkgaW1wdXJpdHkgaXMgY29tbW9ubHkgdXNlZCBpbiBkZWNpc2lvbiB0cmVlIGFsZ29yaXRobXMgdG8gbWVhc3VyZSB0aGUgaW1wdXJpdHkgb3IgZGlzb3JkZXIgd2l0aGluIGEgbm9kZS4K",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def gini_impurity(self, y):\n    \t\"\"\"\n    \tCalculate Gini Impurity for a list of class labels.\n\n    \t:param y: List of class labels\n    \t:return: Gini Impurity rounded to three decimal places\n    \t\"\"\"\n    \tpass\n    \treturn round(val,3)",
    "example_input": "y = [0, 1, 1, 1, 0]\nprint(gini_impurity(y))",
    "example_output": "0.48",
    "example_reasoning": "The Gini Impurity is calculated as 1 - (p_0^2 + p_1^2), where p_0 and p_1 are the probabilities of each class. In this case, p_0 = 2/5 and p_1 = 3/5, resulting in a Gini Impurity of 0.48.",
    "learn_content": "CiMjIFVuZGVyc3RhbmRpbmcgR2luaSBJbXB1cml0eQoKR2luaSBpbXB1cml0eSBpcyBhIHN0YXRpc3RpY2FsIG1lYXN1cmVtZW50IG9mIHRoZSBpbXB1cml0eSBvciBkaXNvcmRlciBpbiBhIGxpc3Qgb2YgZWxlbWVudHMuIEl0IGlzIGNvbW1vbmx5IHVzZWQgaW4gZGVjaXNpb24gdHJlZSBhbGdvcml0aG1zIHRvIGRlY2lkZSB0aGUgb3B0aW1hbCBzcGxpdCBhdCB0cmVlIG5vZGVzLiBJdCBpcyBjYWxjdWxhdGVkIGFzIGZvbGxvd3MsIHdoZXJlICQgcF9pICQgaXMgdGhlIHByb2JhYmlsaXR5IG9mIGVhY2ggY2xhc3MsICQgXGZyYWN7bl9pfXtufSAkOgoKJCQKXHRleHR7R2luaSBJbXB1cml0eX0gPSAxIC0gXHN1bV97aT0xfV57Q30gcF9pXjIKJCQKCkEgR2luaSBpbXB1cml0eSBvZiAwIGluZGljYXRlcyBhIG5vZGUgd2hlcmUgYWxsIGVsZW1lbnRzIGJlbG9uZyB0byB0aGUgc2FtZSBjbGFzcywgd2hlcmVhcyBhIEdpbmkgaW1wdXJpdHkgb2YgMS0xL0MgaW5kaWNhdGVzIG1heGltdW0gaW1wdXJpdHksIHdoZXJlIGVsZW1lbnRzIGFyZSBldmVubHkgZGlzdHJpYnV0ZWQgYW1vbmcgZWFjaCBjbGFzcy4gVGhpcyBtZWFucyB0aGF0IGEgbG93ZXIgaW1wdXJpdHkgaW1wbGllcyBhIGxlc3MgaG9tb2dlbmVvdXMgZGlzdHJpYnV0aW9uIG9mIGVsZW1lbnRzLCBzdWdnZXN0aW5nIGEgZ29vZCBzcGxpdCwgYXMgZGVjaXNpb24gdHJlZXMgYWltIHRvIG1pbmltaXplIGl0IGF0IGVhY2ggbm9kZS4KCiMjIyBBZHZhbnRhZ2VzIGFuZCBMaW1pdGF0aW9ucwoKIyMjIyBBZHZhbnRhZ2VzOgotIENvbXB1dGF0aW9uYWxseSBlZmZpY2llbnQKLSBXb3JrcyBmb3IgYmluYXJ5IGFuZCBtdWx0aS1jbGFzcyBjbGFzc2lmaWNhdGlvbgoKIyMjIyBMaW1pdGF0aW9uczoKLSBCaWFzZWQgdG93YXJkIGxhcmdlciBjbGFzc2VzCi0gTWF5IGNhdXNlIG92ZXJmaXR0aW5nIGluIGRlZXAgZGVjaXNpb24gdHJlZXMKCiMjIyBFeGFtcGxlIENhbGN1bGF0aW9uCgpTdXBwb3NlIHdlIGhhdmUgdGhlIHNldDogJFswLCAxLCAxLCAxLCAwXSQuIFRoZSBwcm9iYWJpbGl0eSBvZiBlYWNoIGNsYXNzIGlzIGNhbGN1bGF0ZWQgYXMgZm9sbG93czoKCiQkCnBfezB9ID0gXGZyYWN7Mn17NX0gXHF1YWQgcF97MX0gPSBcZnJhY3szfXs1fQokJAoKVGhlIEdpbmkgSW1wdXJpdHkgaXMgdGhlbiBjYWxjdWxhdGVkIGFzIGZvbGxvd3M6CgokJApcdGV4dHtHaW5pIEltcHVyaXR5fSA9IDEgLSAocF8wXjIgKyBwXzFeMikgPSAxIC0gXGxlZnQoXGxlZnQoXGZyYWN7Mn17NX1ccmlnaHQpXjIgKyBcbGVmdChcZnJhY3szfXs1fVxyaWdodCleMlxyaWdodCkgPSAwLjQ4CiQkCgo=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def gini_impurity(self, y: list[int]) -> float:\n\n        classes = set(y)\n        n = len(y)\n\n        gini_impurity = 0\n\n        for cls in classes:\n            gini_impurity += (y.count(cls)/n)**2\n\n        return round(1-gini_impurity,3)\n",
    "test_cases": [
      {
        "test": [
          "y = [0, 0, 0, 0, 1, 1, 1, 1]\nprint(gini_impurity(y))"
        ],
        "expected_output": 0.5
      },
      {
        "test": [
          "y = [0, 0, 0, 0, 0, 1]\nprint(gini_impurity(y))"
        ],
        "expected_output": 0.278
      }
    ],
    "function_name": "gini_impurity"
  },
  {
    "id": 65,
    "title": "Implement Compressed Row Sparse Matrix (CSR) Format Conversion",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "IyMgVGFzazogQ29udmVydCBhIERlbnNlIE1hdHJpeCB0byBDb21wcmVzc2VkIFJvdyBTcGFyc2UgKENTUikgRm9ybWF0CgpZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjb252ZXJ0cyBhIGdpdmVuIGRlbnNlIG1hdHJpeCBpbnRvIHRoZSBDb21wcmVzc2VkIFJvdyBTcGFyc2UgKENTUikgZm9ybWF0LCBhbiBlZmZpY2llbnQgc3RvcmFnZSByZXByZXNlbnRhdGlvbiBmb3Igc3BhcnNlIG1hdHJpY2VzLiBUaGUgQ1NSIGZvcm1hdCBvbmx5IHN0b3JlcyBub24temVybyBlbGVtZW50cyBhbmQgdGhlaXIgcG9zaXRpb25zLCBzaWduaWZpY2FudGx5IHJlZHVjaW5nIG1lbW9yeSB1c2FnZSBmb3IgbWF0cmljZXMgd2l0aCBhIGxhcmdlIG51bWJlciBvZiB6ZXJvcy4KCldyaXRlIGEgZnVuY3Rpb24gYGNvbXByZXNzZWRfcm93X3NwYXJzZV9tYXRyaXgoZGVuc2VfbWF0cml4KWAgdGhhdCB0YWtlcyBhIDJEIGxpc3QgYGRlbnNlX21hdHJpeGAgYXMgaW5wdXQgYW5kIHJldHVybnMgYSB0dXBsZSBjb250YWluaW5nIHRocmVlIGxpc3RzOgoKLSAqKlZhbHVlcyBhcnJheSoqOiBMaXN0IG9mIGFsbCBub24temVybyBlbGVtZW50cyBpbiByb3ctbWFqb3Igb3JkZXIuCi0gKipDb2x1bW4gaW5kaWNlcyBhcnJheSoqOiBDb2x1bW4gaW5kZXggZm9yIGVhY2ggbm9uLXplcm8gZWxlbWVudCBpbiB0aGUgdmFsdWVzIGFycmF5LgotICoqUm93IHBvaW50ZXIgYXJyYXkqKjogQ3VtdWxhdGl2ZSBudW1iZXIgb2Ygbm9uLXplcm8gZWxlbWVudHMgcGVyIHJvdywgaW5kaWNhdGluZyB0aGUgc3RhcnQgb2YgZWFjaCByb3cgaW4gdGhlIHZhbHVlcyBhcnJheS4KCiAgICA=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def compressed_row_sparse_matrix(self, dense_matrix):\n    \t\"\"\"\n    \tConvert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    \t:param dense_matrix: 2D list representing a dense matrix\n    \t:return: A tuple containing (values array, column indices array, row pointer array)\n    \t\"\"\"\n    \tpass\n",
    "example_input": "dense_matrix = [\n    [1, 0, 0, 0],\n    [0, 2, 0, 0],\n    [3, 0, 4, 0],\n    [1, 0, 0, 5]\n]\n\nvals, col_idx, row_ptr = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values array:\", vals)\nprint(\"Column indices array:\", col_idx)\nprint(\"Row pointer array:\", row_ptr)",
    "example_output": "Values array: [1, 2, 3, 4, 1, 5]\nColumn indices array: [0, 1, 0, 2, 0, 3]\nRow pointer array: [0, 1, 2, 4, 6]",
    "example_reasoning": "The dense matrix is converted to CSR format with the values array containing non-zero elements, column indices array storing the corresponding column index, and row pointer array indicating the start of each row in the values array.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def compressed_row_sparse_matrix(self, dense_matrix):\n        vals = []\n        col_idx = []\n        row_ptr = [0]\n\n        for row in dense_matrix:\n            for j, val in enumerate(row):\n                if val != 0:\n                    vals.append(val)\n                    col_idx.append(j)\n            row_ptr.append(len(vals))\n\n        return vals, col_idx, row_ptr\n",
    "test_cases": [
      {
        "test": [
          "dense_matrix = [\n    [1, 0, 0, 0],\n    [0, 2, 0, 0],\n    [3, 0, 4, 0],\n    [1, 0, 0, 5]\n]\nvals, col_idx, row_ptr = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values array:\", vals)\nprint(\"Column indices array:\", col_idx)\nprint(\"Row pointer array:\", row_ptr)"
        ],
        "expected_output": "Values array: [1, 2, 3, 4, 1, 5]\nColumn indices array: [0, 1, 0, 2, 0, 3]\nRow pointer array: [0, 1, 2, 4, 6]"
      },
      {
        "test": [
          "dense_matrix = [\n    [0, 0, 0],\n    [1, 2, 0],\n    [0, 3, 4]\n]\nvals, col_idx, row_ptr = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values array:\", vals)\nprint(\"Column indices array:\", col_idx)\nprint(\"Row pointer array:\", row_ptr)"
        ],
        "expected_output": "Values array: [1, 2, 3, 4]\nColumn indices array: [0, 1, 1, 2]\nRow pointer array: [0, 0, 2, 4]"
      }
    ],
    "function_name": "compressed_row_sparse_matrix"
  },
  {
    "id": 66,
    "title": "Implement Orthogonal Projection of a Vector onto a Line",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "IyMgVGFzazogQ29tcHV0ZSB0aGUgT3J0aG9nb25hbCBQcm9qZWN0aW9uIG9mIGEgVmVjdG9yCgpZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSBvcnRob2dvbmFsIHByb2plY3Rpb24gb2YgYSB2ZWN0b3IgKip2Kiogb250byBhbm90aGVyIHZlY3RvciAqKkwqKi4gVGhpcyBwcm9qZWN0aW9uIHJlc3VsdHMgaW4gdGhlIHZlY3RvciBvbiAqKkwqKiB0aGF0IGlzIGNsb3Nlc3QgdG8gKip2KiouCgpXcml0ZSBhIGZ1bmN0aW9uIGBvcnRob2dvbmFsX3Byb2plY3Rpb24odiwgTClgIHRoYXQgdGFrZXMgaW4gdHdvIGxpc3RzLCBgdmAgKHRoZSB2ZWN0b3IgdG8gYmUgcHJvamVjdGVkKSBhbmQgYExgICh0aGUgbGluZSB2ZWN0b3IpLCBhbmQgcmV0dXJucyB0aGUgb3J0aG9nb25hbCBwcm9qZWN0aW9uIG9mIGB2YCBvbnRvIGBMYC4gVGhlIGZ1bmN0aW9uIHNob3VsZCBvdXRwdXQgYSBsaXN0IHJlcHJlc2VudGluZyB0aGUgcHJvamVjdGlvbiB2ZWN0b3Igcm91bmRlZCB0byB0aHJlZSBkZWNpbWFsIHBsYWNlcy4KCiAgICA=",
    "starter_code": "\n\nclass Solution:\n    def orthogonal_projection(self, v, L):\n    \t\"\"\"\n    \tCompute the orthogonal projection of vector v onto line L.\n\n    \t:param v: The vector to be projected\n    \t:param L: The line vector defining the direction of projection\n    \t:return: List representing the projection of v onto L\n    \t\"\"\"\n    \tpass\n",
    "example_input": "v = [3, 4]\nL = [1, 0]\nprint(orthogonal_projection(v, L))",
    "example_output": "[3.0, 0.0]",
    "example_reasoning": "The orthogonal projection of vector [3, 4] onto the line defined by [1, 0] results in the projection vector [3, 0], which lies on the line [1, 0].",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def dot(self, v1, v2):\n        return sum([ax1 * ax2 for ax1, ax2 in zip(v1, v2)])\n\n    def scalar_mult(self, scalar, v):\n        return [scalar * ax for ax in v]\n\n    def orthogonal_projection(self, v, L):\n        L_mag_sq = dot(L, L)\n        proj_scalar = dot(v, L) / L_mag_sq\n        proj_v = scalar_mult(proj_scalar, L)\n        return [round(x, 3) for x in proj_v]\n",
    "test_cases": [
      {
        "test": [
          "v = [3, 4]\nL = [1, 0]\nprint(orthogonal_projection(v, L))"
        ],
        "expected_output": [
          3.0,
          0.0
        ]
      },
      {
        "test": [
          "v = [1, 2, 3]\nL = [0, 0, 1]\nprint(orthogonal_projection(v, L))"
        ],
        "expected_output": [
          0.0,
          0.0,
          3.0
        ]
      }
    ],
    "function_name": "orthogonal_projection"
  },
  {
    "id": 67,
    "title": "Implement Compressed Column Sparse Matrix Format (CSC)",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "IyMgVGFzazogQ3JlYXRlIGEgQ29tcHJlc3NlZCBDb2x1bW4gU3BhcnNlIE1hdHJpeCBSZXByZXNlbnRhdGlvbgoKWW91ciB0YXNrIGlzIHRvIGltcGxlbWVudCBhIGZ1bmN0aW9uIHRoYXQgY29udmVydHMgYSBkZW5zZSBtYXRyaXggaW50byBpdHMgQ29tcHJlc3NlZCBDb2x1bW4gU3BhcnNlIChDU0MpIHJlcHJlc2VudGF0aW9uLiBUaGUgQ1NDIGZvcm1hdCBzdG9yZXMgb25seSBub24temVybyBlbGVtZW50cyBvZiB0aGUgbWF0cml4IGFuZCBpcyBlZmZpY2llbnQgZm9yIG1hdHJpY2VzIHdpdGggYSBoaWdoIG51bWJlciBvZiB6ZXJvIGVsZW1lbnRzLgoKV3JpdGUgYSBmdW5jdGlvbiBgY29tcHJlc3NlZF9jb2xfc3BhcnNlX21hdHJpeChkZW5zZV9tYXRyaXgpYCB0aGF0IHRha2VzIGluIGEgdHdvLWRpbWVuc2lvbmFsIGxpc3QgYGRlbnNlX21hdHJpeGAgYW5kIHJldHVybnMgYSB0dXBsZSBvZiB0aHJlZSBsaXN0czoKCi0gYHZhbHVlc2A6IExpc3Qgb2Ygbm9uLXplcm8gZWxlbWVudHMsIHN0b3JlZCBpbiBjb2x1bW4tbWFqb3Igb3JkZXIuCi0gYHJvdyBpbmRpY2VzYDogTGlzdCBvZiByb3cgaW5kaWNlcyBjb3JyZXNwb25kaW5nIHRvIGVhY2ggdmFsdWUgaW4gdGhlIHZhbHVlcyBhcnJheS4KLSBgY29sdW1uIHBvaW50ZXJgOiBMaXN0IHRoYXQgaW5kaWNhdGVzIHRoZSBzdGFydGluZyBpbmRleCBvZiBlYWNoIGNvbHVtbiBpbiB0aGUgdmFsdWVzIGFycmF5LgoKICAgIA==",
    "starter_code": "\n\nclass Solution:\n    def compressed_col_sparse_matrix(self, dense_matrix):\n    \t\"\"\"\n    \tConvert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    \t:param dense_matrix: List of lists representing the dense matrix\n    \t:return: Tuple of (values, row indices, column pointer)\n    \t\"\"\"\n    \tpass\n",
    "example_input": "dense_matrix = [\n    [0, 0, 3, 0],\n    [1, 0, 0, 4],\n    [0, 2, 0, 0]\n]\n\nvals, row_idx, col_ptr = compressed_col_sparse_matrix(dense_matrix)",
    "example_output": "[1, 2, 3, 4] [1, 2, 0, 1] [0, 1, 2, 3, 4]",
    "example_reasoning": "The dense matrix is converted to CSC format with the values array containing non-zero elements, row indices array storing the corresponding row index, and column pointer array indicating the start of each column in the values array.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def compressed_col_sparse_matrix(self, dense_matrix):\n        vals = []\n        row_idx = []\n        col_ptr = [0]\n\n        rows, cols = len(dense_matrix), len(dense_matrix[0])\n\n        for i in range(cols):\n            for j in range(rows):\n                val = dense_matrix[j][i]\n                if val != 0:\n                    vals.append(val)\n                    row_idx.append(j)\n            col_ptr.append(len(vals))\n\n        return vals, row_idx, col_ptr\n",
    "test_cases": [
      {
        "test": [
          "dense_matrix = [\n    [0, 0, 0],\n    [0, 0, 0],\n    [0, 0, 0]\n]\nvals, row_idx, col_ptr = compressed_col_sparse_matrix(dense_matrix)\nprint(vals)\nprint(row_idx)\nprint(col_ptr)"
        ],
        "expected_output": "[]\n[]\n[0, 0, 0, 0]"
      },
      {
        "test": [
          "dense_matrix = [\n    [0, 0, 0],\n    [1, 2, 0],\n    [0, 3, 4]\n]\nvals, row_idx, col_ptr = compressed_col_sparse_matrix(dense_matrix)\nprint(vals)\nprint(row_idx)\nprint(col_ptr)"
        ],
        "expected_output": "[1, 2, 3, 4]\n[1, 1, 2, 2]\n[0, 1, 3, 4]"
      }
    ],
    "function_name": "compressed_col_sparse_matrix"
  },
  {
    "id": 68,
    "title": "Find the Image of a Matrix Using Row Echelon Form",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "CiMjIFRhc2s6IENvbXB1dGUgdGhlIENvbHVtbiBTcGFjZSBvZiBhIE1hdHJpeAoKSW4gdGhpcyB0YXNrLCB5b3UgYXJlIHJlcXVpcmVkIHRvIGltcGxlbWVudCBhIGZ1bmN0aW9uIGBtYXRyaXhfaW1hZ2UoQSlgIHRoYXQgY2FsY3VsYXRlcyB0aGUgY29sdW1uIHNwYWNlIG9mIGEgZ2l2ZW4gbWF0cml4IGBBYC4gVGhlIGNvbHVtbiBzcGFjZSwgYWxzbyBrbm93biBhcyB0aGUgaW1hZ2Ugb3Igc3BhbiwgY29uc2lzdHMgb2YgYWxsIGxpbmVhciBjb21iaW5hdGlvbnMgb2YgdGhlIGNvbHVtbnMgb2YgYEFgLiBUbyBmaW5kIHRoaXMsIHlvdSdsbCB1c2UgY29uY2VwdHMgZnJvbSBsaW5lYXIgYWxnZWJyYSwgZm9jdXNpbmcgb24gaWRlbnRpZnlpbmcgaW5kZXBlbmRlbnQgY29sdW1ucyB0aGF0IHNwYW4gdGhlIG1hdHJpeCdzIGltYWdlLgoqKllvdXIgdGFzazoqKiBJbXBsZW1lbnQgdGhlIGZ1bmN0aW9uIGBtYXRyaXhfaW1hZ2UoQSlgIHRvIHJldHVybiB0aGUgYmFzaXMgdmVjdG9ycyB0aGF0IHNwYW4gdGhlIGNvbHVtbiBzcGFjZSBvZiBgQWAuIFRoZXNlIHZlY3RvcnMgc2hvdWxkIGJlIGV4dHJhY3RlZCBmcm9tIHRoZSBvcmlnaW5hbCBtYXRyaXggYW5kIGNvcnJlc3BvbmQgdG8gdGhlIGluZGVwZW5kZW50IGNvbHVtbnMuCgo=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def matrix_image(self, A):\n    \t# Write your code here\n    \tpass\n",
    "example_input": "matrix = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\nprint(matrix_image(matrix))",
    "example_output": "# [[1, 2],\n#  [4, 5],\n#  [7, 8]]",
    "example_reasoning": "The column space of the matrix is spanned by the independent columns [1, 2], [4, 5], and [7, 8]. These columns form the basis vectors that represent the image of the matrix.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def rref(self, A):\n        # Convert to float for division operations\n        A = A.astype(np.float32)\n        n, m = A.shape\n\n        for i in range(n):\n            if A[i, i] == 0:\n                nonzero_current_row = np.nonzero(A[i:, i])[0] + i\n                if len(nonzero_current_row) == 0:\n                    continue\n                A[[i, nonzero_current_row[0]]] = A[[nonzero_current_row[0], i]]\n\n            A[i] = A[i] / A[i, i]\n\n            for j in range(n):\n                if i != j:\n                    A[j] -= A[i] * A[j, i]\n        return A\n\n    def find_pivot_columns(self, A):\n        n, m = A.shape\n        pivot_columns = []\n        for i in range(n):\n            nonzero = np.nonzero(A[i, :])[0]\n            if len(nonzero) != 0:\n                pivot_columns.append(nonzero[0])\n        return pivot_columns\n\n    def matrix_image(self, A):\n        # Find the RREF of the matrix\n        Arref = rref(A)\n        # Find the pivot columns\n        pivot_columns = find_pivot_columns(Arref)\n        # Extract the pivot columns from the original matrix\n        image_basis = A[:, pivot_columns]\n        return image_basis\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nmatrix = np.array([[1, 0], [0, 1]])\nprint(matrix_image(matrix))"
        ],
        "expected_output": [
          [
            1,
            0
          ],
          [
            0,
            1
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\nmatrix = np.array([[1, 2], [2, 4]])\nprint(matrix_image(matrix))"
        ],
        "expected_output": [
          [
            1
          ],
          [
            2
          ]
        ]
      }
    ],
    "function_name": "matrix_image"
  },
  {
    "id": 69,
    "title": "Calculate R-squared for Regression Analysis",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "CiMjIFRhc2s6IENvbXB1dGUgdGhlIFItc3F1YXJlZCBWYWx1ZSBpbiBSZWdyZXNzaW9uIEFuYWx5c2lzCgotIFItc3F1YXJlZCwgYWxzbyBrbm93biBhcyB0aGUgY29lZmZpY2llbnQgb2YgZGV0ZXJtaW5hdGlvbiwgaXMgYSBtZWFzdXJlIHRoYXQgaW5kaWNhdGVzIGhvdyB3ZWxsIHRoZSBpbmRlcGVuZGVudCB2YXJpYWJsZXMgZXhwbGFpbiB0aGUgdmFyaWFiaWxpdHkgb2YgdGhlIGRlcGVuZGVudCB2YXJpYWJsZSBpbiBhIHJlZ3Jlc3Npb24gbW9kZWwuIAoKLSAqKllvdXIgVGFzayoqOiAKICAgIFRvIGltcGxlbWVudCB0aGUgZnVuY3Rpb24gYHJfc3F1YXJlZCh5X3RydWUsIHlfcHJlZClgIHRoYXQgY2FsY3VsYXRlcyB0aGUgUi1zcXVhcmVkIHZhbHVlLCBnaXZlbiBhcnJheXMgb2YgdHJ1ZSB2YWx1ZXMgYHlfdHJ1ZWAgYW5kIHByZWRpY3RlZCB2YWx1ZXMgYHlfcHJlZGAuCg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def r_squared(self, y_true, y_pred):\n    \t# Write your code here\n    \tpass\n",
    "example_input": "import numpy as np\n\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 2.1, 2.9, 4.2, 4.8])\nprint(r_squared(y_true, y_pred))",
    "example_output": "0.989",
    "example_reasoning": "The R-squared value is calculated to be 0.989, indicating that the regression model explains 98.9% of the variance in the dependent variable.",
    "learn_content": "CiMgVW5kZXJzdGFuZGluZyBSLXNxdWFyZWQgKFLCsikgaW4gUmVncmVzc2lvbiBBbmFseXNpcwoKUi1zcXVhcmVkLCBhbHNvIGtub3duIGFzIHRoZSBjb2VmZmljaWVudCBvZiBkZXRlcm1pbmF0aW9uLCBpcyBhIHN0YXRpc3RpY2FsIG1lYXN1cmUgdGhhdCByZXByZXNlbnRzIHRoZSBwcm9wb3J0aW9uIG9mIHRoZSB2YXJpYW5jZSBmb3IgYSBkZXBlbmRlbnQgdmFyaWFibGUgdGhhdCdzIGV4cGxhaW5lZCBieSBhbiBpbmRlcGVuZGVudCB2YXJpYWJsZSBvciB2YXJpYWJsZXMgaW4gYSByZWdyZXNzaW9uIG1vZGVsLiBJdCBwcm92aWRlcyBpbnNpZ2h0IGludG8gaG93IHdlbGwgdGhlIG1vZGVsIGZpdHMgdGhlIGRhdGEuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KClRoZSBSLXNxdWFyZWQgdmFsdWUgaXMgY2FsY3VsYXRlZCB1c2luZyB0aGUgZm9sbG93aW5nIGZvcm11bGE6ICAKJCQKUl4yID0gMSAtIFxmcmFje1x0ZXh0e1NTUn19e1x0ZXh0e1NTVH19CiQkCldoZXJlOiAgCgoxKSAkIFx0ZXh0e1NTUn0gJCAoU3VtIG9mIFNxdWFyZWQgUmVzaWR1YWxzKTogVGhlIHN1bSBvZiB0aGUgc3F1YXJlcyBvZiB0aGUgZGlmZmVyZW5jZXMgYmV0d2VlbiB0aGUgYWN0dWFsIHZhbHVlcyBhbmQgdGhlIHByZWRpY3RlZCB2YWx1ZXMuICAKMikgJCBcdGV4dHtTU1R9ICQgKFRvdGFsIFN1bSBvZiBTcXVhcmVzKTogVGhlIHN1bSBvZiB0aGUgc3F1YXJlcyBvZiB0aGUgZGlmZmVyZW5jZXMgYmV0d2VlbiB0aGUgYWN0dWFsIHZhbHVlcyBhbmQgdGhlIG1lYW4gb2YgdGhlIGFjdHVhbCB2YWx1ZXMuCgojIyMgRXF1YXRpb25zIGZvciBTU1IgYW5kIFNTVAoKVG8gY2FsY3VsYXRlIFNTUiBhbmQgU1NULCB3ZSB1c2UgdGhlIGZvbGxvd2luZyBmb3JtdWxhczogIAoKMSkgU1NSOiAgCiQkClx0ZXh0e1NTUn0gPSBcc3VtX3tpPTF9XntufSAoeV9pIC0gXGhhdHt5fV9pKV4yCiQkICAKCjIpIFNTVDogIAokJApcdGV4dHtTU1R9ID0gXHN1bV97aT0xfV57bn0gKHlfaSAtIFxiYXJ7eX0pXjIKJCQgIAoKV2hlcmU6ICAKCjEpICQgeV9pICQ6IEFjdHVhbCB2YWx1ZSAgCjIpICQgXGhhdHt5fV9pICQ6IFByZWRpY3RlZCB2YWx1ZSAgCjMpICQgXGJhcnt5fSAkOiBNZWFuIG9mIHRoZSBhY3R1YWwgdmFsdWVzICAKCiMjIyBTaWduaWZpY2FuY2Ugb2YgUi1zcXVhcmVkCgpSLXNxdWFyZWQgaXMgYSBrZXkgbWV0cmljIGZvciBldmFsdWF0aW5nIGhvdyB3ZWxsIGEgcmVncmVzc2lvbiBtb2RlbCBwZXJmb3Jtcy4gQSBoaWdoZXIgUi1zcXVhcmVkIHZhbHVlIGluZGljYXRlcyBhIGJldHRlciBmaXQgZm9yIHRoZSBtb2RlbCwgbWVhbmluZyBpdCBjYW4gZXhwbGFpbiBtb3JlIHZhcmlhYmlsaXR5IGluIHRoZSBkYXRhLiBIb3dldmVyLCBpdCdzIGltcG9ydGFudCB0byBub3RlOiAgCgotIEEgaGlnaCBSLXNxdWFyZWQgZG9lcyBub3QgYWx3YXlzIGltcGx5IHRoYXQgdGhlIG1vZGVsIGlzIGdvb2Q7IGl0IGNhbiBzb21ldGltZXMgYmUgbWlzbGVhZGluZyBpZiBvdmVyZml0dGluZyBvY2N1cnMuICAKLSBJdCBzaG91bGQgYmUgdXNlZCBpbiBjb25qdW5jdGlvbiB3aXRoIG90aGVyIG1ldHJpY3MgZm9yIGNvbXByZWhlbnNpdmUgbW9kZWwgZXZhbHVhdGlvbi4KCiMjIyBJbXBsZW1lbnRpbmcgUi1zcXVhcmVkIENhbGN1bGF0aW9uCgpJbiB0aGlzIHByb2JsZW0sIHlvdSB3aWxsIGltcGxlbWVudCBhIGZ1bmN0aW9uIHRvIGNhbGN1bGF0ZSBSLXNxdWFyZWQgZ2l2ZW4gYXJyYXlzIG9mIHRydWUgYW5kIHByZWRpY3RlZCB2YWx1ZXMgZnJvbSBhIHJlZ3Jlc3Npb24gdGFzay4gVGhlIHJlc3VsdHMgc2hvdWxkIGJlIHJvdW5kZWQgdG8gdGhyZWUgZGVjaW1hbCBwbGFjZXMuICAKCkluIHRoZSBzb2x1dGlvbiwgdGhlIGltcGxlbWVudGVkICQgclxfc3F1YXJlZCgpICQgZnVuY3Rpb24gY2FsY3VsYXRlcyBSLXNxdWFyZWQgYnkgZmlyc3QgZGV0ZXJtaW5pbmcgU1NSIGFuZCBTU1QsIHRoZW4gYXBwbHlpbmcgdGhlbSB0byBjb21wdXRlICQgUl4yICQuIEl0IGhhbmRsZXMgZWRnZSBjYXNlcyBzdWNoIGFzIHBlcmZlY3QgcHJlZGljdGlvbnMgYW5kIHNpdHVhdGlvbnMgd2hlcmUgYWxsIHRydWUgdmFsdWVzIGFyZSBpZGVudGljYWwuCgojIyMgUmVmZXJlbmNlCgpZb3UgY2FuIHJlZmVyIHRvIHRoaXMgcmVzb3VyY2UgZm9yIG1vcmUgaW5mb3JtYXRpb246ICAKW0NvZWZmaWNpZW50IG9mIERldGVybWluYXRpb25dKGh0dHBzOi8vd3d3Lm5jbC5hYy51ay93ZWJ0ZW1wbGF0ZS9hc2stYXNzZXRzL2V4dGVybmFsL21hdGhzLXJlc291cmNlcy9zdGF0aXN0aWNzL3JlZ3Jlc3Npb24tYW5kLWNvcnJlbGF0aW9uL2NvZWZmaWNpZW50LW9mLWRldGVybWluYXRpb24tci1zcXVhcmVkLmh0bWwpCg==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def r_squared(self, y_true, y_pred):\n        \"\"\"\n        Calculate the R-squared (R\u00b2) coefficient of determination.\n\n        Args:\n            y_true (numpy.ndarray): Array of true values\n            y_pred (numpy.ndarray): Array of predicted values\n\n        Returns:\n            float: R-squared value rounded to 3 decimal places\n        \"\"\"\n        if np.array_equal(y_true, y_pred):\n            return 1.0\n\n        # Calculate mean of true values\n        y_mean = np.mean(y_true)\n\n        # Calculate Sum of Squared Residuals (SSR)\n        ssr = np.sum((y_true - y_pred) ** 2)\n\n        # Calculate Total Sum of Squares (SST)\n        sst = np.sum((y_true - y_mean) ** 2)\n\n        try:\n            # Calculate R-squared\n            r2 = 1 - (ssr / sst)\n            if np.isinf(r2):\n                return 0.0\n            return round(r2, 3)\n        except ZeroDivisionError:\n            return 0.0\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1, 2, 3, 4, 5])\nprint(r_squared(y_true, y_pred))"
        ],
        "expected_output": 1.0
      },
      {
        "test": [
          "import numpy as np\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 2.1, 2.9, 4.2, 4.8])\nprint(r_squared(y_true, y_pred))"
        ],
        "expected_output": 0.989
      }
    ],
    "function_name": "r_squared"
  },
  {
    "id": 70,
    "title": "Calculate Image Brightness",
    "difficulty": "easy",
    "category": "computer vision",
    "problem_description": "CiMjIFRhc2s6IEltYWdlIEJyaWdodG5lc3MgQ2FsY3VsYXRvcgoKSW4gdGhpcyB0YXNrLCB5b3Ugd2lsbCBpbXBsZW1lbnQgYSBmdW5jdGlvbiBgY2FsY3VsYXRlX2JyaWdodG5lc3MoaW1nKWAgdGhhdCBjYWxjdWxhdGVzIHRoZSBhdmVyYWdlIGJyaWdodG5lc3Mgb2YgYSBncmF5c2NhbGUgaW1hZ2UuIFRoZSBpbWFnZSBpcyByZXByZXNlbnRlZCBhcyBhIDJEIG1hdHJpeCwgd2hlcmUgZWFjaCBlbGVtZW50IHJlcHJlc2VudHMgYSBwaXhlbCB2YWx1ZSBiZXR3ZWVuIDAgKGJsYWNrKSBhbmQgMjU1ICh3aGl0ZSkuCgojIyMgKipZb3VyIFRhc2sqKjoKSW1wbGVtZW50IHRoZSBmdW5jdGlvbiBgY2FsY3VsYXRlX2JyaWdodG5lc3MoaW1nKWAgdG86CjEuIFJldHVybiB0aGUgYXZlcmFnZSBicmlnaHRuZXNzIG9mIHRoZSBpbWFnZSByb3VuZGVkIHRvIHR3byBkZWNpbWFsIHBsYWNlcy4KMi4gSGFuZGxlIGVkZ2UgY2FzZXM6CiAgIC0gSWYgdGhlIGltYWdlIG1hdHJpeCBpcyBlbXB0eS4KICAgLSBJZiB0aGUgcm93cyBpbiB0aGUgbWF0cml4IGhhdmUgaW5jb25zaXN0ZW50IGxlbmd0aHMuCiAgIC0gSWYgYW55IHBpeGVsIHZhbHVlcyBhcmUgb3V0c2lkZSB0aGUgdmFsaWQgcmFuZ2UgKDAtMjU1KS4KCkZvciBhbnkgb2YgdGhlc2UgZWRnZSBjYXNlcywgdGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gYC0xYC4KCg==",
    "starter_code": "\n\nclass Solution:\n    def calculate_brightness(self, img):\n    \t# Write your code here\n    \tpass\n",
    "example_input": "img = [\n    [100, 200],\n    [50, 150]\n]\nprint(calculate_brightness(img))",
    "example_output": "125.0",
    "example_reasoning": "The average brightness is calculated as (100 + 200 + 50 + 150) / 4 = 125.0",
    "learn_content": "CiMgSW1hZ2UgQnJpZ2h0bmVzcyBDYWxjdWxhdG9yCgpDb25zaWRlciBhIGdyYXlzY2FsZSBpbWFnZSByZXByZXNlbnRlZCBhcyBhIDJEIG1hdHJpeCB3aGVyZSBlYWNoIGVsZW1lbnQgcmVwcmVzZW50cyBhIHBpeGVsIHZhbHVlIGJldHdlZW4gMCAoYmxhY2spIGFuZCAyNTUgKHdoaXRlKToKCiQkCkltYWdlID0gXGJlZ2lue3BtYXRyaXh9CnBfezExfSAmIHBfezEyfSBcXApwX3syMX0gJiBwX3syMn0KXGVuZHtwbWF0cml4fQokJAoKVGhlIGF2ZXJhZ2UgYnJpZ2h0bmVzcyBpcyBjYWxjdWxhdGVkIGFzOgoKJCQKQnJpZ2h0bmVzcyA9IFxmcmFje1xzdW1fe2k9MX1ee219IFxzdW1fe2o9MX1ee259IHBfe2lqfX17bSBcdGltZXMgbn0KJCQKCldoZXJlOgoKMSkgJHBfe2lqfSQgaXMgdGhlIHBpeGVsIHZhbHVlIGF0IHBvc2l0aW9uICQoaSxqKSQgIAoyKSAkbSQgaXMgdGhlIG51bWJlciBvZiByb3dzICAKMykgJG4kIGlzIHRoZSBudW1iZXIgb2YgY29sdW1ucyAgCgojIyMgVGhpbmdzIHRvIE5vdGU6CgoxKSBBbGwgcGl4ZWwgdmFsdWVzIG11c3QgYmUgYmV0d2VlbiAwIGFuZCAyNTUgIAoyKSBUaGUgaW1hZ2UgbWF0cml4IG11c3QgYmUgd2VsbC1mb3JtZWQgKGFsbCByb3dzIHNhbWUgbGVuZ3RoKSAgCjMpIEVtcHR5IG9yIGludmFsaWQgaW1hZ2VzIHJldHVybiAtMSAgCg==",
    "solution_code": "\n\nclass Solution:\n    def calculate_brightness(self, img):\n        # Check if image is empty or has no columns\n        if not img or not img[0]:\n            return -1\n\n        rows, cols = len(img), len(img[0])\n\n        # Check if all rows have same length and values are valid\n        for row in img:\n            if len(row) != cols:\n                return -1\n            for pixel in row:\n                if not 0 <= pixel <= 255:\n                    return -1\n\n        # Calculate average brightness\n        total = sum(sum(row) for row in img)\n        return round(total / (rows * cols), 2)\n",
    "test_cases": [
      {
        "test": [
          "# Test empty image\nprint(calculate_brightness([]))"
        ],
        "expected_output": -1
      },
      {
        "test": [
          "# Test invalid dimensions\nprint(calculate_brightness([[100, 200], [150]]))"
        ],
        "expected_output": -1
      }
    ],
    "function_name": "calculate_brightness"
  },
  {
    "id": 71,
    "title": "Calculate Root Mean Square Error (RMSE)",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "CiMjIFRhc2s6IENvbXB1dGUgUm9vdCBNZWFuIFNxdWFyZSBFcnJvciAoUk1TRSkKCkluIHRoaXMgdGFzaywgeW91IGFyZSByZXF1aXJlZCB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiBgcm1zZSh5X3RydWUsIHlfcHJlZClgIHRoYXQgY2FsY3VsYXRlcyB0aGUgUm9vdCBNZWFuIFNxdWFyZSBFcnJvciAoUk1TRSkgYmV0d2VlbiB0aGUgYWN0dWFsIHZhbHVlcyBhbmQgdGhlIHByZWRpY3RlZCB2YWx1ZXMuIFJNU0UgaXMgYSBjb21tb25seSB1c2VkIG1ldHJpYyBmb3IgZXZhbHVhdGluZyB0aGUgYWNjdXJhY3kgb2YgcmVncmVzc2lvbiBtb2RlbHMsIHByb3ZpZGluZyBpbnNpZ2h0IGludG8gdGhlIHN0YW5kYXJkIGRldmlhdGlvbiBvZiByZXNpZHVhbHMuCgojIyMgWW91ciBUYXNrOgpJbXBsZW1lbnQgdGhlIGZ1bmN0aW9uIGBybXNlKHlfdHJ1ZSwgeV9wcmVkKWAgdG86CjEuIENhbGN1bGF0ZSB0aGUgUk1TRSBiZXR3ZWVuIHRoZSBhcnJheXMgYHlfdHJ1ZWAgYW5kIGB5X3ByZWRgLgoyLiBSZXR1cm4gdGhlIFJNU0UgdmFsdWUgcm91bmRlZCB0byB0aHJlZSBkZWNpbWFsIHBsYWNlcy4KMy4gRW5zdXJlIHRoZSBmdW5jdGlvbiBoYW5kbGVzIGVkZ2UgY2FzZXMgc3VjaCBhczoKICAgLSBNaXNtYXRjaGVkIGFycmF5IHNoYXBlcy4KICAgLSBFbXB0eSBhcnJheXMuCiAgIC0gSW52YWxpZCBpbnB1dCB0eXBlcy4KClRoZSBSTVNFIGlzIGRlZmluZWQgYXM6CgokJApcdGV4dHtSTVNFfSA9IFxzcXJ0e1xmcmFjezF9e259IFxzdW1fe2k9MX1ee259ICh5X3tcdGV4dHt0cnVlfSwgaX0gLSB5X3tcdGV4dHtwcmVkfSwgaX0pXjJ9CiQkCgpXaGVyZToKLSAkIG4gJCBpcyB0aGUgbnVtYmVyIG9mIG9ic2VydmF0aW9ucy4KLSAkIHlfe1x0ZXh0e3RydWV9LCBpfSAkIGFuZCAkIHlfe1x0ZXh0e3ByZWR9LCBpfSAkIGFyZSB0aGUgYWN0dWFsIGFuZCBwcmVkaWN0ZWQgdmFsdWVzIGZvciB0aGUgJCBpICQtdGggb2JzZXJ2YXRpb24uCg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def rmse(self, y_true, y_pred):\n    \t# Write your code here\n    \treturn round(rmse_res,3)\n",
    "example_input": "y_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nprint(rmse(y_true, y_pred))",
    "example_output": "0.612",
    "example_reasoning": "The RMSE is calculated as sqrt((0.5^2 + 0.5^2 + 0^2 + 1^2) / 4) = 0.612",
    "learn_content": "CiMjIFJvb3QgTWVhbiBTcXVhcmUgRXJyb3IgKFJNU0UpCgpSTVNFIGlzIHVzZWQgdG8gbWVhc3VyZSB0aGUgYWNjdXJhY3kgb2YgcHJlZGljdGlvbnMgaW4gcmVncmVzc2lvbiBtb2RlbHMuIEl0IHJlcHJlc2VudHMgdGhlIGRpZmZlcmVuY2UgYmV0d2VlbiB0aGUgcHJlZGljdGlvbnMgYW5kIHRoZSBhY3R1YWwgdmFsdWVzLiBJbiBvdGhlciB3b3JkcywgaXQgaXMgdGhlIHN0YW5kYXJkIGRldmlhdGlvbiBvZiB0aGUgcmVzaWR1YWxzIG9yIHByZWRpY3Rpb24gZXJyb3JzLgoKIyMjICoqVGhlb3J5KioKVGhlIFJNU0UgaXMgZGVmaW5lZCBhczoKCiQkClx0ZXh0e1JNU0V9ID0gXHNxcnR7XGZyYWN7MX17bn0gXHN1bV97aT0xfV57bn0gKHlfe1x0ZXh0e3RydWV9X2l9IC0geV97XHRleHR7cHJlZH1faX0pXjJ9CiQkCgp3aGVyZToKLSAkIG4gJDogVGhlIG51bWJlciBvZiBvYnNlcnZhdGlvbnMuCi0gJCB5X3tcdGV4dHt0cnVlfV9pfSAkOiBUaGUgYWN0dWFsIHZhbHVlcy4KLSAkIHlfe1x0ZXh0e3ByZWR9X2l9ICQ6IFRoZSBwcmVkaWN0ZWQgdmFsdWVzLgoKIyMjICoqU3RlcHMgZm9yIENhbGN1bGF0aW9uKioKMS4gRm9yIGVhY2ggcGFpciBvZiBhY3R1YWwgYW5kIHByZWRpY3RlZCB2YWx1ZXMsIGNhbGN1bGF0ZSB0aGUgZGlmZmVyZW5jZSAkIHlfe1x0ZXh0e3RydWV9X2l9IC0geV97XHRleHR7cHJlZH1faX0gJC4KMi4gU3F1YXJlIGVhY2ggb2YgdGhlc2UgZGlmZmVyZW5jZXMgYW5kIGZpbmQgdGhlaXIgbWVhbi4KMy4gVGFrZSB0aGUgc3F1YXJlIHJvb3Qgb2YgdGhlIG1lYW4gdmFsdWUuCgojIyMgKipXaGVuIHRvIFVzZSBSTVNFIHZzLiBNQUUqKgotICoqUk1TRSoqOiBVc2VkIHdoZW4gbGFyZ2UgZGV2aWF0aW9ucy9lcnJvcnMgYXJlIG1vcmUgcHJvYmxlbWF0aWMgYW5kIHNob3VsZCBiZSBwZW5hbGl6ZWQgbW9yZSBoZWF2aWx5LgotICoqTUFFKio6IFVzZWQgd2hlbiBlcnJvcnMgc2hvdWxkIGJlIHRyZWF0ZWQgZXF1YWxseSwgcmVnYXJkbGVzcyBvZiB0aGVpciBzaXplLgo=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def rmse(self, y_true, y_pred):\n        if y_true.shape != y_pred.shape:\n            raise ValueError(\"Arrays must have the same shape\")\n        if y_true.size == 0:\n            raise ValueError(\"Arrays cannot be empty\")\n        return round(np.sqrt(np.mean((y_true - y_pred) ** 2)), 3)\n",
    "test_cases": [
      {
        "test": [
          "# Test Case 1: Normal Case  \ny_true1 = np.array([3, -0.5, 2, 7])\ny_pred1 = np.array([2.5, 0.0, 2, 8])\nprint(rmse(y_true1, y_pred1))"
        ],
        "expected_output": 0.612
      },
      {
        "test": [
          "# Test Case 2: 2D Array \ny_true2 = np.array([[0.5, 1], [-1, 1], [7, -6]])\ny_pred2 = np.array([[0, 2], [-1, 2], [8, -5]])\nprint(rmse(y_true2, y_pred2))"
        ],
        "expected_output": 0.842
      }
    ],
    "function_name": "rmse"
  },
  {
    "id": 72,
    "title": "Calculate Jaccard Index for Binary Classification",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "CiMjIFRhc2s6IEltcGxlbWVudCB0aGUgSmFjY2FyZCBJbmRleAoKWW91ciB0YXNrIGlzIHRvIGltcGxlbWVudCBhIGZ1bmN0aW9uIGBqYWNjYXJkX2luZGV4KHlfdHJ1ZSwgeV9wcmVkKWAgdGhhdCBjYWxjdWxhdGVzIHRoZSBKYWNjYXJkIEluZGV4LCBhIG1lYXN1cmUgb2Ygc2ltaWxhcml0eSBiZXR3ZWVuIHR3byBiaW5hcnkgc2V0cy4gVGhlIEphY2NhcmQgSW5kZXggaXMgd2lkZWx5IHVzZWQgaW4gYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHRhc2tzIHRvIGV2YWx1YXRlIHRoZSBvdmVybGFwIGJldHdlZW4gcHJlZGljdGVkIGFuZCB0cnVlIGxhYmVscy4KCiMjIyBZb3VyIFRhc2s6CkltcGxlbWVudCB0aGUgZnVuY3Rpb24gYGphY2NhcmRfaW5kZXgoeV90cnVlLCB5X3ByZWQpYCB0bzoKMS4gQ2FsY3VsYXRlIHRoZSBKYWNjYXJkIEluZGV4IGJldHdlZW4gdGhlIGFycmF5cyBgeV90cnVlYCBhbmQgYHlfcHJlZGAuCjIuIFJldHVybiB0aGUgSmFjY2FyZCBJbmRleCBhcyBhIGZsb2F0IHZhbHVlLgozLiBFbnN1cmUgdGhlIGZ1bmN0aW9uIGhhbmRsZXMgY2FzZXMgd2hlcmU6CiAgIC0gVGhlcmUgaXMgbm8gb3ZlcmxhcCBiZXR3ZWVuIGB5X3RydWVgIGFuZCBgeV9wcmVkYC4KICAgLSBCb3RoIGFycmF5cyBjb250YWluIG9ubHkgemVyb3MgKGVkZ2UgY2FzZXMpLgoKVGhlIEphY2NhcmQgSW5kZXggaXMgZGVmaW5lZCBhczoKCiQkClxzY3JpcHRzaXplClx0ZXh0e0phY2NhcmQgSW5kZXh9ID0gXGZyYWN7XHRleHR7TnVtYmVyIG9mIGVsZW1lbnRzIGluIHRoZSBpbnRlcnNlY3Rpb24gb2YgfSB5X3tcdGV4dHt0cnVlfX0gXHRleHR7IGFuZCB9IHlfe1x0ZXh0e3ByZWR9fX17XHRleHR7TnVtYmVyIG9mIGVsZW1lbnRzIGluIHRoZSB1bmlvbiBvZiB9IHlfe1x0ZXh0e3RydWV9fSBcdGV4dHsgYW5kIH0geV97XHRleHR7cHJlZH19fQokJAoKCldoZXJlOgotICQgeV97XHRleHR7dHJ1ZX19ICQgYW5kICQgeV97XHRleHR7cHJlZH19ICQgYXJlIGJpbmFyeSBhcnJheXMgb2YgdGhlIHNhbWUgbGVuZ3RoLCByZXByZXNlbnRpbmcgdHJ1ZSBhbmQgcHJlZGljdGVkIGxhYmVscy4KLSBUaGUgcmVzdWx0IHJhbmdlcyBmcm9tIDAgKG5vIG92ZXJsYXApIHRvIDEgKHBlcmZlY3Qgb3ZlcmxhcCkuCg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def jaccard_index(self, y_true, y_pred):\n    \t# Write your code here\n    \treturn round(result, 3)\n",
    "example_input": "y_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nprint(jaccard_index(y_true, y_pred))",
    "example_output": "0.75",
    "example_reasoning": "The Jaccard Index is calculated as 3 / 4 = 0.75, indicating a 75% overlap between the true and predicted labels.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def jaccard_index(self, y_true, y_pred):\n        intersection = np.sum((y_true == 1) & (y_pred == 1))\n        union = np.sum((y_true == 1) | (y_pred == 1))\n        result = intersection / union\n        if np.isnan(result):\n            return 0.0\n        return round(result, 3)\n",
    "test_cases": [
      {
        "test": [
          "# Test case 1: Perfect match\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 1, 0, 1])\nprint(jaccard_index(y_true, y_pred))"
        ],
        "expected_output": 1.0
      },
      {
        "test": [
          "# Test case 2: No overlap\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([0, 1, 0, 0, 1, 1])\nprint(jaccard_index(y_true, y_pred))"
        ],
        "expected_output": 0.0
      }
    ],
    "function_name": "jaccard_index"
  },
  {
    "id": 73,
    "title": "Calculate Dice Score for Classification",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "CiMjIFRhc2s6IENvbXB1dGUgdGhlIERpY2UgU2NvcmUKCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiBgZGljZV9zY29yZSh5X3RydWUsIHlfcHJlZClgIHRoYXQgY2FsY3VsYXRlcyB0aGUgRGljZSBTY29yZSwgYWxzbyBrbm93biBhcyB0aGUgU8O4cmVuc2VuLURpY2UgY29lZmZpY2llbnQgb3IgRjEtc2NvcmUsIGZvciBiaW5hcnkgY2xhc3NpZmljYXRpb24uIFRoZSBEaWNlIFNjb3JlIGlzIHVzZWQgdG8gbWVhc3VyZSB0aGUgc2ltaWxhcml0eSBiZXR3ZWVuIHR3byBzZXRzIGFuZCBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIGluIHRhc2tzIGxpa2UgaW1hZ2Ugc2VnbWVudGF0aW9uIGFuZCBiaW5hcnkgY2xhc3NpZmljYXRpb24uCgojIyMgWW91ciBUYXNrOgpJbXBsZW1lbnQgdGhlIGZ1bmN0aW9uIGBkaWNlX3Njb3JlKHlfdHJ1ZSwgeV9wcmVkKWAgdG86CjEuIENhbGN1bGF0ZSB0aGUgRGljZSBTY29yZSBiZXR3ZWVuIHRoZSBhcnJheXMgYHlfdHJ1ZWAgYW5kIGB5X3ByZWRgLgoyLiBSZXR1cm4gdGhlIERpY2UgU2NvcmUgYXMgYSBmbG9hdCB2YWx1ZSByb3VuZGVkIHRvIDMgZGVjaW1hbCBwbGFjZXMuCjMuIEhhbmRsZSBlZGdlIGNhc2VzIGFwcHJvcHJpYXRlbHksIHN1Y2ggYXMgd2hlbiB0aGVyZSBhcmUgbm8gdHJ1ZSBvciBwcmVkaWN0ZWQgcG9zaXRpdmVzLgoKVGhlIERpY2UgU2NvcmUgaXMgZGVmaW5lZCBhczoKCiQkClxzY3JpcHRzaXplClx0ZXh0e0RpY2UgU2NvcmV9ID0KXGZyYWN7MiBcdGltZXMgKFx0ZXh0e051bWJlciBvZiBlbGVtZW50cyBpbiB0aGUgaW50ZXJzZWN0aW9uIG9mIH0geV97XHRleHR7dHJ1ZX19IFx0ZXh0eyBhbmQgfSB5X3tcdGV4dHtwcmVkfX0pfXtcdGV4dHtOdW1iZXIgb2YgZWxlbWVudHMgaW4gfSB5X3tcdGV4dHt0cnVlfX0gKyBcdGV4dHtOdW1iZXIgb2YgZWxlbWVudHMgaW4gfSB5X3tcdGV4dHtwcmVkfX19CiQkCgpXaGVyZToKLSAkIHlfe1x0ZXh0e3RydWV9fSAkIGFuZCAkIHlfe1x0ZXh0e3ByZWR9fSAkIGFyZSBiaW5hcnkgYXJyYXlzIG9mIHRoZSBzYW1lIGxlbmd0aCwgcmVwcmVzZW50aW5nIHRydWUgYW5kIHByZWRpY3RlZCBsYWJlbHMuCi0gVGhlIHJlc3VsdCByYW5nZXMgZnJvbSAwIChubyBvdmVybGFwKSB0byAxIChwZXJmZWN0IG92ZXJsYXApLgo=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def dice_score(self, y_true, y_pred):\n    \t# Write your code here\n    \treturn round(res, 3)\n",
    "example_input": "y_true = np.array([1, 1, 0, 1, 0, 1])\ny_pred = np.array([1, 1, 0, 0, 0, 1])\nprint(dice_score(y_true, y_pred))",
    "example_output": "0.857",
    "example_reasoning": "The Dice Score is calculated as (2 * 3) / (2 * 3 + 0 + 1) = 0.857, indicating an 85.7% overlap between the true and predicted labels.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def dice_score(self, y_true, y_pred):\n        intersection = np.logical_and(y_true, y_pred).sum()\n        true_sum = y_true.sum()\n        pred_sum = y_pred.sum()\n\n        # Handle edge cases\n        if true_sum == 0 or pred_sum == 0:\n            return 0.0\n\n        dice = (2.0 * intersection) / (true_sum + pred_sum)\n        return round(float(dice), 3)\n",
    "test_cases": [
      {
        "test": [
          "y_true = np.array([1, 1, 0, 0])\ny_pred = np.array([1, 1, 0, 0])\nprint(dice_score(y_true, y_pred))"
        ],
        "expected_output": 1.0
      },
      {
        "test": [
          "y_true = np.array([1, 1, 0, 0])\ny_pred = np.array([0, 0, 1, 1])\nprint(dice_score(y_true, y_pred))"
        ],
        "expected_output": 0.0
      }
    ],
    "function_name": "dice_score"
  },
  {
    "id": 74,
    "title": "Create Composite Hypervector for a Dataset Row",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "IyMgVGFzazogR2VuZXJhdGUgYSBDb21wb3NpdGUgSHlwZXJ2ZWN0b3IgVXNpbmcgSHlwZXJkaW1lbnNpb25hbCBDb21wdXRpbmcKCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgdGhlIGZ1bmN0aW9uIGBjcmVhdGVfcm93X2h2KHJvdywgZGltLCByYW5kb21fc2VlZHMpYCB0byBnZW5lcmF0ZSBhIGNvbXBvc2l0ZSBoeXBlcnZlY3RvciBmb3IgYSBnaXZlbiBkYXRhc2V0IHJvdyB1c2luZyBIeXBlcmRpbWVuc2lvbmFsIENvbXB1dGluZyAoSERDKS4gRWFjaCBmZWF0dXJlIGluIHRoZSByb3cgaXMgcmVwcmVzZW50ZWQgYnkgYmluZGluZyBoeXBlcnZlY3RvcnMgZm9yIHRoZSBmZWF0dXJlIG5hbWUgYW5kIGl0cyB2YWx1ZS4gVGhlIGh5cGVydmVjdG9ycyBmb3IgdGhlIHZhbHVlcyBhcmUgY3JlYXRlZCB1c2luZyB0aGUgc2FtZSBmZWF0dXJlIHNlZWQgcHJvdmlkZWQgaW4gdGhlIGByYW5kb21fc2VlZHNgIGRpY3Rpb25hcnkgdG8gZW5zdXJlIHJlcHJvZHVjaWJpbGl0eS4gQWxsIGZlYXR1cmUgaHlwZXJ2ZWN0b3JzIGFyZSB0aGVuIGJ1bmRsZWQgdG8gY3JlYXRlIGEgY29tcG9zaXRlIGh5cGVydmVjdG9yIGZvciB0aGUgcm93LgoKIyMjIElucHV0OgotIGByb3dgOiBBIGRpY3Rpb25hcnkgcmVwcmVzZW50aW5nIGEgZGF0YXNldCByb3csIHdoZXJlIGtleXMgYXJlIGZlYXR1cmUgbmFtZXMgYW5kIHZhbHVlcyBhcmUgdGhlaXIgY29ycmVzcG9uZGluZyB2YWx1ZXMuCi0gYGRpbWA6IFRoZSBkaW1lbnNpb25hbGl0eSBvZiB0aGUgaHlwZXJ2ZWN0b3JzLgotIGByYW5kb21fc2VlZHNgOiBBIGRpY3Rpb25hcnkgd2hlcmUga2V5cyBhcmUgZmVhdHVyZSBuYW1lcyBhbmQgdmFsdWVzIGFyZSBzZWVkcyB0byBlbnN1cmUgcmVwcm9kdWNpYmlsaXR5IG9mIGh5cGVydmVjdG9ycy4KCiMjIyBPdXRwdXQ6Ci0gQSBjb21wb3NpdGUgaHlwZXJ2ZWN0b3IgcmVwcmVzZW50aW5nIHRoZSBlbnRpcmUgcm93Lgo=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def create_row_hv(self, row, dim, random_seeds):\n    \t# Write your code here\n    \tpass\n",
    "example_input": "row = {\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}\ndim = 5\nrandom_seeds = {\"FeatureA\": 42, \"FeatureB\": 7}\nprint(create_row_hv(row, dim, random_seeds))",
    "example_output": "[ 1, -1,  1,  1,  1]",
    "example_reasoning": "The composite hypervector is created by binding hypervectors for each feature and bundling them together.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def create_hv(self, dim):\n        return np.random.choice([-1, 1], dim)\n\n    def create_col_hvs(self, dim, seed):\n        np.random.seed(seed)\n        return create_hv(dim), create_hv(dim)\n\n    def bind(self, hv1, hv2):\n        return hv1 * hv2\n\n    def bundle(self, hvs, dim):\n        bundled = np.sum(list(hvs.values()), axis=0)\n        return sign(bundled)\n\n    def sign(self, vector, threshold=0.01):\n        return np.array([1 if v >= 0 else -1 for v in vector])\n\n    def create_row_hv(self, row, dim, random_seeds):\n        row_hvs = {col: bind(*create_col_hvs(dim, random_seeds[col])) for col in row.keys()}\n        return bundle(row_hvs, dim)\n",
    "test_cases": [
      {
        "test": [
          "row = {\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}\ndim = 5\nrandom_seeds = {\"FeatureA\": 42, \"FeatureB\": 7}\nprint(create_row_hv(row, dim, random_seeds))"
        ],
        "expected_output": [
          1,
          -1,
          1,
          1,
          1
        ]
      },
      {
        "test": [
          "row = {\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}\ndim = 10\nrandom_seeds = {\"FeatureA\": 42, \"FeatureB\": 7}\nprint(create_row_hv(row, dim, random_seeds))"
        ],
        "expected_output": [
          1,
          -1,
          1,
          1,
          -1,
          -1,
          -1,
          -1,
          -1,
          -1
        ]
      }
    ],
    "function_name": "create_row_hv"
  },
  {
    "id": 75,
    "title": "Generate a Confusion Matrix for Binary Classification",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "CiMjIFRhc2s6IEdlbmVyYXRlIGEgQ29uZnVzaW9uIE1hdHJpeAoKWW91ciB0YXNrIGlzIHRvIGltcGxlbWVudCB0aGUgZnVuY3Rpb24gYGNvbmZ1c2lvbl9tYXRyaXgoZGF0YSlgIHRoYXQgZ2VuZXJhdGVzIGEgY29uZnVzaW9uIG1hdHJpeCBmb3IgYSBiaW5hcnkgY2xhc3NpZmljYXRpb24gcHJvYmxlbS4gVGhlIGNvbmZ1c2lvbiBtYXRyaXggcHJvdmlkZXMgYSBzdW1tYXJ5IG9mIHRoZSBwcmVkaWN0aW9uIHJlc3VsdHMgb24gYSBjbGFzc2lmaWNhdGlvbiBwcm9ibGVtLCBhbGxvd2luZyB5b3UgdG8gdmlzdWFsaXplIGhvdyBtYW55IGRhdGEgcG9pbnRzIHdlcmUgY29ycmVjdGx5IG9yIGluY29ycmVjdGx5IGxhYmVsZWQuCgojIyMgSW5wdXQ6Ci0gQSBsaXN0IG9mIGxpc3RzLCB3aGVyZSBlYWNoIGlubmVyIGxpc3QgcmVwcmVzZW50cyBhIHBhaXIgCi0gYFt5X3RydWUsIHlfcHJlZF1gIGZvciBvbmUgb2JzZXJ2YXRpb24uIGB5X3RydWVgIGlzIHRoZSBhY3R1YWwgbGFiZWwsIGFuZCBgeV9wcmVkYCBpcyB0aGUgcHJlZGljdGVkIGxhYmVsLgoKIyMjIE91dHB1dDoKLSBBICQyIFx0aW1lcyAyJCBjb25mdXNpb24gbWF0cml4IHJlcHJlc2VudGVkIGFzIGEgbGlzdCBvZiBsaXN0cy4K",
    "starter_code": "from collections import Counter\n\nclass Solution:\n    def confusion_matrix(self, data):\n    \t# Implement the function here\n    \tpass\n",
    "example_input": "data = [[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]\nprint(confusion_matrix(data))",
    "example_output": "[[1, 1], [2, 1]]",
    "example_reasoning": "The confusion matrix shows the counts of true positives, false negatives, false positives, and true negatives.",
    "learn_content": "CiMjIEdlbmVyYXRlIENvbmZ1c2lvbiBNYXRyaXgKClRoZSBjb25mdXNpb24gbWF0cml4IGlzIGEgdmVyeSB1c2VmdWwgdG9vbCB0byBnZXQgYSBiZXR0ZXIgdW5kZXJzdGFuZGluZyBvZiB0aGUgcGVyZm9ybWFuY2Ugb2YgYSBjbGFzc2lmaWNhdGlvbiBtb2RlbC4gSW4gaXQsIHlvdSBjYW4gdmlzdWFsaXplIGhvdyBtYW55IGRhdGEgcG9pbnRzIHdlcmUgbGFiZWxlZCBhY2NvcmRpbmcgdG8gdGhlaXIgY29ycmVjdCBjYXRlZ29yaWVzLgoKRm9yIGEgYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHByb2JsZW0gb2YgYSBkYXRhc2V0IHdpdGggJCBuICQgb2JzZXJ2YXRpb25zLCB0aGUgY29uZnVzaW9uIG1hdHJpeCBpcyBhICQgMiBcdGltZXMgMiAkIG1hdHJpeCB3aXRoIHRoZSBmb2xsb3dpbmcgc3RydWN0dXJlOgoKJCQKTSA9IFxiZWdpbntwbWF0cml4fSAKVFAgJiBGTiBcXApGUCAmIFROClxlbmR7cG1hdHJpeH0KJCQKCldoZXJlOgoKLSAqKlRQKio6IFRydWUgcG9zaXRpdmVzLCB0aGUgbnVtYmVyIG9mIG9ic2VydmF0aW9ucyBmcm9tIHRoZSBwb3NpdGl2ZSBsYWJlbCB0aGF0IHdlcmUgY29ycmVjdGx5IGxhYmVsZWQgYXMgcG9zaXRpdmUuCi0gKipGTioqOiBGYWxzZSBuZWdhdGl2ZXMsIHRoZSBudW1iZXIgb2Ygb2JzZXJ2YXRpb25zIGZyb20gdGhlIHBvc2l0aXZlIGxhYmVsIHRoYXQgd2VyZSBpbmNvcnJlY3RseSBsYWJlbGVkIGFzIG5lZ2F0aXZlLgotICoqRlAqKjogRmFsc2UgcG9zaXRpdmVzLCB0aGUgbnVtYmVyIG9mIG9ic2VydmF0aW9ucyBmcm9tIHRoZSBuZWdhdGl2ZSBsYWJlbCB0aGF0IHdlcmUgaW5jb3JyZWN0bHkgbGFiZWxlZCBhcyBwb3NpdGl2ZS4KLSAqKlROKio6IFRydWUgbmVnYXRpdmVzLCB0aGUgbnVtYmVyIG9mIG9ic2VydmF0aW9ucyBmcm9tIHRoZSBuZWdhdGl2ZSBsYWJlbCB0aGF0IHdlcmUgY29ycmVjdGx5IGxhYmVsZWQgYXMgbmVnYXRpdmUuCgpBIGNvbmZ1c2lvbiBtYXRyaXggaXMgYSBncmVhdCBzdGFydGluZyBwb2ludCBmb3IgY29tcHV0aW5nIG1vcmUgYWR2YW5jZWQgbWV0cmljcyBzdWNoIGFzIHByZWNpc2lvbiBhbmQgcmVjYWxsIHRoYXQgY2FwdHVyZSB0aGUgbW9kZWwncyBwZXJmb3JtYW5jZS4K",
    "solution_code": "from collections import Counter\n\nclass Solution:\n    def confusion_matrix(self, data):\n        # Count all occurrences\n        counts = Counter(tuple(pair) for pair in data)\n        # Get metrics\n        TP, FN, FP, TN = counts[(1, 1)], counts[(1, 0)], counts[(0, 1)], counts[(0, 0)]\n        # Define matrix and return\n        confusion_matrix = [[TP, FN], [FP, TN]]\n        return confusion_matrix\n",
    "test_cases": [
      {
        "test": [
          "data = [[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]\nprint(confusion_matrix(data))"
        ],
        "expected_output": [
          [
            1,
            1
          ],
          [
            2,
            1
          ]
        ]
      },
      {
        "test": [
          "data = [[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]\nprint(confusion_matrix(data))"
        ],
        "expected_output": [
          [
            5,
            5
          ],
          [
            4,
            3
          ]
        ]
      }
    ],
    "function_name": "confusion_matrix"
  },
  {
    "id": 76,
    "title": "Calculate Cosine Similarity Between Vectors",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "CiMjIFRhc2s6IEltcGxlbWVudCBDb3NpbmUgU2ltaWxhcml0eQoKSW4gdGhpcyB0YXNrLCB5b3UgbmVlZCB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiBgY29zaW5lX3NpbWlsYXJpdHkodjEsIHYyKWAgdGhhdCBjYWxjdWxhdGVzIHRoZSBjb3NpbmUgc2ltaWxhcml0eSBiZXR3ZWVuIHR3byB2ZWN0b3JzLiBDb3NpbmUgc2ltaWxhcml0eSBtZWFzdXJlcyB0aGUgY29zaW5lIG9mIHRoZSBhbmdsZSBiZXR3ZWVuIHR3byB2ZWN0b3JzLCBpbmRpY2F0aW5nIHRoZWlyIGRpcmVjdGlvbmFsIHNpbWlsYXJpdHkuCgojIyMgSW5wdXQ6Ci0gYHYxYCBhbmQgYHYyYDogTnVtcHkgYXJyYXlzIHJlcHJlc2VudGluZyB0aGUgaW5wdXQgdmVjdG9ycy4KCiMjIyBPdXRwdXQ6Ci0gQSBmbG9hdCByZXByZXNlbnRpbmcgdGhlIGNvc2luZSBzaW1pbGFyaXR5LCByb3VuZGVkIHRvIHRocmVlIGRlY2ltYWwgcGxhY2VzLgoKIyMjIENvbnN0cmFpbnRzOgotIEJvdGggaW5wdXQgdmVjdG9ycyBtdXN0IGhhdmUgdGhlIHNhbWUgc2hhcGUuCi0gSW5wdXQgdmVjdG9ycyBjYW5ub3QgYmUgZW1wdHkgb3IgaGF2ZSB6ZXJvIG1hZ25pdHVkZS4K",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def cosine_similarity(self, v1, v2):\n    \t# Implement your code here\n    \tpass\n",
    "example_input": "import numpy as np\n\nv1 = np.array([1, 2, 3])\nv2 = np.array([2, 4, 6])\nprint(cosine_similarity(v1, v2))",
    "example_output": "1.0",
    "example_reasoning": "The cosine similarity between v1 and v2 is 1.0, indicating perfect similarity.",
    "learn_content": "CiMjIENvc2luZSBTaW1pbGFyaXR5CgpDb3NpbmUgc2ltaWxhcml0eSBtZWFzdXJlcyB0aGUgY29zaW5lIG9mIHRoZSBhbmdsZSBiZXR3ZWVuIHR3byB2ZWN0b3JzLiBJdCBkb2Vzbid0IGNvbnNpZGVyIHRoZSBtYWduaXR1ZGUgb2YgdGhlIHZlY3RvcnMgYnV0IGZvY3VzZXMgb24gdGhlIGFuZ2xlIGJldHdlZW4gdGhlbS4KCiMjIyBDb3NpbmUgU2ltaWxhcml0eSBGb3JtdWxhCiQkClxjb3MoXHRoZXRhKSA9IFxmcmFje1xzdW1fe2k9MX1ee3B9IEFfaSBCX2l9e1xzcXJ0e1xzdW1fe2k9MX1ee3B9IEFfaV4yfSBcc3FydHtcc3VtX3tpPTF9XntwfSBCX2leMn19CiQkCgojIyMgSW1wbGVtZW50YXRpb24gU3RlcHMgZm9yIENvc2luZSBTaW1pbGFyaXR5CjEuICoqSGFuZGxlIElucHV0Kio6IEVuc3VyZSBpbnB1dCB2ZWN0b3JzIGhhdmUgdGhlIHNhbWUgZGltZW5zaW9ucyBhbmQgaGFuZGxlIGVkZ2UgY2FzZXMgKGUuZy4sIHplcm8gdmVjdG9ycykuCjIuICoqRG90IFByb2R1Y3QqKjogQ29tcHV0ZSAkXHN1bV97aT0xfV57cH0gQV9pIEJfaSAkIGZvciB0aGUgdHdvIHZlY3RvcnMuCjMuICoqTWFnbml0dWRlcyoqOiBDb21wdXRlIHRoZSBMMiBub3JtcyAkIFxzcXJ0e1xzdW1fe2k9MX1ee3B9IEFfaV4yfSAkIGFuZCAkIFxzcXJ0e1xzdW1fe2k9MX1ee3B9IEJfaV4yfSAkLgo0LiAqKkZpbmFsIFJlc3VsdCoqOiBEaXZpZGUgdGhlIGRvdCBwcm9kdWN0IGJ5IHRoZSBwcm9kdWN0IG9mIHRoZSBtYWduaXR1ZGVzLgoKIyMjIFVzZSBDYXNlcwoxLiAqKlRleHQgYW5kIEltYWdlIFNpbWlsYXJpdHkqKgoyLiAqKlJlY29tbWVuZGF0aW9uIFN5c3RlbXMqKgozLiAqKlF1ZXJ5IE1hdGNoaW5nKioKCiMjIyBQaXRmYWxscwoxLiAqKk1hZ25pdHVkZSBCbGluZG5lc3MqKjoKICAgLSBFeGFtcGxlOgogICAgIC0gJCBcdGV4dHt2ZWN0b3IxfSA9ICgxLCAxKSAkCiAgICAgLSAkIFx0ZXh0e3ZlY3RvcjJ9ID0gKDEwMDAsIDEwMDApICQKICAgICAtIENvc2luZSBzaW1pbGFyaXR5ICQgPSAxICQsIGRlc3BpdGUgdGhlIHZhc3RseSBkaWZmZXJlbnQgbWFnbml0dWRlcy4KMi4gKipTcGFyc2UgRGF0YSBJc3N1ZXMqKjoKICAgLSBJbiBoaWdoLWRpbWVuc2lvbmFsIHNwYWNlcywgd2hlcmUgZGF0YSBpcyBvZnRlbiBzcGFyc2UsIGNvc2luZSBzaW1pbGFyaXR5IG1heSBiZWNvbWUgbGVzcyByZWxpYWJsZS4KMy4gKipOb24tTmVnYXRpdmUgRGF0YSBMaW1pdGF0aW9uKio6CiAgIC0gSWYgYWxsIHZhbHVlcyBhcmUgcG9zaXRpdmUsIGNvc2luZSBzaW1pbGFyaXR5IGNhbm5vdCBjYXB0dXJlIG5lZ2F0aXZlIHJlbGF0aW9uc2hpcHMgb3IgaW52ZXJzZSB0cmVuZHMuCgo=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def cosine_similarity(self, v1, v2):\n        if v1.shape != v2.shape:\n            raise ValueError(\"Arrays must have the same shape\")\n\n        if v1.size == 0:\n            raise ValueError(\"Arrays cannot be empty\")\n\n        # Flatten arrays in case of 2D\n        v1_flat = v1.flatten()\n        v2_flat = v2.flatten()\n\n        dot_product = np.dot(v1_flat, v2_flat)\n        magnitude1 = np.sqrt(np.sum(v1_flat**2))\n        magnitude2 = np.sqrt(np.sum(v2_flat**2))\n\n        if magnitude1 == 0 or magnitude2 == 0:\n            raise ValueError(\"Vectors cannot have zero magnitude\")\n\n        return round(dot_product / (magnitude1 * magnitude2), 3)\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\n\nv1 = np.array([1, 2, 3])\nv2 = np.array([2, 4, 6])\nprint(cosine_similarity(v1, v2))"
        ],
        "expected_output": 1.0
      },
      {
        "test": [
          "import numpy as np\n\nv1 = np.array([1, 2, 3])\nv2 = np.array([-1, -2, -3])\nprint(cosine_similarity(v1, v2))"
        ],
        "expected_output": -1.0
      }
    ],
    "function_name": "cosine_similarity"
  },
  {
    "id": 77,
    "title": "Calculate Performance Metrics for a Classification Model",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "CiMjIyBUYXNrOiBJbXBsZW1lbnQgUGVyZm9ybWFuY2UgTWV0cmljcyBDYWxjdWxhdGlvbgoKSW4gdGhpcyB0YXNrLCB5b3UgYXJlIHJlcXVpcmVkIHRvIGltcGxlbWVudCBhIGZ1bmN0aW9uIGBwZXJmb3JtYW5jZV9tZXRyaWNzKGFjdHVhbCwgcHJlZGljdGVkKWAgdGhhdCBjb21wdXRlcyB2YXJpb3VzIHBlcmZvcm1hbmNlIG1ldHJpY3MgZm9yIGEgYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHByb2JsZW0uIFRoZXNlIG1ldHJpY3MgaW5jbHVkZToKCi0gQ29uZnVzaW9uIE1hdHJpeAotIEFjY3VyYWN5Ci0gRjEgU2NvcmUKLSBTcGVjaWZpY2l0eQotIE5lZ2F0aXZlIFByZWRpY3RpdmUgVmFsdWUKClRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBpbiB0d28gbGlzdHM6CgotIGBhY3R1YWxgOiBUaGUgYWN0dWFsIGNsYXNzIGxhYmVscyAoMSBmb3IgcG9zaXRpdmUsIDAgZm9yIG5lZ2F0aXZlKS4KLSBgcHJlZGljdGVkYDogVGhlIHByZWRpY3RlZCBjbGFzcyBsYWJlbHMgZnJvbSB0aGUgbW9kZWwuCgojIyMgT3V0cHV0CgpUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiBhIHR1cGxlIGNvbnRhaW5pbmc6CgoxLiBgY29uZnVzaW9uX21hdHJpeGA6IEEgMngyIG1hdHJpeC4KMi4gYGFjY3VyYWN5YDogQSBmbG9hdCByZXByZXNlbnRpbmcgdGhlIGFjY3VyYWN5IG9mIHRoZSBtb2RlbC4KMy4gYGYxX3Njb3JlYDogQSBmbG9hdCByZXByZXNlbnRpbmcgdGhlIEYxIHNjb3JlIG9mIHRoZSBtb2RlbC4KNC4gYHNwZWNpZmljaXR5YDogQSBmbG9hdCByZXByZXNlbnRpbmcgdGhlIHNwZWNpZmljaXR5IG9mIHRoZSBtb2RlbC4KNS4gYG5lZ2F0aXZlX3ByZWRpY3RpdmVfdmFsdWVgOiBBIGZsb2F0IHJlcHJlc2VudGluZyB0aGUgbmVnYXRpdmUgcHJlZGljdGl2ZSB2YWx1ZS4KCiMjIyBDb25zdHJhaW50cwoKLSBBbGwgZWxlbWVudHMgaW4gdGhlIGBhY3R1YWxgIGFuZCBgcHJlZGljdGVkYCBsaXN0cyBtdXN0IGJlIGVpdGhlciAwIG9yIDEuCi0gQm90aCBsaXN0cyBtdXN0IGhhdmUgdGhlIHNhbWUgbGVuZ3RoLgo=",
    "starter_code": "\n\nclass Solution:\n    def performance_metrics(self, actual: list[int], predicted: list[int]) -> tuple:\n    \t# Implement your code here\n    \treturn confusion_matrix, round(accuracy, 3), round(f1, 3), round(specificity, 3), round(negativePredictive, 3)\n",
    "example_input": "actual = [1, 0, 1, 0, 1]\npredicted = [1, 0, 0, 1, 1]\nprint(performance_metrics(actual, predicted))",
    "example_output": "([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)",
    "example_reasoning": "The function calculates the confusion matrix, accuracy, F1 score, specificity, and negative predictive value based on the input labels. The resulting values are rounded to three decimal places as required.",
    "learn_content": "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",
    "solution_code": "from collections import Counter\n\nclass Solution:\n    def performance_metrics(self, actual: list[int], predicted: list[int]) -> tuple:\n        data = list(zip(actual, predicted))\n        counts = Counter(tuple(pair) for pair in data)\n        TP, FN, FP, TN = counts[(1, 1)], counts[(1, 0)], counts[(0, 1)], counts[(0, 0)]\n        confusion_matrix = [[TP, FN], [FP, TN]]\n        accuracy = (TP + TN) / (TP + TN + FP + FN)\n        precision = TP / (TP + FP)\n        recall = TP / (TP + FN)\n        f1 = 2 * precision * recall / (precision + recall)\n        negativePredictive = TN / (TN + FN)\n        specificity = TN / (TN + FP)\n        return confusion_matrix, round(accuracy, 3), round(f1, 3), round(specificity, 3), round(negativePredictive, 3)\n",
    "test_cases": [
      {
        "test": [
          "actual = [1, 0, 1, 0, 1]\npredicted = [1, 0, 0, 1, 1]\nprint(performance_metrics(actual, predicted))"
        ],
        "expected_output": "([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)"
      },
      {
        "test": [
          "actual = [1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1]\npredicted = [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]\nprint(performance_metrics(actual, predicted))"
        ],
        "expected_output": "([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)"
      }
    ],
    "function_name": "performance_metrics"
  },
  {
    "id": 78,
    "title": "Descriptive Statistics Calculator",
    "difficulty": "easy",
    "category": "statistics",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHZhcmlvdXMgZGVzY3JpcHRpdmUgc3RhdGlzdGljcyBtZXRyaWNzIGZvciBhIGdpdmVuIGRhdGFzZXQuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBhIGxpc3Qgb3IgTnVtUHkgYXJyYXkgb2YgbnVtZXJpY2FsIHZhbHVlcyBhbmQgcmV0dXJuIGEgZGljdGlvbmFyeSBjb250YWluaW5nIG1lYW4sIG1lZGlhbiwgbW9kZSwgdmFyaWFuY2UsIHN0YW5kYXJkIGRldmlhdGlvbiwgcGVyY2VudGlsZXMgKDI1dGgsIDUwdGgsIDc1dGgpLCBhbmQgaW50ZXJxdWFydGlsZSByYW5nZSAoSVFSKS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def descriptive_statistics(self, data):\n    \t# Your code here\n    \tstats_dict = {\n            \"mean\": mean,\n            \"median\": median,\n            \"mode\": mode,\n            \"variance\": np.round(variance,4),\n            \"standard_deviation\": np.round(std_dev,4),\n            \"25th_percentile\": percentiles[0],\n            \"50th_percentile\": percentiles[1],\n            \"75th_percentile\": percentiles[2],\n            \"interquartile_range\": iqr\n        }\n    \treturn {}",
    "example_input": "[10, 20, 30, 40, 50]",
    "example_output": "{'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.142135623730951, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}",
    "example_reasoning": "The dataset is processed to calculate all descriptive statistics. The mean is the average value, the median is the central value, the mode is the most frequent value, and variance and standard deviation measure the spread of data. Percentiles and IQR describe data distribution.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def descriptive_statistics(self, data):\n        \"\"\"\n        Calculate various descriptive statistics metrics for a given dataset.\n        :param data: List or numpy array of numerical values\n        :return: Dictionary containing mean, median, mode, variance, standard deviation,\n                 percentiles (25th, 50th, 75th), and interquartile range (IQR)\n        \"\"\"\n        # Ensure data is a numpy array for easier calculations\n        data = np.array(data)\n\n        # Mean\n        mean = np.mean(data)\n\n        # Median\n        median = np.median(data)\n\n        # Mode\n        unique, counts = np.unique(data, return_counts=True)\n        mode = unique[np.argmax(counts)] if len(data) > 0 else None\n\n        # Variance\n        variance = np.var(data)\n\n        # Standard Deviation\n        std_dev = np.sqrt(variance)\n\n        # Percentiles (25th, 50th, 75th)\n        percentiles = np.percentile(data, [25, 50, 75])\n\n        # Interquartile Range (IQR)\n        iqr = percentiles[2] - percentiles[0]\n\n        # Compile results into a dictionary\n        stats_dict = {\n            \"mean\": mean,\n            \"median\": median,\n            \"mode\": mode,\n            \"variance\": np.round(variance,4),\n            \"standard_deviation\": np.round(std_dev,4),\n            \"25th_percentile\": percentiles[0],\n            \"50th_percentile\": percentiles[1],\n            \"75th_percentile\": percentiles[2],\n            \"interquartile_range\": iqr\n        }\n\n        return stats_dict",
    "test_cases": [
      {
        "test": [
          [
            10,
            20,
            30,
            40,
            50
          ]
        ],
        "expected_output": {
          "mean": 30.0,
          "median": 30.0,
          "mode": 10,
          "variance": 200.0,
          "standard_deviation": 14.1421,
          "25th_percentile": 20.0,
          "50th_percentile": 30.0,
          "75th_percentile": 40.0,
          "interquartile_range": 20.0
        }
      },
      {
        "test": [
          [
            1,
            2,
            2,
            3,
            4,
            4,
            4,
            5
          ]
        ],
        "expected_output": {
          "mean": 3.125,
          "median": 3.5,
          "mode": 4,
          "variance": 1.6094,
          "standard_deviation": 1.2686,
          "25th_percentile": 2.0,
          "50th_percentile": 3.5,
          "75th_percentile": 4.0,
          "interquartile_range": 2.0
        }
      }
    ],
    "function_name": "descriptive_statistics"
  },
  {
    "id": 79,
    "title": "Binomial Distribution Probability",
    "difficulty": "medium",
    "category": "probability",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBwcm9iYWJpbGl0eSBvZiBhY2hpZXZpbmcgZXhhY3RseSBrIHN1Y2Nlc3NlcyBpbiBuIGluZGVwZW5kZW50IEJlcm5vdWxsaSB0cmlhbHMsIGVhY2ggd2l0aCBwcm9iYWJpbGl0eSBwIG9mIHN1Y2Nlc3MsIHVzaW5nIHRoZSBCaW5vbWlhbCBkaXN0cmlidXRpb24gZm9ybXVsYS4=",
    "starter_code": "import math\n\nclass Solution:\n    def binomial_probability(self, n, k, p):\n    \t\"\"\"\n        Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n        each with probability p of success, using the Binomial distribution formula.\n        \"\"\"\n    \t# Your code here\n    \treturn round(probability, 5)",
    "example_input": "n = 6, k = 2, p = 0.5",
    "example_output": "0.23438",
    "example_reasoning": "The function calculates the Binomial probability, the intermediate steps include calculating the binomial coefficient, raising p and (1-p) to the appropriate powers, and multiplying the results.",
    "learn_content": "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",
    "solution_code": "import math\n\nclass Solution:\n    def binomial_probability(self, n, k, p):\n        \"\"\"\n        Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n        each with probability p of success, using the Binomial distribution formula.\n        :param n: Total number of trials\n        :param k: Number of successes\n        :param p: Probability of success on each trial\n        :return: Probability of k successes in n trials\n        \"\"\"\n        # Calculate binomial coefficient (n choose k)\n        binomial_coeff = math.comb(n, k)\n        # Calculate the probability using the binomial formula\n        probability = binomial_coeff * (p ** k) * ((1 - p) ** (n - k))\n        # Return the probability, rounded to five decimal places\n        return round(probability, 5)",
    "test_cases": [
      {
        "test": [
          6,
          2,
          0.5
        ],
        "expected_output": 0.23438
      },
      {
        "test": [
          6,
          4,
          0.7
        ],
        "expected_output": 0.32414
      }
    ],
    "function_name": "binomial_probability"
  },
  {
    "id": 80,
    "title": "Normal Distribution PDF Calculator",
    "difficulty": "medium",
    "category": "probability",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBwcm9iYWJpbGl0eSBkZW5zaXR5IGZ1bmN0aW9uIChQREYpIG9mIHRoZSBub3JtYWwgZGlzdHJpYnV0aW9uIGZvciBhIGdpdmVuIHZhbHVlLCBtZWFuLCBhbmQgc3RhbmRhcmQgZGV2aWF0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHVzZSB0aGUgbWF0aGVtYXRpY2FsIGZvcm11bGEgb2YgdGhlIG5vcm1hbCBkaXN0cmlidXRpb24gdG8gcmV0dXJuIHRoZSBQREYgdmFsdWUgcm91bmRlZCB0byA1IGRlY2ltYWwgcGxhY2VzLg==",
    "starter_code": "import math\n\nclass Solution:\n    def normal_pdf(self, x, mean, std_dev):\n    \t\"\"\"\n    \tCalculate the probability density function (PDF) of the normal distribution.\n    \t:param x: The value at which the PDF is evaluated.\n    \t:param mean: The mean (\u03bc) of the distribution.\n    \t:param std_dev: The standard deviation (\u03c3) of the distribution.\n    \t\"\"\"\n    \t# Your code here\n    \tpass\n    \treturn round(val,5)",
    "example_input": "x = 16, mean = 15, std_dev = 2.04",
    "example_output": "0.17342",
    "example_reasoning": "The function computes the PDF using x = 16, mean = 15, and std_dev = 2.04.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyBOb3JtYWwgRGlzdHJpYnV0aW9uCgpUaGUgTm9ybWFsIERpc3RyaWJ1dGlvbiwgYWxzbyBrbm93biBhcyB0aGUgR2F1c3NpYW4gRGlzdHJpYnV0aW9uLCBpcyBhIGNvbnRpbnVvdXMgcHJvYmFiaWxpdHkgZGlzdHJpYnV0aW9uIHRoYXQgaXMgc3ltbWV0cmljYWwgYW5kIGJlbGwtc2hhcGVkLCByZXByZXNlbnRpbmcgdGhlIGRpc3RyaWJ1dGlvbiBvZiBkYXRhIGFyb3VuZCB0aGUgbWVhbi4KCiMjIyBLZXkgQ2hhcmFjdGVyaXN0aWNzCgotICoqU3ltbWV0cnkqKjogVGhlIGRpc3RyaWJ1dGlvbiBpcyBzeW1tZXRyaWMgYXJvdW5kIHRoZSBtZWFuLCB3aGljaCBtZWFucyB0aGUgbGVmdCBhbmQgcmlnaHQgaGFsdmVzIG9mIHRoZSBncmFwaCBhcmUgbWlycm9yIGltYWdlcy4KLSAqKk1lYW4sIE1lZGlhbiwgYW5kIE1vZGUqKjogSW4gYSBwZXJmZWN0bHkgbm9ybWFsIGRpc3RyaWJ1dGlvbiwgdGhlIG1lYW4sIG1lZGlhbiwgYW5kIG1vZGUgYXJlIGFsbCBlcXVhbC4KLSAqKlNoYXBlKio6IFRoZSBiZWxsLXNoYXBlZCBjdXJ2ZSBpcyBkZWZpbmVkIGJ5IGl0cyBtZWFuICgkXG11JCkgYW5kIHN0YW5kYXJkIGRldmlhdGlvbiAoJFxzaWdtYSQpLgotICoqRW1waXJpY2FsIFJ1bGUqKjogQXBwcm94aW1hdGVseToKICAtICQ2OFwlJCBvZiBkYXRhIGZhbGxzIHdpdGhpbiAxIHN0YW5kYXJkIGRldmlhdGlvbiAoJFxtdSBccG0gXHNpZ21hJCkuCiAgLSAkOTVcJSQgb2YgZGF0YSBmYWxscyB3aXRoaW4gMiBzdGFuZGFyZCBkZXZpYXRpb25zICgkXG11IFxwbSAyXHNpZ21hJCkuCiAgLSAkOTkuN1wlJCBvZiBkYXRhIGZhbGxzIHdpdGhpbiAzIHN0YW5kYXJkIGRldmlhdGlvbnMgKCRcbXUgXHBtIDNcc2lnbWEkKS4KCiMjIyBNYXRoZW1hdGljYWwgRm9ybXVsYQoKVGhlIHByb2JhYmlsaXR5IGRlbnNpdHkgZnVuY3Rpb24gKFBERikgb2YgYSBub3JtYWwgZGlzdHJpYnV0aW9uIGlzIGdpdmVuIGJ5OgoKJCQKZih4KSA9IFxmcmFjezF9e1xzcXJ0ezJccGlcc2lnbWFeMn19IGVeey1cZnJhY3soeC1cbXUpXjJ9ezJcc2lnbWFeMn19CiQkCgotICoqJHgkKio6IFJhbmRvbSB2YXJpYWJsZQotICoqJFxtdSQqKjogTWVhbiBvZiB0aGUgZGlzdHJpYnV0aW9uCi0gKiokXHNpZ21hJCoqOiBTdGFuZGFyZCBkZXZpYXRpb24gb2YgdGhlIGRpc3RyaWJ1dGlvbgoKIyMjIEltcGxlbWVudGF0aW9uIFN0ZXBzCgoxLiAqKkNhbGN1bGF0ZSB0aGUgbWVhbiAoJFxtdSQpIGFuZCBzdGFuZGFyZCBkZXZpYXRpb24gKCRcc2lnbWEkKToqKgogICAtICRcbXUgPSBcZnJhY3tcc3VtIHhfaX17Tn0kCiAgIC0gJFxzaWdtYSA9IFxzcXJ0e1xmcmFje1xzdW0gKHhfaSAtIFxtdSleMn17Tn19JAoKMi4gKipVc2UgdGhlIG5vcm1hbCBkaXN0cmlidXRpb24gZm9ybXVsYSB0byBjYWxjdWxhdGUgdGhlIHByb2JhYmlsaXR5IGRlbnNpdHkgZm9yIGVhY2ggdmFsdWU6KioKICAgLSAkXGZyYWN7MX17XHNxcnR7MlxwaVxzaWdtYV4yfX0gZV57LVxmcmFjeyh4LVxtdSleMn17MlxzaWdtYV4yfX0kCgozLiAqKlZpc3VhbGl6ZSB0aGUgY3VydmUqKjoKICAgLSBQbG90IHRoZSBjYWxjdWxhdGVkIFBERiB2YWx1ZXMgZm9yIGEgcmFuZ2Ugb2YgJHgkIHRvIHZpc3VhbGl6ZSB0aGUgYmVsbC1zaGFwZWQgY3VydmUuCgojIyMgRXhhbXBsZSBDYWxjdWxhdGlvbgoKR2l2ZW46CgotIERhdGE6IFsxMCwgMTIsIDE0LCAxNiwgMTgsIDIwXQoKMS4gKipNZWFuICgkXG11JCk6KioKICAgJCQKICAgXG11ID0gXGZyYWN7MTAgKyAxMiArIDE0ICsgMTYgKyAxOCArIDIwfXs2fSA9IDE1CiAgICQkCgoyLiAqKlN0YW5kYXJkIERldmlhdGlvbiAoJFxzaWdtYSQpOioqCiAgICQkCiAgIFxzaWdtYSA9IFxzcXJ0e1xmcmFjeygxMC0xNSleMiArIFxkb3RzICsgKDIwLTE1KV4yfXs2fX0gPSBcc3FydHtcZnJhY3syNX17Nn19IFxhcHByb3ggMi4wNAogICAkJAoKMy4gKipQREYgZm9yICR4ID0gMTYkOioqCiAgICQkCiAgIGYoMTYpID0gXGZyYWN7MX17XHNxcnR7MlxwaSgyLjA0KV4yfX0gZV57LVxmcmFjeygxNi0xNSleMn17MigyLjA0KV4yfX0gXGFwcHJveCAwLjE3NgogICAkJAoKIyMjIEFwcGxpY2F0aW9ucwoKVGhlIE5vcm1hbCBEaXN0cmlidXRpb24gaXMgd2lkZWx5IHVzZWQgaW46CgotIERhdGEgQW5hbHlzaXMKLSBTdGF0aXN0aWNhbCBJbmZlcmVuY2UKLSBNYWNoaW5lIExlYXJuaW5nIEFsZ29yaXRobXMKLSBRdWFsaXR5IENvbnRyb2wKLSBSaXNrIE1hbmFnZW1lbnQKClRoaXMgZGlzdHJpYnV0aW9uIGlzIGNydWNpYWwgaW4gZmllbGRzIGxpa2UgZWNvbm9taWNzLCBiaW9sb2d5LCBwc3ljaG9sb2d5LCBhbmQgZW5naW5lZXJpbmcgdG8gbW9kZWwgbmF0dXJhbCBwaGVub21lbmEgYW5kIG1ha2UgcHJlZGljdGlvbnMuCg==",
    "solution_code": "import math\n\nclass Solution:\n    def normal_pdf(self, x, mean, std_dev):\n        \"\"\"\n        Calculate the probability density function (PDF) of the normal distribution.\n        :param x: The value at which the PDF is evaluated.\n        :param mean: The mean (\u03bc) of the distribution.\n        :param std_dev: The standard deviation (\u03c3) of the distribution.\n        :return: The PDF value for the given x.\n        \"\"\"\n        coefficient = 1 / (math.sqrt(2 * math.pi) * std_dev)\n        exponent = math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n        return round(coefficient * exponent, 5)",
    "test_cases": [
      {
        "test": [
          0,
          0,
          1
        ],
        "expected_output": 0.39894
      },
      {
        "test": [
          16,
          15,
          2.04
        ],
        "expected_output": 0.17342
      }
    ],
    "function_name": "normal_pdf"
  },
  {
    "id": 81,
    "title": "Poisson Distribution Probability Calculator",
    "difficulty": "easy",
    "category": "probability",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBwcm9iYWJpbGl0eSBvZiBvYnNlcnZpbmcgZXhhY3RseSBrIGV2ZW50cyBpbiBhIGZpeGVkIGludGVydmFsIHVzaW5nIHRoZSBQb2lzc29uIGRpc3RyaWJ1dGlvbiBmb3JtdWxhLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgayAobnVtYmVyIG9mIGV2ZW50cykgYW5kIGxhbSAobWVhbiByYXRlIG9mIG9jY3VycmVuY2VzKSBhcyBpbnB1dHMgYW5kIHJldHVybiB0aGUgcHJvYmFiaWxpdHkgcm91bmRlZCB0byA1IGRlY2ltYWwgcGxhY2VzLg==",
    "starter_code": "import math\n\nclass Solution:\n    def poisson_probability(self, k, lam):\n    \t\"\"\"\n    \tCalculate the probability of observing exactly k events in a fixed interval,\n    \tgiven the mean rate of events lam, using the Poisson distribution formula.\n    \t:param k: Number of events (non-negative integer)\n    \t:param lam: The average rate (mean) of occurrences in a fixed interval\n    \t\"\"\"\n    \t# Your code here\n    \tpass\n    \treturn round(val,5)",
    "example_input": "k = 3, lam = 5",
    "example_output": "0.14037",
    "example_reasoning": "The function calculates the probability for a given number of events occurring in a fixed interval, based on the mean rate of occurrences.",
    "learn_content": "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",
    "solution_code": "import math\n\nclass Solution:\n    def poisson_probability(self, k, lam):\n        \"\"\"\n        Calculate the probability of observing exactly k events in a fixed interval,\n        given the mean rate of events lam, using the Poisson distribution formula.\n        :param k: Number of events (non-negative integer)\n        :param lam: The average rate (mean) of occurrences in a fixed interval\n        :return: Probability of k events occurring\n        \"\"\"\n        # Calculate the Poisson probability using the formula\n        probability = (lam ** k) * math.exp(-lam) / math.factorial(k)\n        # Return the probability, rounded to five decimal places\n        return round(probability, 5)",
    "test_cases": [
      {
        "test": [
          3,
          5
        ],
        "expected_output": 0.14037
      },
      {
        "test": [
          0,
          5
        ],
        "expected_output": 0.00674
      }
    ],
    "function_name": "poisson_probability"
  },
  {
    "id": 82,
    "title": "Grayscale Image Contrast Calculator",
    "difficulty": "easy",
    "category": "computer vision",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBjb250cmFzdCBvZiBhIGdyYXlzY2FsZSBpbWFnZSB1c2luZyB0aGUgZGlmZmVyZW5jZSBiZXR3ZWVuIHRoZSBtYXhpbXVtIGFuZCBtaW5pbXVtIHBpeGVsIHZhbHVlcy4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def calculate_contrast(self, img) -> int:\n    \t\"\"\"\n    \tCalculate the contrast of a grayscale image.\n    \tArgs:\n    \t\timg (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "img = np.array([[0, 50], [200, 255]])",
    "example_output": "255",
    "example_reasoning": "The function calculates contrast by finding the difference between the maximum (255) and minimum (0) pixel values in the image, resulting in a contrast of 255.",
    "learn_content": "IyMgQ2FsY3VsYXRpbmcgQ29udHJhc3Qgb2YgYSBHcmF5c2NhbGUgSW1hZ2UKCkNvbnRyYXN0IGluIGEgZ3JheXNjYWxlIGltYWdlIHJlZmVycyB0byB0aGUgZGlmZmVyZW5jZSBpbiBsdW1pbmFuY2Ugb3IgY29sb3IgdGhhdCBtYWtlcyBhbiBvYmplY3QgZGlzdGluZ3Vpc2hhYmxlLiBIZXJlIGFyZSBtZXRob2RzIHRvIGNhbGN1bGF0ZSBjb250cmFzdDoKCiMjIyAxLiBCYXNpYyBDb250cmFzdCBDYWxjdWxhdGlvbgoKVGhlIHNpbXBsZXN0IHdheSB0byBkZWZpbmUgdGhlIGNvbnRyYXN0IG9mIGEgZ3JheXNjYWxlIGltYWdlIGlzIGJ5IHVzaW5nIHRoZSBkaWZmZXJlbmNlIGJldHdlZW4gdGhlIG1heGltdW0gYW5kIG1pbmltdW0gcGl4ZWwgdmFsdWVzOgoKJCQKXHRleHR7Q29udHJhc3R9ID0gXG1heChJKSAtIFxtaW4oSSkKJCQKCiMjIyAyLiBSTVMgQ29udHJhc3QKClJvb3QgTWVhbiBTcXVhcmUgKFJNUykgY29udHJhc3QgY29uc2lkZXJzIHRoZSBzdGFuZGFyZCBkZXZpYXRpb24gb2YgcGl4ZWwgaW50ZW5zaXRpZXM6CgokJApcdGV4dHtSTVMgQ29udHJhc3R9ID0gXGZyYWN7XHNpZ21hfXtcbXV9CiQkCgojIyMgMy4gTWljaGVsc29uIENvbnRyYXN0CgpNaWNoZWxzb24gY29udHJhc3QgaXMgZGVmaW5lZCBhczoKCiQkCkMgPSBcZnJhY3tJX3tcdGV4dHttYXh9fSAtIElfe1x0ZXh0e21pbn19fXtJX3tcdGV4dHttYXh9fSArIElfe1x0ZXh0e21pbn19fQokJAoKIyMjIEV4YW1wbGUgQ2FsY3VsYXRpb24KCkZvciBhIGdyYXlzY2FsZSBpbWFnZSB3aXRoIHBpeGVsIHZhbHVlcyByYW5naW5nIGZyb20gNTAgdG8gMjAwOgoKMS4gKipNYXhpbXVtIFBpeGVsIFZhbHVlKio6IDIwMCAgCjIuICoqTWluaW11bSBQaXhlbCBWYWx1ZSoqOiA1MCAgCjMuICoqQ29udHJhc3QgQ2FsY3VsYXRpb24qKjoKCiQkClx0ZXh0e0NvbnRyYXN0fSA9IDIwMCAtIDUwID0gMTUwCiQkCgojIyMgQXBwbGljYXRpb25zCgpDYWxjdWxhdGluZyBjb250cmFzdCBpcyBjcnVjaWFsIGluOgoKLSBJbWFnZSBxdWFsaXR5IGFzc2Vzc21lbnQKLSBQcmVwcm9jZXNzaW5nIGluIGNvbXB1dGVyIHZpc2lvbgotIEVuaGFuY2luZyB2aXNpYmlsaXR5IGluIGltYWdlcwotIE9iamVjdCBkZXRlY3Rpb24gYW5kIGFuYWx5c2lzCg==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def calculate_contrast(self, img):\n        \"\"\"\n        Calculate the contrast of a grayscale image.\n        Args:\n            img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n        Returns:\n            float: Contrast value rounded to 3 decimal places.\n        \"\"\"\n        # Find the maximum and minimum pixel values\n        max_pixel = np.max(img)\n        min_pixel = np.min(img)\n\n        # Calculate contrast\n        contrast = max_pixel - min_pixel\n\n        return round(float(contrast), 3)",
    "test_cases": [
      {
        "test": [
          [
            [
              0,
              50
            ],
            [
              200,
              255
            ]
          ]
        ],
        "expected_output": 255
      },
      {
        "test": [
          [
            [
              128,
              128
            ],
            [
              128,
              128
            ]
          ]
        ],
        "expected_output": 0
      }
    ],
    "function_name": "calculate_contrast"
  },
  {
    "id": 83,
    "title": "Dot Product Calculator",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBkb3QgcHJvZHVjdCBvZiB0d28gdmVjdG9ycy4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIHR3byAxRCBOdW1QeSBhcnJheXMgYXMgaW5wdXQgYW5kIHJldHVybiB0aGUgZG90IHByb2R1Y3QgYXMgYSBzaW5nbGUgbnVtYmVyLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def calculate_dot_product(self, vec1, vec2) -> float:\n    \t\"\"\"\n    \tCalculate the dot product of two vectors.\n    \tArgs:\n    \t\tvec1 (numpy.ndarray): 1D array representing the first vector.\n    \t\tvec2 (numpy.ndarray): 1D array representing the second vector.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "vec1 = np.array([1, 2, 3]), vec2 = np.array([4, 5, 6])",
    "example_output": "32",
    "example_reasoning": "The function calculates the dot product by multiplying corresponding elements of the two vectors and summing the results. For vec1 = [1, 2, 3] and vec2 = [4, 5, 6], the result is (1 * 4) + (2 * 5) + (3 * 6) = 32.",
    "learn_content": "IyMgQ2FsY3VsYXRpbmcgdGhlIERvdCBQcm9kdWN0IG9mIFR3byBWZWN0b3JzCgpUaGUgZG90IHByb2R1Y3QsIGFsc28ga25vd24gYXMgdGhlIHNjYWxhciBwcm9kdWN0LCBpcyBhIG1hdGhlbWF0aWNhbCBvcGVyYXRpb24gdGhhdCB0YWtlcyB0d28gZXF1YWwtbGVuZ3RoIHZlY3RvcnMgYW5kIHJldHVybnMgYSBzaW5nbGUgbnVtYmVyLiBJdCBpcyB3aWRlbHkgdXNlZCBpbiBwaHlzaWNzLCBnZW9tZXRyeSwgYW5kIGxpbmVhciBhbGdlYnJhLgoKIyMjIDEuIEZvcm11bGEgZm9yIHRoZSBEb3QgUHJvZHVjdAoKVGhlIGRvdCBwcm9kdWN0IG9mIHR3byB2ZWN0b3JzICRcbWF0aGJme2F9JCBhbmQgJFxtYXRoYmZ7Yn0kLCBlYWNoIG9mIGxlbmd0aCAkbiQsIGlzIGNhbGN1bGF0ZWQgYXMgZm9sbG93czoKCiQkClxtYXRoYmZ7YX0gXGNkb3QgXG1hdGhiZntifSA9IFxzdW1fe2k9MX1ee259IGFfaSBiX2kKJCQKClRoaXMgbWVhbnMgbXVsdGlwbHlpbmcgY29ycmVzcG9uZGluZyBlbGVtZW50cyBvZiB0aGUgdHdvIHZlY3RvcnMgYW5kIHN1bW1pbmcgdXAgdGhlIHJlc3VsdHMuCgojIyMgMi4gR2VvbWV0cmljIEludGVycHJldGF0aW9uCgpJbiBnZW9tZXRyaWMgdGVybXMsIHRoZSBkb3QgcHJvZHVjdCBjYW4gYWxzbyBiZSBleHByZXNzZWQgYXM6CgokJApcbWF0aGJme2F9IFxjZG90IFxtYXRoYmZ7Yn0gPSB8XG1hdGhiZnthfXwgfFxtYXRoYmZ7Yn18IFxjb3MgXHRoZXRhCiQkCgpXaGVyZToKCi0gJHxcbWF0aGJme2F9fCQgYW5kICR8XG1hdGhiZntifXwkIGFyZSB0aGUgbWFnbml0dWRlcyBvZiB0aGUgdmVjdG9ycy4KLSAkXHRoZXRhJCBpcyB0aGUgYW5nbGUgYmV0d2VlbiB0aGUgdHdvIHZlY3RvcnMuCgojIyMgMy4gUHJvcGVydGllcyBvZiB0aGUgRG90IFByb2R1Y3QKCjEuICoqQ29tbXV0YXRpdmUqKjogIAogICAkJAogICBcbWF0aGJme2F9IFxjZG90IFxtYXRoYmZ7Yn0gPSBcbWF0aGJme2J9IFxjZG90IFxtYXRoYmZ7YX0KICAgJCQKCjIuICoqRGlzdHJpYnV0aXZlKio6ICAKICAgJCQKICAgXG1hdGhiZnthfSBcY2RvdCAoXG1hdGhiZntifSArIFxtYXRoYmZ7Y30pID0gXG1hdGhiZnthfSBcY2RvdCBcbWF0aGJme2J9ICsgXG1hdGhiZnthfSBcY2RvdCBcbWF0aGJme2N9CiAgICQkCgozLiAqKk9ydGhvZ29uYWwgVmVjdG9ycyoqOiAgCiAgIElmOiAgCiAgICQkCiAgIFxtYXRoYmZ7YX0gXGNkb3QgXG1hdGhiZntifSA9IDAKICAgJCQgIAogICBUaGVuICRcbWF0aGJme2F9JCBhbmQgJFxtYXRoYmZ7Yn0kIGFyZSBwZXJwZW5kaWN1bGFyLgoKIyMjIDQuIEV4YW1wbGUgQ2FsY3VsYXRpb24KCkdpdmVuIHR3byB2ZWN0b3JzOgoKLSAkXG1hdGhiZnthfSA9IFsxLCAyLCAzXSQKLSAkXG1hdGhiZntifSA9IFs0LCA1LCA2XSQKClRoZSBkb3QgcHJvZHVjdCBpcyBjYWxjdWxhdGVkIGFzOgoKJCQKXG1hdGhiZnthfSBcY2RvdCBcbWF0aGJme2J9ID0gKDEgXGNkb3QgNCkgKyAoMiBcY2RvdCA1KSArICgzIFxjZG90IDYpID0gNCArIDEwICsgMTggPSAzMgokJAoKIyMjIENvbmNsdXNpb24KClRoZSBkb3QgcHJvZHVjdCBpcyBhIGZ1bmRhbWVudGFsIG9wZXJhdGlvbiBpbiB2ZWN0b3IgYWxnZWJyYSwgdXNlZnVsIGluIGRldGVybWluaW5nIGFuZ2xlcyBiZXR3ZWVuIHZlY3RvcnMsIHByb2plY3Rpb25zLCBhbmQgaW4gbWFueSBhcHBsaWNhdGlvbnMgYWNyb3NzIHBoeXNpY3MgYW5kIGVuZ2luZWVyaW5nLgoK",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def calculate_dot_product(self, vec1, vec2):\n        \"\"\"\n        Calculate the dot product of two vectors.\n        Args:\n            vec1 (numpy.ndarray): 1D array representing the first vector.\n            vec2 (numpy.ndarray): 1D array representing the second vector.\n        Returns:\n            float: Dot product of the two vectors.\n        \"\"\"\n        return np.dot(vec1, vec2)",
    "test_cases": [
      {
        "test": [
          [
            1,
            2,
            3
          ],
          [
            4,
            5,
            6
          ]
        ],
        "expected_output": 32
      },
      {
        "test": [
          [
            -1,
            2,
            3
          ],
          [
            4,
            -5,
            6
          ]
        ],
        "expected_output": 4
      }
    ],
    "function_name": "calculate_dot_product"
  },
  {
    "id": 84,
    "title": "Phi Transformation for Polynomial Features",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gcGVyZm9ybSBhIFBoaSBUcmFuc2Zvcm1hdGlvbiB0aGF0IG1hcHMgaW5wdXQgZmVhdHVyZXMgaW50byBhIGhpZ2hlci1kaW1lbnNpb25hbCBzcGFjZSBieSBnZW5lcmF0aW5nIHBvbHlub21pYWwgZmVhdHVyZXMuIFRoZSB0cmFuc2Zvcm1hdGlvbiBhbGxvd3MgbW9kZWxzIGxpa2UgbGluZWFyIHJlZ3Jlc3Npb24gdG8gZml0IG5vbmxpbmVhciBkYXRhIGJ5IGludHJvZHVjaW5nIG5ldyBmZWF0dXJlIGRpbWVuc2lvbnMgdGhhdCByZXByZXNlbnQgcG9seW5vbWlhbCBjb21iaW5hdGlvbnMgb2YgdGhlIG9yaWdpbmFsIGlucHV0IGZlYXR1cmVzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBsaXN0IG9mIG51bWVyaWNhbCBkYXRhIGFuZCBhIGRlZ3JlZSBhcyBpbnB1dHMsIGFuZCByZXR1cm4gYSBuZXN0ZWQgbGlzdCB3aGVyZSBlYWNoIGlubmVyIGxpc3QgcmVwcmVzZW50cyB0aGUgdHJhbnNmb3JtZWQgZmVhdHVyZXMgb2YgYSBkYXRhIHBvaW50LiBJZiB0aGUgZGVncmVlIGlzIGxlc3MgdGhhbiAwLCB0aGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiBhbiBlbXB0eSBsaXN0Lg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def phi_transform(self, data: list[float], degree: int) -> list[list[float]]:\n    \t\"\"\"\n    \tPerform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    \tArgs:\n    \t\tdata (list[float]): A list of numerical values to transform.\n    \t\tdegree (int): The degree of the polynomial expansion.\n\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "data = [1.0, 2.0], degree = 2",
    "example_output": "[[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]",
    "example_reasoning": "The Phi Transformation generates polynomial features for each data point up to the specified degree. For data = [1.0, 2.0] and degree = 2, the transformation creates a nested list where each row contains powers of the data point from 0 to 2.",
    "learn_content": "IyMgUGhpIFRyYW5zZm9ybWF0aW9uCgpUaGUgUGhpIFRyYW5zZm9ybWF0aW9uIG1hcHMgaW5wdXQgZmVhdHVyZXMgaW50byBhIGhpZ2hlci1kaW1lbnNpb25hbCBzcGFjZSBieSBnZW5lcmF0aW5nIHBvbHlub21pYWwgZmVhdHVyZXMuIFRoaXMgYWxsb3dzIG1vZGVscyBsaWtlIGxpbmVhciByZWdyZXNzaW9uIHRvIGZpdCBub25saW5lYXIgZGF0YSBieSBpbnRyb2R1Y2luZyBuZXcgZmVhdHVyZSBkaW1lbnNpb25zIHRoYXQgcmVwcmVzZW50IHBvbHlub21pYWwgY29tYmluYXRpb25zIG9mIHRoZSBvcmlnaW5hbCBpbnB1dCBmZWF0dXJlcy4KCiMjIyBXaHkgVXNlIFBoaSBUcmFuc2Zvcm1hdGlvbj8KCi0gVG8gaW5jcmVhc2UgdGhlIGV4cHJlc3NpdmUgcG93ZXIgb2Ygc2ltcGxlIG1vZGVscyBzdWNoIGFzIGxpbmVhciBtb2RlbHMuCi0gVG8gZW5hYmxlIGJldHRlciBmaXR0aW5nIG9mIG5vbmxpbmVhciByZWxhdGlvbnNoaXBzIGluIHRoZSBkYXRhLgoKIyMjIEVxdWF0aW9ucwoKRm9yIGFuIGlucHV0IHZhbHVlICR4JCwgdGhlIFBoaSBUcmFuc2Zvcm1hdGlvbiBleHBhbmRzIGl0IGFzOgoKJCQKXFBoaSh4KSA9IFsxLCB4LCB4XjIsIHheMywgXGRvdHMsIHheZF0KJCQKCldoZXJlICRkJCBpcyB0aGUgc3BlY2lmaWVkIGRlZ3JlZSwgYW5kICRcUGhpKHgpJCByZXByZXNlbnRzIHRoZSB0cmFuc2Zvcm1lZCBmZWF0dXJlIHZlY3Rvci4KCiMjIyBFeGFtcGxlIDE6IFBvbHlub21pYWwgRXhwYW5zaW9uIGZvciBPbmUgVmFsdWUKCkdpdmVuICR4ID0gMyQgYW5kICRkID0gMyQsIHRoZSBQaGkgVHJhbnNmb3JtYXRpb24gaXM6CgokJApcUGhpKDMpID0gWzEsIDMsIDksIDI3XQokJAoKIyMjIEV4YW1wbGUgMjogVHJhbnNmb3JtYXRpb24gZm9yIE11bHRpcGxlIFZhbHVlcwoKRm9yICRcdGV4dHtkYXRhfSA9IFsxLCAyXSQgYW5kICRkID0gMiQsIHRoZSBQaGkgVHJhbnNmb3JtYXRpb24gaXM6CgokJApcUGhpKFsxLCAyXSkgPSBcYmVnaW57Ym1hdHJpeH0gMSAmIDEgJiAxIFxcIDEgJiAyICYgNCBcZW5ke2JtYXRyaXh9CiQkCg==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def phi_transform(self, data: list[float], degree: int) -> list[list[float]]:\n    \t\"\"\"\n    \tPerform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    \tArgs:\n    \t\tdata (list[float]): A list of numerical values to transform.\n    \t\tdegree (int): The degree of the polynomial expansion.\n\n    \tReturns:\n    \t\tlist[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \t\"\"\"\n    \tif degree < 0 or not data:\n    \t\treturn []\n    \treturn np.array([[x ** i for i in range(degree + 1)] for x in data]).tolist()",
    "test_cases": [
      {
        "test": [
          [],
          2
        ],
        "expected_output": []
      },
      {
        "test": [
          [
            1.0,
            2.0
          ],
          -1
        ],
        "expected_output": []
      }
    ],
    "function_name": "phi_transform"
  },
  {
    "id": 85,
    "title": "Positional Encoding Calculator",
    "difficulty": "hard",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gaW1wbGVtZW50IHRoZSBQb3NpdGlvbmFsIEVuY29kaW5nIGxheWVyIGZvciBUcmFuc2Zvcm1lcnMuCiAgICAgIFRoZSBmdW5jdGlvbiBzaG91bGQgY2FsY3VsYXRlIHBvc2l0aW9uYWwgZW5jb2RpbmdzIGZvciBhIHNlcXVlbmNlIGxlbmd0aCAoYHBvc2l0aW9uYCkgYW5kIG1vZGVsIGRpbWVuc2lvbmFsaXR5IChgZF9tb2RlbGApIHVzaW5nIHNpbmUgYW5kIGNvc2luZSBmdW5jdGlvbnMgYXMgc3BlY2lmaWVkIGluIHRoZSBUcmFuc2Zvcm1lciBhcmNoaXRlY3R1cmUuCiAgICAgIFRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIC0xIGlmIGBwb3NpdGlvbmAgaXMgMCwgb3IgaWYgYGRfbW9kZWxgIGlzIGxlc3MgdGhhbiBvciBlcXVhbCB0byAwLiBUaGUgb3V0cHV0IHNob3VsZCBiZSBhIG51bXB5IGFycmF5IG9mIHR5cGUgYGZsb2F0MTZgLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def pos_encoding(self, position: int, d_model: int):\n    \t# Your code here\n    \tpos_encoding = np.float16(pos_encoding)\n    \treturn pos_encoding",
    "example_input": "position = 2, d_model = 8",
    "example_output": "[[[ 0.,0.,0.,0.,1.,1.,1.,1.,]\n  [ 0.8413,0.0998,0.01,0.001,0.5405,0.995,1.,1.]]]",
    "example_reasoning": "The function computes the positional encoding by calculating sine values for even indices and cosine values for odd indices, ensuring that the encoding provides the required positional information.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def pos_encoding(self, position: int, d_model: int):\n\n        if position == 0 or d_model <= 0:\n            return -1\n\n        # Create position and dimension indices\n        pos = np.arange(position, dtype=np.float32).reshape(position, 1)\n        ind = np.arange(d_model, dtype=np.float32).reshape(1, d_model)\n\n        # Compute the angles\n        angle_rads = pos / np.power(10000, (2 * (ind // 2)) / d_model)\n\n        # Apply sine to even indices, cosine to odd indices\n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Even indices (0, 2, 4...)\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Odd indices (1, 3, 5...)\n\n        # Convert to float16 as required\n        return angle_rads.astype(np.float16)\n",
    "test_cases": [
      {
        "test": [
          2,
          8
        ],
        "expected_output": [
          [
            0.0,
            1.0,
            0.0,
            1.0,
            0.0,
            1.0,
            0.0,
            1.0
          ],
          [
            0.8413,
            0.5405,
            0.09985,
            0.995,
            0.01,
            1.0,
            0.001,
            1.0
          ]
        ]
      },
      {
        "test": [
          5,
          16
        ],
        "expected_output": [
          [
            0.0,
            1.0,
            0.0,
            1.0,
            0.0,
            1.0,
            0.0,
            1.0,
            0.0,
            1.0,
            0.0,
            1.0,
            0.0,
            1.0,
            0.0,
            1.0
          ],
          [
            0.8413,
            0.5405,
            0.311,
            0.9502,
            0.09985,
            0.9951,
            0.03162,
            0.9995,
            0.01,
            1.0,
            0.003162,
            1.0,
            0.001,
            1.0,
            0.0003161,
            1.0
          ],
          [
            0.9092,
            -0.4163,
            0.5913,
            0.8066,
            0.1986,
            0.98,
            0.06323,
            0.998,
            0.02,
            1.0,
            0.006325,
            1.0,
            0.002001,
            1.0,
            0.0006323,
            1.0
          ],
          [
            0.1411,
            -0.9902,
            0.8125,
            0.5825,
            0.2954,
            0.9556,
            0.09473,
            0.9956,
            0.03,
            0.9995,
            0.009483,
            1.0,
            0.003,
            1.0,
            0.0009489,
            1.0
          ],
          [
            -0.7568,
            -0.6538,
            0.9536,
            0.301,
            0.3894,
            0.9209,
            0.1261,
            0.9922,
            0.03998,
            0.999,
            0.01265,
            1.0,
            0.004002,
            1.0,
            0.001265,
            1.0
          ]
        ]
      }
    ],
    "function_name": "pos_encoding"
  },
  {
    "id": 86,
    "title": "Detect Overfitting or Underfitting",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gZGV0ZXJtaW5lIHdoZXRoZXIgYSBtYWNoaW5lIGxlYXJuaW5nIG1vZGVsIGlzIG92ZXJmaXR0aW5nLCB1bmRlcmZpdHRpbmcsIG9yIHBlcmZvcm1pbmcgd2VsbCBiYXNlZCBvbiB0cmFpbmluZyBhbmQgdGVzdCBhY2N1cmFjeSB2YWx1ZXMuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSB0d28gaW5wdXRzOiBgdHJhaW5pbmdfYWNjdXJhY3lgIGFuZCBgdGVzdF9hY2N1cmFjeWAuIEl0IHNob3VsZCByZXR1cm4gb25lIG9mIHRocmVlIHZhbHVlczogMSBpZiBPdmVyZml0dGluZywgLTEgaWYgVW5kZXJmaXR0aW5nLCBvciAwIGlmIGEgR29vZCBmaXQuIFRoZSBydWxlcyBmb3IgZGV0ZXJtaW5hdGlvbiBhcmUgYXMgZm9sbG93czogCi0gKipPdmVyZml0dGluZyoqOiBUaGUgdHJhaW5pbmcgYWNjdXJhY3kgaXMgc2lnbmlmaWNhbnRseSBoaWdoZXIgdGhhbiB0aGUgdGVzdCBhY2N1cmFjeSAoZGlmZmVyZW5jZSA+IDAuMikuCi0gKipVbmRlcmZpdHRpbmcqKjogQm90aCB0cmFpbmluZyBhbmQgdGVzdCBhY2N1cmFjeSBhcmUgYmVsb3cgMC43LgotICoqR29vZCBmaXQqKjogTmVpdGhlciBvZiB0aGUgYWJvdmUgY29uZGl0aW9ucyBpcyB0cnVlLg==",
    "starter_code": "\n\nclass Solution:\n    def model_fit_quality(self, training_accuracy, test_accuracy):\n    \t\"\"\"\n    \tDetermine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    \t:param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    \t:param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    \t:return: int, one of '1', '-1', or '0'.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "training_accuracy = 0.95, test_accuracy = 0.65",
    "example_output": "'1'",
    "example_reasoning": "The training accuracy is much higher than the test accuracy (difference = 0.30 > 0.2). This indicates that the model is overfitting to the training data and generalizes poorly to unseen data.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def model_fit_quality(self, training_accuracy, test_accuracy):\n        \"\"\"\n        Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n        :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n        :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n        :return: int, one of '1', '-1', or '0'.\n        \"\"\"\n        if training_accuracy - test_accuracy > 0.2:\n            return 1\n        elif training_accuracy < 0.7 and test_accuracy < 0.7:\n            return -1\n        else:\n            return 0",
    "test_cases": [
      {
        "test": [
          0.95,
          0.65
        ],
        "expected_output": 1
      },
      {
        "test": [
          0.6,
          0.5
        ],
        "expected_output": -1
      }
    ],
    "function_name": "model_fit_quality"
  },
  {
    "id": 87,
    "title": "Adam Optimizer",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBBZGFtIG9wdGltaXplciB1cGRhdGUgc3RlcCBmdW5jdGlvbi4gWW91ciBmdW5jdGlvbiBzaG91bGQgdGFrZSB0aGUgY3VycmVudCBwYXJhbWV0ZXIgdmFsdWUsIGdyYWRpZW50LCBhbmQgbW92aW5nIGF2ZXJhZ2VzIGFzIGlucHV0cywgYW5kIHJldHVybiB0aGUgdXBkYXRlZCBwYXJhbWV0ZXIgdmFsdWUgYW5kIG5ldyBtb3ZpbmcgYXZlcmFnZXMuIFRoZSBmdW5jdGlvbiBzaG91bGQgYWxzbyBoYW5kbGUgc2NhbGFyIGFuZCBhcnJheSBpbnB1dHMgYW5kIGluY2x1ZGUgYmlhcyBjb3JyZWN0aW9uIGZvciB0aGUgbW92aW5nIGF2ZXJhZ2VzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def adam_optimizer(self, parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \t\"\"\"\n    \tUpdate parameters using the Adam optimizer.\n    \tAdjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    \t:param parameter: Current parameter value\n    \t:param grad: Current gradient\n    \t:param m: First moment estimate\n    \t:param v: Second moment estimate\n    \t:param t: Current timestep\n    \t:param learning_rate: Learning rate (default=0.001)\n    \t:param beta1: First moment decay rate (default=0.9)\n    \t:param beta2: Second moment decay rate (default=0.999)\n    \t:param epsilon: Small constant for numerical stability (default=1e-8)\n    \t:return: tuple: (updated_parameter, updated_m, updated_v)\n    \t\"\"\"\n    \t# Your code here\n    \treturn np.round(parameter,5), np.round(m,5), np.round(v,5)",
    "example_input": "parameter = 1.0, grad = 0.1, m = 0.0, v = 0.0, t = 1",
    "example_output": "(0.999, 0.01, 0.0001)",
    "example_reasoning": "The Adam optimizer computes updated values for the parameter, first moment (m), and second moment (v) using bias-corrected estimates of gradients. With input values parameter=1.0, grad=0.1, m=0.0, v=0.0, and t=1, the updated parameter becomes 0.999.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def adam_optimizer(self, parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        \"\"\"\n        Update parameters using the Adam optimizer.\n        Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n        :param parameter: Current parameter value\n        :param grad: Current gradient\n        :param m: First moment estimate\n        :param v: Second moment estimate\n        :param t: Current timestep\n        :param learning_rate: Learning rate (default=0.001)\n        :param beta1: First moment decay rate (default=0.9)\n        :param beta2: Second moment decay rate (default=0.999)\n        :param epsilon: Small constant for numerical stability (default=1e-8)\n        :return: tuple: (updated_parameter, updated_m, updated_v)\n        \"\"\"\n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * grad\n\n        # Update biased second raw moment estimate\n        v = beta2 * v + (1 - beta2) * (grad**2)\n\n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1**t)\n\n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2**t)\n\n        # Update parameters\n        update = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n        parameter = parameter - update\n\n        return np.round(parameter,5), np.round(m,5), np.round(v,5)",
    "test_cases": [
      {
        "test": [
          1.0,
          0.1,
          0.0,
          0.0,
          1
        ],
        "expected_output": "(0.999, 0.01, 0.0001)"
      },
      {
        "test": [
          [
            1.0,
            2.0
          ],
          [
            0.1,
            0.2
          ],
          2,
          2,
          1
        ],
        "expected_output": "(array([0.999, 1.999]), array([0.01, 0.02]), array([1.e-05, 4.e-05]))"
      }
    ],
    "function_name": "adam_optimizer"
  },
  {
    "id": 88,
    "title": "GPT-2 Text Generation",
    "difficulty": "hard",
    "category": "nlp",
    "problem_description": "IyMjIEltcGxlbWVudCBhIFNpbXBsaWZpZWQgR1BULTItbGlrZSBUZXh0IEdlbmVyYXRpb24gRnVuY3Rpb24KCllvdSBhcmUgdGFza2VkIHdpdGggaW1wbGVtZW50aW5nIGEgc2ltcGxpZmllZCBHUFQtMi1saWtlIHRleHQgZ2VuZXJhdGlvbiBmdW5jdGlvbiBpbiBQeXRob24uIFRoaXMgZnVuY3Rpb24gd2lsbCBpbmNvcnBvcmF0ZSB0aGUgZm9sbG93aW5nIGNvbXBvbmVudHMgb2YgYSBtaW5pbWFsIEdQVC0yIGFyY2hpdGVjdHVyZToKCi0gKipUb2tlbiBFbWJlZGRpbmdzKio6IE1hcCBpbnB1dCB0b2tlbnMgdG8gZGVuc2UgdmVjdG9yIHJlcHJlc2VudGF0aW9ucy4KLSAqKlBvc2l0aW9uYWwgRW1iZWRkaW5ncyoqOiBBZGQgcG9zaXRpb25hbCBpbmZvcm1hdGlvbiB0byB0b2tlbiBlbWJlZGRpbmdzLgotICoqTXVsdGktaGVhZCBBdHRlbnRpb24qKjogQXR0ZW5kIHRvIHZhcmlvdXMgcGFydHMgb2YgdGhlIHNlcXVlbmNlLgotICoqRmVlZC1Gb3J3YXJkIE5ldHdvcmsqKjogUHJvY2VzcyBhdHRlbnRpb24gb3V0cHV0cyB0aHJvdWdoIGEgZGVuc2UgbGF5ZXIuCi0gKipMYXllciBOb3JtYWxpemF0aW9uKio6IFN0YWJpbGl6ZSB0aGUgdHJhaW5pbmcgcHJvY2Vzcy4KClRoZSBmdW5jdGlvbiBtdXN0IHRha2UgaW4gdGhlIGZvbGxvd2luZyBwYXJhbWV0ZXJzOgoKMS4gKipQcm9tcHQqKjogVGhlIGluaXRpYWwgdGV4dCB0byBndWlkZSB0aGUgZ2VuZXJhdGlvbiBwcm9jZXNzLgoyLiAqKk51bWJlciBvZiBUb2tlbnMgdG8gR2VuZXJhdGUqKjogU3BlY2lmeSBob3cgbWFueSB0b2tlbnMgdG8gb3V0cHV0LgoKWW91ciBmdW5jdGlvbiBzaG91bGQgb3V0cHV0IHRoZSBnZW5lcmF0ZWQgdGV4dC4KCkFkZGl0aW9uYWxseSwgdXRpbGl6ZSB0aGUgaGVscGVyIGZ1bmN0aW9uIGBsb2FkX2VuY29kZXJfaHBhcmFtc19hbmRfcGFyYW1zYCB0byByZXRyaWV2ZToKCi0gQSBkdW1teSBlbmNvZGVyLgotIE1vZGVsIGh5cGVycGFyYW1ldGVycy4KLSBNb2RlbCBwYXJhbWV0ZXJzLgoKQnVpbGQgeW91ciB0ZXh0IGdlbmVyYXRpb24gbG9naWMgYXJvdW5kIHRoZXNlIGNvbXBvbmVudHMuIFRoaXMgZXhlcmNpc2UgaXMgZGVzaWduZWQgdG8gaGVscCB5b3UgdW5kZXJzdGFuZCB0aGUgY29yZSBjb25jZXB0cyBiZWhpbmQgR1BULTIncyBhdXRvcmVncmVzc2l2ZSB0ZXh0IGdlbmVyYXRpb24u",
    "starter_code": "\n\nclass Solution:\n    def gen_text(self, prompt: str, n_tokens_to_generate: int = 40):\n    \t####your code goes here####\n    \tpass\n\n    def load_encoder_hparams_and_params(self, model_size: str = \"124M\", models_dir: str = \"models\"):\n    \tclass DummyBPE:\n    \t\tdef __init__(self):\n    \t\t\tself.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n    \t\tdef encode(self, self, text: str):\n    \t\t\ttokens = text.strip().split()\n    \t\t\treturn [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n    \t\tdef decode(self, self, token_ids: list):\n    \t\t\treversed_dict = {v: k for k, v in self.encoder_dict.items()}\n    \t\t\treturn \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    \thparams = {\n    \t\t\"n_ctx\": 1024,\n    \t\t\"n_head\": 12\n    \t}\n\n    \tparams = {\n    \t\t\"wte\": np.random.rand(3, 10),\n    \t\t\"wpe\": np.random.rand(1024, 10),\n    \t\t\"blocks\": [],\n    \t\t\"ln_f\": {\n    \t\t\t\"g\": np.ones(10),\n    \t\t\t\"b\": np.zeros(10),\n    \t\t}\n    \t}\n\n    \tencoder = DummyBPE()\n    \treturn encoder, hparams, params",
    "example_input": "prompt=\"hello\", n_tokens_to_generate=5",
    "example_output": "world <UNK> <UNK> <UNK> <UNK>",
    "example_reasoning": "The function encodes the input \"hello\" into tokens using the dummy encoder, then runs a simplified GPT-2 forward pass to generate 5 tokens. Finally, it decodes the generated tokens back into text.",
    "learn_content": "# Understanding Transformer Architecture and Text Generation

Transformers have revolutionized the field of Natural Language Processing (NLP) with their efficient and scalable architecture. This guide provides an in-depth look into the core components of transformers and how they facilitate advanced text generation.

## 1. Introduction to Transformers

Transformers are a groundbreaking neural network architecture that has significantly advanced NLP. Introduced in the seminal paper *"Attention is All You Need"* by Vaswani et al. (2017), transformers have outperformed traditional models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) in various NLP tasks.

### Key Advantages of Transformers

- **Parallel Processing**:  
  Unlike RNNs, which process input sequences sequentially, transformers handle entire sequences simultaneously. This parallelism leads to substantial improvements in training speed and efficiency.

- **Scalability**:  
  Transformers can effectively scale to handle large datasets and complex tasks, making them ideal for applications like language translation, text generation, and summarization.

- **Self-Attention Mechanism**:  
  The core innovation of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sequence relative to each other. This capability enables the model to capture long-range dependencies and contextual relationships within the text.

### Applications of Transformers

- **Text Generation**: Creating coherent and contextually relevant text based on a given prompt.
- **Machine Translation**: Translating text from one language to another with high accuracy.
- **Text Summarization**: Condensing long documents into concise summaries while retaining key information.
- **Question Answering**: Providing accurate answers to user queries based on contextual understanding.

---

## 2. Core Concepts

To fully grasp the transformer architecture, it's essential to understand its foundational components. Below are the core concepts that constitute the building blocks of transformers:

### 2.1 GELU Activation Function

The Gaussian Error Linear Unit (GELU) is an advanced activation function that enhances the performance of deep neural networks.

**Mathematical Expression**:  
$$
\text{GELU}(x) = 0.5 \cdot x \cdot (1 + \tanh(\sqrt{\frac{2}{\pi}} \cdot (x + 0.044715 \cdot x^3)))
$$

**Purpose**:  
GELU introduces non-linearity in the network while maintaining smooth gradient flow. Unlike the Rectified Linear Unit (ReLU) or Sigmoid functions, GELU provides a probabilistic approach to activation, allowing for better handling of uncertainty and improving model performance in deep architectures.

**Benefits**:
- **Smooth Activation**: Reduces the likelihood of "dead neurons" that can occur with ReLU.
- **Improved Gradient Flow**: Facilitates more stable and efficient training by preventing gradient vanishing or exploding.

### 2.2 Softmax for Attention

Softmax is a fundamental function used to convert raw attention scores into a probability distribution, ensuring that the weights sum to one.

**Mathematical Expression**:  
$$
\text{Softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n \exp(x_j)}
$$

**Purpose**:  
In the context of attention mechanisms, Softmax normalizes the attention scores, enabling the model to focus on relevant parts of the input sequence by assigning higher weights to more important tokens.

**Example**:  
If the attention scores for a sentence are `[2, 1, 0.1]`, applying Softmax will convert these to probabilities like `[0.659, 0.242, 0.099]`, indicating the relative importance of each token.

### 2.3 Layer Normalization

Layer normalization stabilizes and accelerates the training process by standardizing the inputs across the features.

**Mathematical Expression**:  
$$
\text{LayerNorm}(x) = g \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + b
$$

Where:
- \( \mu \): Mean of input \( x \) along the last axis.
- \( \sigma^2 \): Variance of \( x \).
- \( g, b \): Learnable scaling and bias parameters.
- \( \epsilon \): A small constant to prevent division by zero.

**Purpose**:  
By normalizing the inputs, layer normalization ensures that each layer receives inputs with a consistent distribution, which enhances training stability and convergence speed.

---

### 2.4 Multi-Head Attention

Multi-head attention is an extension of the attention mechanism that allows the model to focus on different representation subspaces simultaneously.

**Components**:
- **Query (Q), Key (K), Value (V) Matrices**: Each attention head computes its own set of Q, K, and V matrices by projecting the input embeddings into different subspaces.
- **Scaled Dot-Product Attention**:
  $$
  \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
  $$

**Benefits**:
- **Diversity of Attention**: Allows the model to focus on different parts of the input simultaneously.
- **Enhanced Representation**: Captures richer features by aggregating multiple attention heads.

---

### 2.5 Feedforward Network (FFN)

The Feedforward Network is a simple yet powerful component applied to each position independently within the transformer.

**Mathematical Expression**:  
$$
\text{FFN}(x) = \text{Linear}_2(\text{GELU}(\text{Linear}_1(x)))
$$

**Structure**:
1. First Linear Layer: Projects the input to a higher-dimensional space.
2. GELU Activation: Introduces non-linearity to the model.
3. Second Linear Layer: Projects the data back to the original dimensionality.

**Purpose**:  
The FFN enhances the model's capacity to learn intricate patterns.

---

### 2.6 Transformer Block

A transformer block is the fundamental building unit of the transformer architecture, combining multi-head attention and the feedforward network with residual connections and layer normalization.

**Structure**:
- **Multi-Head Attention Layer**:  
  $$ x_1 = \text{LayerNorm}(x + \text{MHA}(x)) $$
- **Feedforward Network**:  
  $$ x_2 = \text{LayerNorm}(x_1 + \text{FFN}(x_1)) $$

**Advantages**:
- **Deep Architecture Support**: Facilitates the construction of deep networks without significant performance degradation.
- **Modularity**: Each transformer block can be stacked multiple times, allowing for scalable model depth.

---

### 2.7 GPT-2 Text Generation

GPT-2 (Generative Pre-trained Transformer 2) leverages the transformer architecture for generating human-like text. Developed by OpenAI, GPT-2 has demonstrated remarkable capabilities in various NLP tasks.

**Key Components**:
- **Word and Positional Embeddings**: Captures semantic meaning and token position in a sequence.
- **Causal Attention**: Ensures left-to-right text generation by masking future tokens.
- **Stacked Transformer Blocks**: Refines input representations iteratively.

**Text Generation Process**:
1. Provide a prompt to initiate the process.
2. Tokenize the input into embeddings.
3. Process embeddings through transformer blocks.
4. Generate the next token based on probabilities.
5. Repeat steps 3-4 to produce coherent text.

---

### Conclusion

Transformers have fundamentally transformed NLP by introducing efficient and scalable architectures capable of handling complex language tasks. Understanding their core components such as GELU activation, Softmax attention, layer normalization, multi-head attention, feedforward networks, and the transformer block provides a foundation for leveraging these models in various applications. GPT-2 exemplifies the transformative power of these architectures while highlighting ethical considerations for their use.
",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def gelu(self, x):\n        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n    def layer_norm(self, x, g, b, eps=1e-5):\n        mean = np.mean(x, axis=-1, keepdims=True)\n        variance = np.var(x, axis=-1, keepdims=True)\n        return g * (x - mean) / np.sqrt(variance + eps) + b\n\n    def linear(self, x, w, b):\n        return x @ w + b\n\n    def ffn(self, x, c_fc, c_proj):\n        return linear(gelu(linear(x, **c_fc)), **c_proj)\n\n    def attention(self, q, k, v, mask):\n        return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n\n    def mha(self, x, c_attn, c_proj, n_head):\n        x = linear(x, **c_attn)\n        qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), np.split(x, 3, axis=-1)))\n        causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n        out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n        x = linear(np.hstack(out_heads), **c_proj)\n        return x\n\n    def transformer_block(self, x, mlp, attn, ln_1, ln_2, n_head):\n        x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n        x = x + ffn(layer_norm(x, **ln_2), **mlp)\n        return x\n\n    def gpt2(self, inputs, wte, wpe, blocks, ln_f, n_head):\n        x = wte[inputs] + wpe[range(len(inputs))]\n        for block in blocks:\n            x = transformer_block(x, **block, n_head=n_head)\n        return layer_norm(x, **ln_f) @ wte.T\n\n    def generate(self, inputs, params, n_head, n_tokens_to_generate):\n        for _ in range(n_tokens_to_generate):\n            logits = gpt2(inputs, **params, n_head=n_head)\n            next_id = np.argmax(logits[-1])\n            inputs.append(int(next_id))\n        return inputs[len(inputs) - n_tokens_to_generate:]\n\n    def gen_text(self, prompt: str, n_tokens_to_generate: int = 40):\n        np.random.seed(42)  # Set the random seed for reproducibility\n        encoder, hparams, params = load_encoder_hparams_and_params()\n        input_ids = encoder.encode(prompt)\n        assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n        output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n        output_text = encoder.decode(output_ids)\n        return output_text",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nprint(gen_text(\"hello\", n_tokens_to_generate=5))"
        ],
        "expected_output": "hello hello hello <UNK> <UNK>"
      },
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nprint(gen_text(\"hello world\", n_tokens_to_generate=10))"
        ],
        "expected_output": "world world world world world world world world world world"
      }
    ],
    "function_name": "gen_text"
  },
  {
    "id": 89,
    "title": "The Pattern Weaver's Code",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "RGVlcCBpbiB0aGUgQ3J5c3RhbCBDYXZlLCB0aGUgZW5pZ21hdGljIFBhdHRlcm4gV2VhdmVyIGNyZWF0ZXMgc3R1bm5pbmcgc2VxdWVuY2VzIGJ5IHVuY292ZXJpbmcgdGhlIGludHJpY2F0ZSByZWxhdGlvbnNoaXBzIGJldHdlZW4gY3J5c3RhbHMuIEVhY2ggY3J5c3RhbCBpcyBtYXJrZWQgYnkgYSB1bmlxdWUgbnVtZXJpYyB2YWx1ZSwgYW5kIHRoZSBXZWF2ZXIgZW1waGFzaXplcyB0aGF0IHRoZSB0cnVlIHBvd2VyIG9mIGFueSBjcnlzdGFsIGRlcGVuZHMgb24gaG93IGl0IGludGVyYWN0cyB3aXRoIGFsbCBvdGhlcnMuIFlvdSBoYXZlIGRpc2NvdmVyZWQgTiBjcnlzdGFscywgZWFjaCB3aXRoIGEgc3BlY2lmaWMgdmFsdWUsIGFuZCB5b3VyIHRhc2sgaXMgdG8gcmV2ZWFsIHRoZWlyIGVuaGFuY2VkIHBhdHRlcm5zIGJ5IGFuYWx5emluZyB0aGVzZSByZWxhdGlvbnNoaXBzIHVzaW5nIHNlbGYtYXR0ZW50aW9uLiBHaXZlbiBhIHNlcXVlbmNlIG9mIGNyeXN0YWxzIGFuZCB0aGVpciB2YWx1ZXMsIHlvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYSBzaW1wbGlmaWVkIHNlbGYtYXR0ZW50aW9uIG1lY2hhbmlzbS4gRm9yIGVhY2ggY3J5c3RhbCwgY2FsY3VsYXRlIGl0cyByZWxhdGlvbnNoaXAgd2l0aCBldmVyeSBvdGhlciBjcnlzdGFsLCBjb21wdXRlIHRoZSBhdHRlbnRpb24gc2NvcmVzIHVzaW5nIHRoZSBzb2Z0bWF4IGZ1bmN0aW9uLCBhbmQgZGVyaXZlIHRoZSBmaW5hbCB3ZWlnaHRlZCBwYXR0ZXJuIGZvciBlYWNoIGNyeXN0YWwuIFRoaXMgUHJvYmxlbSB3YXMgbWFkZSB3aXRoIHRoZSBoZWxwIG9mIEdyb3VuZFplcm8gQUk=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def softmax(self, values):\n    \t# Implement the softmax function\n    \tpass\n\n    def pattern_weaver(self, n, crystal_values, dimension):\n    \t# Your code here\n    \treturn np.round(x,3)",
    "example_input": "number of crystals: 5\nvalues: 4 2 7 1 9\n dimension: 1",
    "example_output": "[8.9993, 8.9638, 9.0, 8.7259, 9.0]",
    "example_reasoning": "The self-attention mechanism calculates relationships (attention scores) for each crystal using the given formula. These scores are converted to probabilities using the softmax function, and the final weighted pattern for each crystal is derived by summing the weighted values.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def softmax(self, values):\n        exps = np.exp(values - np.max(values))\n        return exps / np.sum(exps)\n\n    def pattern_weaver(self, n, crystal_values, dimension):\n        dimension_sqrt = np.sqrt(dimension)\n        final_patterns = []\n\n        for i in range(n):\n            attention_scores = []\n            for j in range(n):\n                score = crystal_values[i] * crystal_values[j] / dimension_sqrt\n                attention_scores.append(score)\n\n            softmax_scores = softmax(attention_scores)\n            weighted_sum = sum(softmax_scores[k] * crystal_values[k] for k in range(n))\n            final_patterns.append(round(weighted_sum, 4))\n\n        return final_patterns",
    "test_cases": [
      {
        "test": [
          "print(pattern_weaver(5, [4, 2, 7, 1, 9], 1))"
        ],
        "expected_output": [
          8.9993,
          8.9638,
          9.0,
          8.7259,
          9.0
        ]
      },
      {
        "test": [
          "print(pattern_weaver(3, [1, 3, 5], 1))"
        ],
        "expected_output": [
          4.7019,
          4.995,
          4.9999
        ]
      }
    ],
    "function_name": "softmax"
  },
  {
    "id": 90,
    "title": "BM25 Ranking ",
    "difficulty": "medium",
    "category": "nlp",
    "problem_description": "SW1wbGVtZW50IHRoZSBCTTI1IHJhbmtpbmcgZnVuY3Rpb24gdG8gY2FsY3VsYXRlIGRvY3VtZW50IHNjb3JlcyBmb3IgYSBxdWVyeSBpbiBhbiBpbmZvcm1hdGlvbiByZXRyaWV2YWwgY29udGV4dC4gQk0yNSBpcyBhbiBhZHZhbmNlZCB2YXJpYXRpb24gb2YgVEYtSURGIHRoYXQgaW5jb3Jwb3JhdGVzIHRlcm0gZnJlcXVlbmN5IHNhdHVyYXRpb24sIGRvY3VtZW50IGxlbmd0aCBub3JtYWxpemF0aW9uLCBhbmQgYSBjb25maWd1cmFibGUgcGVuYWx0eSBmb3IgZG9jdW1lbnQgbGVuZ3RoIGVmZmVjdHMu",
    "starter_code": "import numpy as np\nfrom collections import Counter\n\nclass Solution:\n    def calculate_bm25_scores(self, corpus, query, k1=1.5, b=0.75):\n    \t# Your code here\n    \tpass\n    \treturn np.round(scores,3)",
    "example_input": "corpus = [['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], query = ['the', 'cat']",
    "example_output": "[0.693, 0., 0. ]",
    "example_reasoning": "BM25 calculates scores for each document in the corpus by evaluating how well the query terms match each document while considering term frequency saturation and document length normalization.",
    "learn_content": "IyMgKipCTTI1KioKCkJNMjUgKEJlc3QgTWF0Y2ggMjUpIGlzIHVzZWQgaW4gaW5mb3JtYXRpb24gcmV0cmlldmFsIGZvciBzZWFyY2ggcmVsZXZhbmNlLiBTaW1pbGFyIHRvIFRGLUlERiwgaXQgcmVmbGVjdHMgdGhlIGltcG9ydGFuY2Ugb2YgYSB3b3JkIGluIGEgZG9jdW1lbnQgd2l0aGluIGEgY29sbGVjdGlvbiBvciBjb3JwdXMuIEhvd2V2ZXIsIEJNMjUgaW1wcm92ZXMgdXBvbiBURi1JREYgYnkgYWRkcmVzc2luZyBrZXkgbGltaXRhdGlvbnMuCgojIyMgTGltaXRhdGlvbnMgb2YgVEYtSURGIEFkZHJlc3NlZCBieSBCTTI1CgoxLiAqKlNhdHVyYXRpb24qKjogSW4gVEYtSURGLCBoYXZpbmcgYSB0ZXJtIG11bHRpcGxlIHRpbWVzIGluIGEgZG9jdW1lbnQgc2tld3MgdGhlIHRlcm0gZnJlcXVlbmN5LCBtYWtpbmcgdGhlIGRvY3VtZW50IG92ZXJseSByZWxldmFudC4gQk0yNSBtaXRpZ2F0ZXMgdGhpcyBieSB1c2luZzogIAogICAkJCBcdGV4dHtURi1hZGp1c3RlZH0gPSBcZnJhY3tcdGV4dHtURn19e1x0ZXh0e1RGfSArIGtfMX0gJCQgIAoKMi4gKipEb2N1bWVudCBMZW5ndGggTm9ybWFsaXphdGlvbioqOiBCTTI1IGFjY291bnRzIGZvciBkb2N1bWVudCBsZW5ndGggYnkgbm9ybWFsaXppbmcgdGVybSBmcmVxdWVuY2llcyB1c2luZzogIAogICAkJCBcdGV4dHtOb3JtYWxpemVkIExlbmd0aH0gPSAxIC0gYiArIGIgXHRpbWVzIFxmcmFje1x0ZXh0e0RvYyBMZW59fXtcdGV4dHtBdmVyYWdlIERvYyBMZW59fSAkJCAgCgozLiAqKkFtcGxpZnlpbmcgUGFyYW1ldGVyKio6IFRoZSAkYiQgcGFyYW1ldGVyIGNvbnRyb2xzIHRoZSBpbmZsdWVuY2Ugb2YgZG9jdW1lbnQgbGVuZ3RoIG5vcm1hbGl6YXRpb24uIEhpZ2hlciAkYiQgdmFsdWVzIGFtcGxpZnkgdGhlIGVmZmVjdC4KCiMjIyBGaW5hbCBCTTI1IEZvcm11bGEKClRoZSBCTTI1IHNjb3JlIGZvciBhIHRlcm0gaXMgZ2l2ZW4gYnk6ICAKJCQgXHRleHR7Qk0yNX0gPSBcdGV4dHtJREZ9IFx0aW1lcyBcZnJhY3tcdGV4dHtURn0gXHRpbWVzIChrXzEgKyAxKX0ge1x0ZXh0e1RGfSArIGtfMSBcdGltZXMgKDEgLSBiICsgYiBcdGltZXMgXGZyYWN7XHRleHR7ZGx9fXtcdGV4dHthZGx9fSl9ICQkICAKCldoZXJlOiAgCi0gJCBcdGV4dHtURn0gJDogVGVybSBmcmVxdWVuY3kgaW4gdGhlIGRvY3VtZW50LiAgCi0gJCBcdGV4dHtJREZ9ICQ6IEludmVyc2UgZG9jdW1lbnQgZnJlcXVlbmN5LCBjYWxjdWxhdGVkIGFzICQgXGxvZyhcZnJhY3tOICsgMX17XHRleHR7ZGZ9ICsgMX0pICQuICAKLSAkIE4gJDogVG90YWwgbnVtYmVyIG9mIGRvY3VtZW50cy4gIAotICQgXHRleHR7ZGZ9ICQ6IE51bWJlciBvZiBkb2N1bWVudHMgY29udGFpbmluZyB0aGUgdGVybS4gIAotICQgXHRleHR7ZGx9ICQ6IERvY3VtZW50IGxlbmd0aC4gIAotICQgXHRleHR7YWRsfSAkOiBBdmVyYWdlIGRvY3VtZW50IGxlbmd0aC4gIAotICQga18xICQ6IFNhdHVyYXRpb24gcGFyYW1ldGVyLiAgCi0gJCBiICQ6IE5vcm1hbGl6YXRpb24gcGFyYW1ldGVyLgoKIyMjIEltcGxlbWVudGF0aW9uIFN0ZXBzCgoxLiBDb21wdXRlIGRvY3VtZW50IGxlbmd0aCAoJCBkbCAkKSBhbmQgYXZlcmFnZSBkb2N1bWVudCBsZW5ndGggKCQgYWRsICQpLiAgCjIuIENhbGN1bGF0ZSB0ZXJtIGZyZXF1ZW5jaWVzICgkIFRGICQpIHVzaW5nIHRoZSBCTTI1IGZvcm11bGEuICAKMy4gQ29tcHV0ZSBpbnZlcnNlIGRvY3VtZW50IGZyZXF1ZW5jaWVzICgkIElERiAkKSBmb3IgZWFjaCB0ZXJtLiAgCjQuIENhbGN1bGF0ZSBCTTI1IHNjb3JlcyBmb3IgZWFjaCBkb2N1bWVudC4KCiMjIyBBcHBsaWNhdGlvbnMKCkJNMjUgaXMgd2lkZWx5IHVzZWQgaW46ICAKLSBTZWFyY2ggRW5naW5lcyAgCi0gUmVjb21tZW5kYXRpb24gU3lzdGVtcyAgCi0gTmF0dXJhbCBMYW5ndWFnZSBQcm9jZXNzaW5nIChOTFApICAKClVuZGVyc3RhbmRpbmcgQk0yNSBlbmFibGVzIHRoZSBjcmVhdGlvbiBvZiByb2J1c3Qgc3lzdGVtcyBmb3Igc2VhcmNoIGFuZCByYW5raW5nIHRhc2tzLgo=",
    "solution_code": "import numpy as np\nfrom collections import Counter\n\nclass Solution:\n    def calculate_bm25_scores(self, corpus, query, k1=1.5, b=0.75):\n        if not corpus or not query:\n            raise ValueError(\"Corpus and query cannot be empty\")\n\n        doc_lengths = [len(doc) for doc in corpus]\n        avg_doc_length = np.mean(doc_lengths)\n        doc_term_counts = [Counter(doc) for doc in corpus]\n        doc_freqs = Counter()\n        for doc in corpus:\n            doc_freqs.update(set(doc))\n\n        scores = np.zeros(len(corpus))\n        N = len(corpus)\n\n        for term in query:\n            df = doc_freqs.get(term, 0) + 1\n            idf = np.log((N + 1) / df)\n\n            for idx, term_counts in enumerate(doc_term_counts):\n                if term not in term_counts:\n                    continue\n\n                tf = term_counts[term]\n                doc_len_norm = 1 - b + b * (doc_lengths[idx] / avg_doc_length)\n                term_score = (tf * (k1 + 1)) / (tf + k1 * doc_len_norm)\n                scores[idx] += idf * term_score\n\n        return np.round(scores, 3)",
    "test_cases": [
      {
        "test": [
          [
            [
              "the",
              "cat",
              "sat"
            ],
            [
              "the",
              "dog",
              "ran"
            ],
            [
              "the",
              "bird",
              "flew"
            ]
          ],
          [
            "the",
            "cat"
          ]
        ],
        "expected_output": [
          0.693,
          0.0,
          0.0
        ]
      },
      {
        "test": [
          [
            "<ast.BinOp object at 0x00000268D1ACA230>",
            [
              "the"
            ]
          ],
          [
            "the"
          ]
        ],
        "expected_output": [
          0,
          0
        ]
      }
    ],
    "function_name": "calculate_bm25_scores"
  },
  {
    "id": 91,
    "title": "Calculate F1 Score from Predicted and True Labels",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBGMSBzY29yZSBnaXZlbiBwcmVkaWN0ZWQgYW5kIHRydWUgbGFiZWxzLiBUaGUgRjEgc2NvcmUgaXMgYSB3aWRlbHkgdXNlZCBtZXRyaWMgaW4gbWFjaGluZSBsZWFybmluZywgY29tYmluaW5nIHByZWNpc2lvbiBhbmQgcmVjYWxsIGludG8gYSBzaW5nbGUgbWVhc3VyZS4gcm91bmQgeW91ciBzb2x1dGlvbiB0byB0aGUgM3JkIGRlY2ltYWwgcGxhY2U=",
    "starter_code": "\n\nclass Solution:\n    def calculate_f1_score(self, y_true, y_pred):\n    \t\"\"\"\n    \tCalculate the F1 score based on true and predicted labels.\n\n    \tArgs:\n    \t\ty_true (list): True labels (ground truth).\n    \t\ty_pred (list): Predicted labels.\n\n    \tReturns:\n    \t\tfloat: The F1 score rounded to three decimal places.\n    \t\"\"\"\n    \t# Your code here\n    \tpass\n    \treturn round(f1,3)",
    "example_input": "y_true = [1, 0, 1, 1, 0], y_pred = [1, 0, 0, 1, 1]",
    "example_output": "0.667",
    "example_reasoning": "The true positives, false positives, and false negatives are calculated from the given labels. Precision and recall are derived, and the F1 score is computed as their harmonic mean.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def calculate_f1_score(self, y_true, y_pred):\n        \"\"\"\n        Calculate the F1 score based on true and predicted labels.\n\n        Args:\n            y_true (list): True labels (ground truth).\n            y_pred (list): Predicted labels.\n\n        Returns:\n            float: The F1 score rounded to three decimal places.\n        \"\"\"\n        if len(y_true) != len(y_pred):\n            raise ValueError(\"Lengths of y_true and y_pred must be the same\")\n\n        tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp == 1)\n        fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n        fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n\n        if tp + fp == 0 or tp + fn == 0:\n            return 0.0\n\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n\n        if precision + recall == 0:\n            return 0.0\n\n        f1_score = 2 * (precision * recall) / (precision + recall)\n        return round(f1_score, 3)",
    "test_cases": [
      {
        "test": [
          [
            1,
            0,
            1,
            1,
            0
          ],
          [
            1,
            0,
            0,
            1,
            1
          ]
        ],
        "expected_output": 0.667
      },
      {
        "test": [
          [
            1,
            1,
            0,
            0
          ],
          [
            1,
            0,
            0,
            1
          ]
        ],
        "expected_output": 0.5
      }
    ],
    "function_name": "calculate_f1_score"
  },
  {
    "id": 92,
    "title": "Linear Regression - Power Grid Optimization",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SXQgaXMgdGhlIHllYXIgMjE1Ny4gTWFycyBoYXMgaXRzIGZpcnN0IHRocml2aW5nIGNvbG9ueSwgYW5kIGVuZXJneSBjb25zdW1wdGlvbiBpcyBzdGVhZGlseSBvbiB0aGUgcmlzZS4gQXMgdGhlIGxlYWQgZGF0YSBzY2llbnRpc3QsIHlvdSBoYXZlIGRhaWx5IHBvd2VyIHVzYWdlIG1lYXN1cmVtZW50cyAoMTAgZGF5cykgYWZmZWN0ZWQgYnkgYm90aCBhIGdyb3dpbmcgbGluZWFyIHRyZW5kIGFuZCBhIGRhaWx5IGZsdWN0dWF0aW9uLiBUaGUgZmx1Y3R1YXRpb24gZm9sbG93cyB0aGUgZm9ybXVsYSBm4bWiID0gMTAgeCBzaW4oMs+AIHggaSAvIDEwKSwgd2hlcmUgaSBpcyB0aGUgZGF5IG51bWJlciAoMSB0aHJvdWdoIDEwKS4gWW91ciBjaGFsbGVuZ2UgaXMgdG8gcmVtb3ZlIHRoaXMga25vd24gZmx1Y3R1YXRpb24gZnJvbSBlYWNoIGRhdGEgcG9pbnQsIGZpdCBhIGxpbmVhciByZWdyZXNzaW9uIG1vZGVsIHRvIHRoZSBkZXRyZW5kZWQgZGF0YSwgcHJlZGljdCBkYXkgMTUncyBiYXNlIGNvbnN1bXB0aW9uLCBhZGQgYmFjayB0aGUgZmx1Y3R1YXRpb24gZm9yIGRheSAxNSwgYW5kIGZpbmFsbHkgaW5jbHVkZSBhIDUlIHNhZmV0eSBtYXJnaW4uIFRoZSBmaW5hbCBhbnN3ZXIgbXVzdCBiZSBhbiBpbnRlZ2VyLCBlbnN1cmluZyB5b3UgbWVldCB0aGUgY29sb255J3MgZnV0dXJlIG5lZWRzLg==",
    "starter_code": "import math\n\nclass Solution:\n    PI = 3.14159\n\n    def power_grid_forecast(self, consumption_data):\n    \t# 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    \t# 2) Perform linear regression on the detrended data.\n    \t# 3) Predict day 15's base consumption.\n    \t# 4) Add the day 15 fluctuation back.\n    \t# 5) Round, then add a 5% safety margin (rounded up).\n    \t# 6) Return the final integer.\n    \tpass",
    "example_input": "Daily consumption data for 10 days: [150, 165, 185, 195, 210, 225, 240, 260, 275, 290]",
    "example_output": "404",
    "example_reasoning": "For each of the 10 days, we subtract the daily fluctuation given by 10xsin(2\u03c0xi/10). We then perform linear regression on the resulting values, predict day 15\u2019s base usage, and add back the day 15 fluctuation. Finally, we apply a 5% margin. Running the provided solution code yields 404 for this dataset.",
    "learn_content": "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",
    "solution_code": "import math\n\nclass Solution:\n    PI = 3.14159\n\n    def power_grid_forecast(self, consumption_data):\n        # consumption_data: list of 10 daily consumption values\n        # days: 1 through 10\n        days = list(range(1, 11))\n        n = len(days)\n\n        # 1) Remove daily fluctuation f_i = 10 * sin(2\u03c0 * i / 10)\n        detrended = []\n        for i, cons in zip(days, consumption_data):\n            fluctuation_i = 10 * math.sin((2 * PI * i) / 10)\n            detrended_value = cons - fluctuation_i\n            detrended.append(detrended_value)\n\n        # 2) Perform linear regression on the detrended data\n        sum_x = sum(days)\n        sum_y = sum(detrended)\n        sum_xy = sum(x * y for x, y in zip(days, detrended))\n        sum_x2 = sum(x**2 for x in days)\n\n        # slope (m) and intercept (b) for y = m*x + b\n        m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n        b = (sum_y - m * sum_x) / n\n\n        # 3) Predict day 15's base usage\n        day_15_base = m * 15 + b\n\n        # 4) Add back daily fluctuation for day 15\n        day_15_fluctuation = 10 * math.sin((2 * PI * 15) / 10)\n        day_15_prediction = day_15_base + day_15_fluctuation\n\n        # 5) Round and add 5% safety margin\n        day_15_rounded = round(day_15_prediction)\n        final_15 = math.ceil(day_15_rounded * 1.05)\n\n        return final_15",
    "test_cases": [
      {
        "test": [
          [
            150,
            165,
            185,
            195,
            210,
            225,
            240,
            260,
            275,
            290
          ]
        ],
        "expected_output": 404
      },
      {
        "test": [
          [
            160,
            170,
            190,
            200,
            215,
            230,
            245,
            265,
            280,
            295
          ]
        ],
        "expected_output": 407
      }
    ],
    "function_name": "power_grid_forecast"
  },
  {
    "id": 93,
    "title": "Calculate Mean Absolute Error (MAE)",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBNZWFuIEFic29sdXRlIEVycm9yIChNQUUpIGJldHdlZW4gdHdvIGFycmF5cyBvZiBhY3R1YWwgYW5kIHByZWRpY3RlZCB2YWx1ZXMuIFRoZSBNQUUgaXMgYSBtZXRyaWMgdXNlZCB0byBtZWFzdXJlIHRoZSBhdmVyYWdlIG1hZ25pdHVkZSBvZiBlcnJvcnMgaW4gYSBzZXQgb2YgcHJlZGljdGlvbnMgd2l0aG91dCBjb25zaWRlcmluZyB0aGVpciBkaXJlY3Rpb24u",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def mae(self, y_true, y_pred):\n    \t\"\"\"\n    \tCalculate Mean Absolute Error between two arrays.\n\n    \tParameters:\n    \ty_true (numpy.ndarray): Array of true values\n        y_pred (numpy.ndarray): Array of predicted values\n\n    \tReturns:\n    \tfloat: Mean Absolute Error rounded to 3 decimal places\n    \t\"\"\"\n    \t# Your code here\n    \tpass\n    \treturn round(val,3)",
    "example_input": "y_true = np.array([3, -0.5, 2, 7]), y_pred = np.array([2.5, 0.0, 2, 8])",
    "example_output": "0.500",
    "example_reasoning": "The MAE is calculated by taking the mean of the absolute differences between the predicted and true values. Using the formula, the result is 0.500.",
    "learn_content": "IyMgTWVhbiBBYnNvbHV0ZSBFcnJvciAoTUFFKQoKVGhlIE1lYW4gQWJzb2x1dGUgRXJyb3IgKE1BRSkgaXMgYSBtZWFzdXJlIG9mIHRoZSBhdmVyYWdlIG1hZ25pdHVkZSBvZiBlcnJvcnMgYmV0d2VlbiBwcmVkaWN0ZWQgYW5kIGFjdHVhbCB2YWx1ZXMuIEhlcmUncyBob3cgdG8gZXhwcmVzcyBpdCBtYXRoZW1hdGljYWxseToKCjEuICoqQmFzaWMgRm9ybXVsYSoqOgogICAtIFRoZSBNQUUgZm9ybXVsYSBjYW4gYmUgd3JpdHRlbiBhczogJE1BRSA9IFxmcmFjezF9e259XHN1bV97aT0xfV5uIHx5X2kgLSBcaGF0e3l9X2l8JAoKICAgV2hlcmU6CiAgIC0gJG4kIGlzIHRoZSBudW1iZXIgb2Ygb2JzZXJ2YXRpb25zCiAgIC0gJHlfaSQgaXMgdGhlIHRydWUgdmFsdWUKICAgLSAkXGhhdHt5fV9pJCBpcyB0aGUgcHJlZGljdGVkIHZhbHVlCiAgIC0gJHwuLi58JCByZXByZXNlbnRzIHRoZSBhYnNvbHV0ZSB2YWx1ZQoKMi4gKipFeGFtcGxlIENhbGN1bGF0aW9uKio6CiAgIEZvciB0aGUgdmFsdWVzOgogICBgYGAKICAgeV90cnVlID0gWzMsIC0wLjUsIDIsIDddCiAgIHlfcHJlZCA9IFsyLjUsIDAuMCwgMiwgOF0KICAgYGBgCgogICBUaGUgY2FsY3VsYXRpb24gd291bGQgYmU6CiAgICQkCiAgIFxiZWdpbnthbGlnbip9CiAgIE1BRSAmPSBcZnJhY3sxfXs0fSh8My0yLjV8ICsgfC0wLjUtMC4wfCArIHwyLTJ8ICsgfDctOHwpIFxcCiAgICY9IFxmcmFjezF9ezR9KDAuNSArIDAuNSArIDAgKyAxKSBcXAogICAmPSBcZnJhY3syfXs0fSBcXAogICAmPSAwLjUKICAgXGVuZHthbGlnbip9CiAgICQkCgozLiAqKlByb3BlcnRpZXMqKjoKICAgLSBNQUUgaXMgYWx3YXlzIG5vbi1uZWdhdGl2ZTogJE1BRSBcZ2VxIDAkCiAgIC0gUGVyZmVjdCBwcmVkaWN0aW9ucyByZXN1bHQgaW4gJE1BRSA9IDAkCiAgIC0gTUFFIGlzIG1lYXN1cmVkIGluIHRoZSBzYW1lIHVuaXRzIGFzIHRoZSBvcmlnaW5hbCBkYXRhCiAgIC0gTUFFIHRyZWF0cyBhbGwgZXJyb3JzIHdpdGggZXF1YWwgd2VpZ2h0ICh1bmxpa2UgTWVhbiBTcXVhcmVkIEVycm9yKQoKNC4gKipDb21wYXJpc29uIHdpdGggT3RoZXIgTWV0cmljcyoqOgogICBUaGUgZm9ybXVsYSBmb3IgTWVhbiBTcXVhcmVkIEVycm9yIChNU0UpIGlzOgogICAkJE1TRSA9IFxmcmFjezF9e259XHN1bV97aT0xfV5uICh5X2kgLSBcaGF0e3l9X2kpXjIkJAoKICAgV2hpbGUgTUFFIHVzZXMgYWJzb2x1dGUgdmFsdWVzLCBNU0Ugc3F1YXJlcyB0aGUgZGlmZmVyZW5jZXMsIHdoaWNoOgogICAtIE1ha2VzIE1TRSBtb3JlIHNlbnNpdGl2ZSB0byBvdXRsaWVycwogICAtIFJlc3VsdHMgaW4gTVNFIHZhbHVlcyB0aGF0IGFyZSBub3QgaW4gdGhlIG9yaWdpbmFsIHVuaXQgb2YgbWVhc3VyZW1lbnQ=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def mae(self, y_true, y_pred):\n        \"\"\"\n        Calculate Mean Absolute Error between two arrays.\n\n        Parameters:\n        y_true (numpy.ndarray): Array of true values\n        y_pred (numpy.ndarray): Array of predicted values\n\n        Returns:\n        float: Mean Absolute Error rounded to 3 decimal places\n        \"\"\"\n        if y_true.shape != y_pred.shape:\n            raise ValueError(\"Arrays must have the same shape\")\n        if y_true.size == 0:\n            raise ValueError(\"Arrays cannot be empty\")\n\n        return round(np.mean(np.abs(y_true - y_pred)), 3)",
    "test_cases": [
      {
        "test": [
          [
            3,
            -0.5,
            2,
            7
          ],
          [
            2.5,
            0.0,
            2,
            8
          ]
        ],
        "expected_output": 0.5
      },
      {
        "test": [
          [
            [
              0.5,
              1
            ],
            [
              -1,
              1
            ],
            [
              7,
              -6
            ]
          ],
          [
            [
              0,
              2
            ],
            [
              -1,
              2
            ],
            [
              8,
              -5
            ]
          ]
        ],
        "expected_output": 0.75
      }
    ],
    "function_name": "mae"
  },
  {
    "id": 94,
    "title": "Implement Multi-Head Attention",
    "difficulty": "hard",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBtdWx0aS1oZWFkIGF0dGVudGlvbiBtZWNoYW5pc20sIGEgY3JpdGljYWwgY29tcG9uZW50IG9mIHRyYW5zZm9ybWVyIG1vZGVscy4gR2l2ZW4gUXVlcnkgKFEpLCBLZXkgKEspLCBhbmQgVmFsdWUgKFYpIG1hdHJpY2VzLCBjb21wdXRlIHRoZSBhdHRlbnRpb24gb3V0cHV0cyBmb3IgbXVsdGlwbGUgaGVhZHMgYW5kIGNvbmNhdGVuYXRlIHRoZSByZXN1bHRzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def compute_qkv(self, X, W_q, W_k, W_v):\n    \tpass\n\n    def self_attention(self, Q, K, V):\n    \tpass\n\n    def multi_head_attention(self, Q, K, V, n_heads):\n    \tpass",
    "example_input": "Q = np.array([[1, 0], [0, 1]]), K = np.array([[1, 0], [0, 1]]), V = np.array([[1, 0], [0, 1]]), n_heads = 2",
    "example_output": "[[1., 0.], [0., 1.]]",
    "example_reasoning": "Multi-head attention is computed for 2 heads using the input Q, K, and V matrices. The resulting outputs for each head are concatenated to form the final attention output.",
    "learn_content": "## Understanding Multi-Head Attention

Multi-head attention is a fundamental mechanism in transformer models, allowing the model to focus on different parts of the input sequence simultaneously. This enables the model to capture a wider variety of relationships and dependencies, which is crucial for handling complex data, such as natural language. By using multiple attention heads, the model learns to attend to various aspects of the input at different levels of abstraction, enhancing its ability to capture complex relationships.

### Concepts

The attention mechanism allows the model to weigh the importance of different input elements based on their relevance to a specific task. In tasks like machine translation, for example, attention helps the model focus on relevant words in a sentence to understand the overall meaning. Multi-head attention extends this concept by using multiple attention heads, each learning different representations of the input data, which improves the model's ability to capture richer relationships and dependencies.

The process of multi-head attention involves several key steps:

1. **Computing Attention Scores:** This involves calculating how much focus each element in the input should receive based on its relationship with other elements.
2. **Applying Softmax:** The attention scores are transformed into probabilities using the softmax function, which normalizes the scores so that they sum to one.
3. **Aggregating Results:** The final output is computed by taking a weighted sum of the input values, where the weights are determined by the attention scores.

### Structure of Multi-Head Attention

The attention mechanism can be described with Query (Q), Key (K), and Value (V) matrices. The process of multi-head attention works by repeating the standard attention mechanism multiple times in parallel, with different sets of learned weight matrices for each attention head.

#### 1. Splitting Q, K, and V

Assume that the input Query (Q), Key (K), and Value (V) matrices have dimensions $(\text{seqLen}, d_{model})$, where $d_{\text{model}}$ is the model dimension. In multi-head attention, these matrices are divided into n smaller matrices, each corresponding to a different attention head. Each smaller matrix has dimensions $(\text{seqLen}, d_k)$, where $d_k = \frac{d_{\text{model}}}{n}$ is the dimensionality of each head.

For each attention $\text{head}_i$, we get its subset of Query $\text{Q}_i$, Key $\text{K}_i$, and Value $\text{V}_i$. These subsets are computed independently for each head.

#### 2. Computing Attention for Each Head

Each head independently computes its attention output. The calculation is similar to the single-head attention mechanism:

$$
\text{score}_i = \frac{Q_i K_i^T}{\sqrt{d_k}}
$$

Where $$d_k$$ is the dimensionality of the key space for each head. The scaling factor $$\frac{1}{\sqrt{d_k}}$$ ensures the dot product doesn't grow too large, preventing instability in the softmax function.

The softmax function is applied to the scores to normalize them, transforming them into attention weights for each head:

$$
\text{SoftmaxScore}_i = \text{softmax}(\text{score}_i)
$$

#### 3. Softmax Calculation and Numerical Stability

When computing the softmax function, especially in the context of attention mechanisms, there's a risk of numerical overflow or underflow, which can occur when the attention scores become very large or very small. This issue arises because the exponential function $$\exp$$ grows very quickly, and when dealing with large numbers, it can result in values that are too large for the computer to handle, leading to overflow errors.

To prevent this, we apply a common technique: subtracting the maximum score from each attention score before applying the exponential function. This helps to ensure that the largest value in the attention scores becomes zero, reducing the likelihood of overflow. Here's how it's done:

$$
\text{SoftmaxScore} = \frac{\exp(\text{score} - \text{score}_{\text{max}})}{\sum\exp(\text{score} - \text{score}_{\text{max}})}
$$

Where $$\text{score}_{i,\text{max}}$$ is the maximum value of the attention scores for the \(i\)-th head. Subtracting the maximum score from each individual score ensures that the largest value becomes 0, which prevents the exponentials from becoming too large.

This subtraction does not affect the final result of the softmax calculation because the softmax is a relative function it's the ratios of the exponentials that matter. Therefore, this adjustment ensures numerical stability while maintaining the correctness of the computation.

To summarize, when computing softmax in multi-head attention:

- Subtract the maximum score from each attention score before applying the exponential function.
- This technique prevents overflow by ensuring that the largest value becomes 0, which keeps the exponential values within a manageable range.
- The relative relationships between the scores remain unchanged, so the softmax output remains correct.

By applying this numerical stability trick, the softmax function becomes more robust and prevents computational issues that could arise during training or inference, especially when dealing with large models or sequences.

Finally, the attention output for each $$\text{head}_i$$ is computed as:

$$
\text{head}_i = \text{SoftmaxScore}_i \cdot V_i
$$

#### 4. Concatenation and Linear Transformation

After computing the attention output for each head, the outputs are concatenated along the feature dimension. This results in a matrix of dimensions $$(\text{seqLen}, d_{\text{model}})$$, where the concatenated attention outputs are passed through a final linear transformation to obtain the final multi-head attention output.

$$
\text{MultiHeadOutput} = \text{concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_n)
$$

The concatenated result is then linearly transformed using a weight matrix $W_{\text{o}}$ to obtain the final output. However, in our case, obtaining the multi-head attention output without this final transformation is sufficient:

$$
\text{MultiHeadOutput} = W_o \cdot \text{MultiHeadOutput}
$$

### Key Points

- Each attention head processes the input independently using its own set of learned weights. This allows each head to focus on different relationships in the data.
- Each head calculates its attention scores based on its corresponding Query, Key, and Value matrices, producing different attention outputs.
- The outputs of all attention heads are concatenated to form a unified representation. This concatenated result is then linearly transformed to generate the final output.

Multi-head attention allows the model to attend to different aspects of the input sequence in parallel, making it more capable of learning complex and diverse relationships. This parallelization of attention heads enhances the model's ability to understand the data from multiple angles simultaneously, contributing to improved performance in tasks like machine translation, text generation, and more.",
    "solution_code": "import numpy as np\nfrom typing import Tuple, List\n\nclass Solution:\n    def compute_qkv(self, X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Compute the Query (Q), Key (K), and Value (V) matrices.\n\n        Args:\n        X: numpy array of shape (seq_len, d_model), input sequence\n        W_q, W_k, W_v: numpy arrays of shape (d_model, d_model), weight matrices for Q, K, and V\n\n        Returns:\n        Q, K, V: numpy arrays of shape (seq_len, d_model)\n        \"\"\"\n        Q = np.dot(X, W_q)  # Compute the Query matrix Q\n        K = np.dot(X, W_k)  # Compute the Key matrix K\n        V = np.dot(X, W_v)  # Compute the Value matrix V\n        return Q, K, V\n\n    def self_attention(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute self-attention for a single head.\n\n        Args:\n        Q: numpy array of shape (seq_len, d_k), Query matrix\n        K: numpy array of shape (seq_len, d_k), Key matrix\n        V: numpy array of shape (seq_len, d_k), Value matrix\n\n        Returns:\n        attention_output: numpy array of shape (seq_len, d_k), output of the self-attention mechanism\n        \"\"\"\n        d_k = Q.shape[1]  # Get the dimension of the keys\n        scores = np.matmul(Q, K.T) / np.sqrt(d_k)  # Compute scaled dot-product attention scores\n        score_max = np.max(scores, axis=1, keepdims=True)  # Find the maximum score for numerical stability\n        attention_weights = np.exp(scores - score_max) / np.sum(np.exp(scores - score_max), axis=1, keepdims=True)  # Compute softmax to get attention weights\n        attention_output = np.matmul(attention_weights, V)  # Compute the final attention output\n        return attention_output\n\n    def multi_head_attention(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray, n_heads: int) -> np.ndarray:\n        \"\"\"\n        Compute multi-head attention.\n\n        Args:\n        Q, K, V: numpy arrays of shape (seq_len, d_model), Query, Key, and Value matrices\n        n_heads: int, number of attention heads\n\n        Returns:\n        attention_output: numpy array of shape (seq_len, d_model), final attention output\n        \"\"\"\n        d_model = Q.shape[1]  # Get the model dimension\n        assert d_model % n_heads == 0  # Ensure d_model is divisible by n_heads\n        d_k = d_model // n_heads  # Dimension for each head\n\n        # Reshape Q, K, V to separate heads\n        Q_reshaped = Q.reshape(Q.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n        K_reshaped = K.reshape(K.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n        V_reshaped = V.reshape(V.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n\n        # Compute attention scores for each head\n        attentions = []  # Store attention outputs for each head\n\n        for i in range(n_heads):\n            attn = self_attention(Q_reshaped[i], K_reshaped[i], V_reshaped[i])  # Compute attention for the i-th head\n            attentions.append(attn)  # Collect attention output\n\n        # Concatenate all head outputs\n        attention_output = np.concatenate(attentions, axis=-1)  # Concatenate along the last axis (columns)\n        return attention_output  # Return the final attention output",
    "test_cases": [
      {
        "test": [
          "m, n = 4, 4\nn_heads = 2\nnp.random.seed(42)\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\nW_q = np.random.randint(0,4,size=(n,n))\nW_k = np.random.randint(0,5,size=(n,n))\nW_v = np.random.randint(0,6,size=(n,n))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(multi_head_attention(Q, K, V, n_heads))"
        ],
        "expected_output": "np.array([[103, 109, 46, 99],\n                                [103, 109, 46, 99],\n                                [103, 109, 46, 99],\n                                [103, 109, 46, 99]])"
      },
      {
        "test": [
          "m, n = 6, 8\nn_heads = 4\nnp.random.seed(42)\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\nW_q = np.random.randint(0,4,size=(n,n))\nW_k = np.random.randint(0,5,size=(n,n))\nW_v = np.random.randint(0,6,size=(n,n))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(multi_head_attention(Q, K, V, n_heads))"
        ],
        "expected_output": [
          [
            500,
            463,
            399,
            495,
            377,
            450,
            531,
            362
          ],
          [
            500,
            463,
            399,
            495,
            377,
            450,
            531,
            362
          ],
          [
            500,
            463,
            399,
            495,
            377,
            450,
            531,
            362
          ],
          [
            500,
            463,
            399,
            495,
            377,
            450,
            531,
            362
          ],
          [
            500,
            463,
            399,
            495,
            377,
            450,
            531,
            362
          ],
          [
            500,
            463,
            399,
            495,
            377,
            450,
            531,
            362
          ]
        ]
      }
    ],
    "function_name": "compute_qkv"
  },
  {
    "id": 95,
    "title": "Calculate the Phi Coefficient",
    "difficulty": "easy",
    "category": "statistics",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBQaGkgY29lZmZpY2llbnQsIGEgbWVhc3VyZSBvZiB0aGUgY29ycmVsYXRpb24gYmV0d2VlbiB0d28gYmluYXJ5IHZhcmlhYmxlcy4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIHR3byBsaXN0cyBvZiBpbnRlZ2VycyAoMHMgYW5kIDFzKSBhcyBpbnB1dCBhbmQgcmV0dXJuIHRoZSBQaGkgY29lZmZpY2llbnQgcm91bmRlZCB0byA0IGRlY2ltYWwgcGxhY2VzLg==",
    "starter_code": "\n\nclass Solution:\n    def phi_corr(self, x: list[int], y: list[int]) -> float:\n    \t\"\"\"\n    \tCalculate the Phi coefficient between two binary variables.\n\n    \tArgs:\n    \tx (list[int]): A list of binary values (0 or 1).\n    \ty (list[int]): A list of binary values (0 or 1).\n\n    \tReturns:\n    \tfloat: The Phi coefficient rounded to 4 decimal places.\n    \t\"\"\"\n    \t# Your code here\n    \tpass\n    \treturn round(val,4)",
    "example_input": "phi_corr([1, 1, 0, 0], [0, 0, 1, 1])",
    "example_output": "-1.0",
    "example_reasoning": "The Phi coefficient measures the correlation between two binary variables. In this example, the variables have a perfect negative correlation, resulting in a Phi coefficient of -1.0.",
    "learn_content": "IyBUaGUgUGhpIGNvZWZmaWNpZW50CgpUaGUgUGhpIGNvZWZmaWNpZW50IGlzIGEgdHlwZSBvZiBjb3JyZWxhdGlvbiBjb2VmZmljaWVudCAsIHdoaWNoIGlzIHVzZWQgd2hlbiB3ZSBuZWVkIHRvIGZpbmQgdGhlIGNvcnJlbGF0aW9uIGJldHdlZW4gdHdvIGJpbmFyeSB2YXJpYWJsZXMuCkZvciBleGFtcGxlIHdoZW4gd2UgaGF2ZSB0d28gdmFyaWFibGVzIHggYW5kIHkgLCB4IGJlaW5nIGdlbmRlciBhbmQgeSBzaWduaWZ5aW5nIHRoZSBwcmVzZW5jZSBvZiBoZWFydCBkaXNlYXNlLiAKQm90aCB2YXJpYWJsZXMgYXJlIGJpbmFyeSBhbmQgaWYgd2UgbmVlZCB0byBmaW5kIGEgY29ycmVsYXRpb24gYmV0d2VlbiB4IGFuZCB5IHRoZW4gd2UgY2FuIHVzZSB0aGUgZm9ybXVsYSBiZWxvdyA6CgoKJFxwaGkgPSBcZnJhY3soeF97MDB9IFxjZG90IHhfezExfSkgLSAoeF97MDF9IFxjZG90IHhfezEwfSl9e1xzcXJ0eyh4X3swMH0gKyB4X3swMX0pKHhfezEwfSArIHhfezExfSkoeF97MDB9ICsgeF97MTB9KSh4X3swMX0gKyB4X3sxMX0pfX0kCgoKCgojIyMgRXhwbGFuYXRpb24gb2YgVGVybXM6Ci0gXCh4XzAwXCk6IFRoZSBudW1iZXIgb2YgY2FzZXMgd2hlcmUgXCh4ID0gMFwpIGFuZCBcKHkgPSAwXCkuCi0gXCh4XzAxXCk6IFRoZSBudW1iZXIgb2YgY2FzZXMgd2hlcmUgXCh4ID0gMFwpIGFuZCBcKHkgPSAxXCkuCi0gXCh4XzEwXCk6IFRoZSBudW1iZXIgb2YgY2FzZXMgd2hlcmUgXCh4ID0gMVwpIGFuZCBcKHkgPSAwXCkuCi0gXCh4XzExXCk6IFRoZSBudW1iZXIgb2YgY2FzZXMgd2hlcmUgXCh4ID0gMVwpIGFuZCBcKHkgPSAxXCku",
    "solution_code": "\n\nclass Solution:\n    def phi_corr(self, x: list[int], y: list[int]) -> float:\n        \"\"\"\n        Calculate the Phi coefficient between two binary variables.\n\n        Args:\n        x (list[int]): A list of binary values (0 or 1).\n        y (list[int]): A list of binary values (0 or 1).\n\n        Returns:\n        float: The Phi coefficient rounded to 4 decimal places.\n        \"\"\"\n        x1y1 = x1y0 = x0y1 = x0y0 = 0\n\n        # Count occurrences\n        for i in range(len(x)):\n            if x[i] == 1:\n                if y[i] == 1:\n                    x1y1 += 1\n                else:\n                    x1y0 += 1\n            elif x[i] == 0:\n                if y[i] == 1:\n                    x0y1 += 1\n                else:\n                    x0y0 += 1\n\n        # Calculate numerator and denominator\n        numerator = (x0y0 * x1y1) - (x0y1 * x1y0)\n        denominator = ((x0y0 + x0y1) * (x1y0 + x1y1) * (x0y0 + x1y0) * (x0y1 + x1y1)) ** 0.5\n\n        if denominator == 0:\n            return 0.0\n\n        phi = numerator / denominator\n        return round(phi, 4)",
    "test_cases": [
      {
        "test": [
          [
            1,
            1,
            0,
            0
          ],
          [
            0,
            0,
            1,
            1
          ]
        ],
        "expected_output": -1.0
      },
      {
        "test": [
          [
            1,
            1,
            0,
            0
          ],
          [
            1,
            0,
            1,
            1
          ]
        ],
        "expected_output": -0.5774
      }
    ],
    "function_name": "phi_corr"
  },
  {
    "id": 96,
    "title": "Implement the Hard Sigmoid Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBIYXJkIFNpZ21vaWQgYWN0aXZhdGlvbiBmdW5jdGlvbiwgYSBjb21wdXRhdGlvbmFsbHkgZWZmaWNpZW50IGFwcHJveGltYXRpb24gb2YgdGhlIHN0YW5kYXJkIHNpZ21vaWQgZnVuY3Rpb24uIFlvdXIgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBzaW5nbGUgaW5wdXQgdmFsdWUgYW5kIHJldHVybiB0aGUgY29ycmVzcG9uZGluZyBvdXRwdXQgYmFzZWQgb24gdGhlIEhhcmQgU2lnbW9pZCBkZWZpbml0aW9uLg==",
    "starter_code": "\n\nclass Solution:\n    def hard_sigmoid(self, x: float) -> float:\n    \t\"\"\"\n    \tImplements the Hard Sigmoid activation function.\n\n    \tArgs:\n    \t\tx (float): Input value\n\n    \tReturns:\n    \t\tfloat: The Hard Sigmoid of the input\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "hard_sigmoid(0.0)",
    "example_output": "0.5",
    "example_reasoning": "The input 0.0 falls in the linear region of the Hard Sigmoid function. Using the formula $HardSigmoid(x) = 0.2x + 0.5$, the output is $0.2 \\times 0.0 + 0.5 = 0.5$.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyB0aGUgSGFyZCBTaWdtb2lkIEFjdGl2YXRpb24gRnVuY3Rpb24KClRoZSBIYXJkIFNpZ21vaWQgaXMgYSBwaWVjZXdpc2UgbGluZWFyIGFwcHJveGltYXRpb24gb2YgdGhlIHNpZ21vaWQgYWN0aXZhdGlvbiBmdW5jdGlvbi4gSXQncyBjb21wdXRhdGlvbmFsbHkgbW9yZSBlZmZpY2llbnQgdGhhbiB0aGUgc3RhbmRhcmQgc2lnbW9pZCBmdW5jdGlvbiB3aGlsZSBtYWludGFpbmluZyBzaW1pbGFyIGNoYXJhY3RlcmlzdGljcy4gVGhpcyBmdW5jdGlvbiBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIGluIG5ldXJhbCBuZXR3b3JrcyB3aGVyZSBjb21wdXRhdGlvbmFsIGVmZmljaWVuY3kgaXMgY3J1Y2lhbC4KCiMjIyBNYXRoZW1hdGljYWwgRGVmaW5pdGlvbgoKVGhlIEhhcmQgU2lnbW9pZCBmdW5jdGlvbiBpcyBtYXRoZW1hdGljYWxseSBkZWZpbmVkIGFzOgoKJCQKSGFyZFNpZ21vaWQoeCkgPSBcYmVnaW57Y2FzZXN9IAowICYgXHRleHR7aWYgfSB4IFxsZXEgLTIuNSBcXCAKMC4yeCArIDAuNSAmIFx0ZXh0e2lmIH0gLTIuNSA8IHggPCAyLjUgXFwgCjEgJiBcdGV4dHtpZiB9IHggXGdlcSAyLjUgClxlbmR7Y2FzZXN9CiQkCgpXaGVyZSAkeCQgaXMgdGhlIGlucHV0IHRvIHRoZSBmdW5jdGlvbi4KCiMjIyBDaGFyYWN0ZXJpc3RpY3MKCi0gKipPdXRwdXQgUmFuZ2U6KiogVGhlIG91dHB1dCBpcyBhbHdheXMgYm91bmRlZCBpbiB0aGUgcmFuZ2UgJFswLCAxXSQKLSAqKlNoYXBlOioqIFRoZSBmdW5jdGlvbiBjb25zaXN0cyBvZiB0aHJlZSBwYXJ0czoKICAtIEEgY29uc3RhbnQgdmFsdWUgb2YgMCBmb3IgaW5wdXRzIDw9IC0yLjUKICAtIEEgbGluZWFyIHNlZ21lbnQgd2l0aCBzbG9wZSAwLjIgZm9yIGlucHV0cyBiZXR3ZWVuIC0yLjUgYW5kIDIuNQogIC0gQSBjb25zdGFudCB2YWx1ZSBvZiAxIGZvciBpbnB1dHMgPj0gMi41Ci0gKipHcmFkaWVudDoqKiBUaGUgZ3JhZGllbnQgaXMgMC4yIGluIHRoZSBsaW5lYXIgcmVnaW9uIGFuZCAwIGluIHRoZSBzYXR1cmF0ZWQgcmVnaW9ucwoKIyMjIEFkdmFudGFnZXMgaW4gTmV1cmFsIE5ldHdvcmtzCgpUaGlzIGZ1bmN0aW9uIGlzIHBhcnRpY3VsYXJseSB1c2VmdWwgaW4gbmV1cmFsIG5ldHdvcmtzIGFzIGl0IHByb3ZpZGVzOgotIENvbXB1dGF0aW9uYWwgZWZmaWNpZW5jeSBjb21wYXJlZCB0byBzdGFuZGFyZCBzaWdtb2lkCi0gQm91bmRlZCBvdXRwdXQgcmFuZ2Ugc2ltaWxhciB0byBzaWdtb2lkCi0gU2ltcGxlIGdyYWRpZW50IGNvbXB1dGF0aW9u",
    "solution_code": "\n\nclass Solution:\n    def hard_sigmoid(self, x: float) -> float:\n        \"\"\"\n        Implements the Hard Sigmoid activation function.\n\n        Args:\n            x (float): Input value\n\n        Returns:\n            float: The Hard Sigmoid of the input\n        \"\"\"\n        if x <= -2.5:\n            return 0.0\n        elif x >= 2.5:\n            return 1.0\n        else:\n            return 0.2 * x + 0.5",
    "test_cases": [
      {
        "test": [
          0.56
        ],
        "expected_output": 0.612
      },
      {
        "test": [
          3.0
        ],
        "expected_output": 1.0
      }
    ],
    "function_name": "hard_sigmoid"
  },
  {
    "id": 97,
    "title": "Implement the ELU Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBFTFUgKEV4cG9uZW50aWFsIExpbmVhciBVbml0KSBhY3RpdmF0aW9uIGZ1bmN0aW9uLCB3aGljaCBoZWxwcyBtaXRpZ2F0ZSB0aGUgbGltaXRhdGlvbnMgb2YgUmVMVSBieSBwcm92aWRpbmcgbmVnYXRpdmUgb3V0cHV0cyBmb3IgbmVnYXRpdmUgaW5wdXRzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIGNvbXB1dGUgdGhlIEVMVSBhY3RpdmF0aW9uIHZhbHVlIGZvciBhIGdpdmVuIGlucHV0Lg==",
    "starter_code": "\n\nclass Solution:\n    def elu(self, x: float, alpha: float = 1.0) -> float:\n    \t\"\"\"\n    \tCompute the ELU activation function.\n\n    \tArgs:\n    \t\tx (float): Input value\n    \t\talpha (float): ELU parameter for negative values (default: 1.0)\n\n    \tReturns:\n    \t\tfloat: ELU activation value\n    \t\"\"\"\n    \t# Your code here\n    \tpass\n    \treturn round(val,4)",
    "example_input": "elu(-1)",
    "example_output": "-0.6321",
    "example_reasoning": "For x = -1 and alpha = 1.0, the ELU activation is computed as $\\alpha(e^x - 1)$.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyB0aGUgRUxVIEFjdGl2YXRpb24gRnVuY3Rpb24KClRoZSBFTFUgKEV4cG9uZW50aWFsIExpbmVhciBVbml0KSBhY3RpdmF0aW9uIGZ1bmN0aW9uIGlzIGFuIGFkdmFuY2VkIGFjdGl2YXRpb24gZnVuY3Rpb24gdGhhdCBhZGRyZXNzZXMgc29tZSBsaW1pdGF0aW9ucyBvZiBSZUxVIGJ5IHByb3ZpZGluZyBuZWdhdGl2ZSB2YWx1ZXMgZm9yIG5lZ2F0aXZlIGlucHV0cywgd2hpY2ggY2FuIGhlbHAgcHJldmVudCB0aGUgImR5aW5nIFJlTFUiIHByb2JsZW0gYW5kIHNwZWVkIHVwIGxlYXJuaW5nLgoKIyMjIE1hdGhlbWF0aWNhbCBEZWZpbml0aW9uCgpUaGUgRUxVIGZ1bmN0aW9uIGlzIG1hdGhlbWF0aWNhbGx5IGRlZmluZWQgYXM6CgokJApFTFUoeCkgPSBcYmVnaW57Y2FzZXN9IAp4ICYgXHRleHR7aWYgfSB4ID4gMCBcXApcYWxwaGEoZV54IC0gMSkgJiBcdGV4dHtvdGhlcndpc2V9ClxlbmR7Y2FzZXN9CiQkCgpXaGVyZToKLSAkeCQgaXMgdGhlIGlucHV0IHRvIHRoZSBmdW5jdGlvbgotICRcYWxwaGEkIGlzIGEgaHlwZXJwYXJhbWV0ZXIgKHR5cGljYWxseSBzZXQgdG8gMS4wKSB0aGF0IGNvbnRyb2xzIHRoZSB2YWx1ZSB0byB3aGljaCBhbiBFTFUgc2F0dXJhdGVzIGZvciBuZWdhdGl2ZSBpbnB1dHMKLSAkZSQgaXMgdGhlIGJhc2Ugb2YgbmF0dXJhbCBsb2dhcml0aG1zIChFdWxlcidzIG51bWJlcikKCiMjIyBDaGFyYWN0ZXJpc3RpY3MKCi0gKipPdXRwdXQgUmFuZ2U6KiogVGhlIG91dHB1dCBpcyBpbiB0aGUgcmFuZ2UgJFstXGFscGhhLCBcaW5mdHkpJC4gRm9yIHBvc2l0aXZlIGlucHV0cywgaXQgYmVoYXZlcyBsaWtlIHRoZSBpZGVudGl0eSBmdW5jdGlvbiwgd2hpbGUgZm9yIG5lZ2F0aXZlIGlucHV0cywgaXQgaGFzIGEgc21vb3RoIGV4cG9uZW50aWFsIGN1cnZlIHRoYXQgYXBwcm9hY2hlcyAtYWxwaGEuCi0gKipTbW9vdGhuZXNzOioqIFVubGlrZSBSZUxVLCBFTFUgaXMgc21vb3RoIGV2ZXJ5d2hlcmUsIGluY2x1ZGluZyBhdCB4ID0gMCwgd2hpY2ggY2FuIGxlYWQgdG8gZmFzdGVyIGxlYXJuaW5nLgotICoqR3JhZGllbnQ6KiogVGhlIGdyYWRpZW50IGlzIDEgZm9yIHBvc2l0aXZlIHZhbHVlcyBhbmQgJFxhbHBoYSBlXngkIGZvciBuZWdhdGl2ZSB2YWx1ZXMsIHByb3ZpZGluZyBub24temVybyBncmFkaWVudHMgZm9yIG5lZ2F0aXZlIGlucHV0cy4KCiMjIyBBZHZhbnRhZ2VzCgoxLiBSZWR1Y2VzIHRoZSB2YW5pc2hpbmcgZ3JhZGllbnQgcHJvYmxlbQoyLiBDYW4gcHJvZHVjZSBuZWdhdGl2ZSBvdXRwdXRzLCBhbGxvd2luZyB0aGUgZnVuY3Rpb24gdG8gcHVzaCBtZWFuIHVuaXQgYWN0aXZhdGlvbnMgY2xvc2VyIHRvIHplcm8KMy4gU21vb3RoZXIgZ3JhZGllbnQgZmxvdyBjb21wYXJlZCB0byBSZUxVCjQuIEJldHRlciBoYW5kbGluZyBvZiBub2lzZSBpbiB0aGUgZGF0YSBkdWUgdG8gdGhlIGJvdW5kZWQgbmVnYXRpdmUgcGFydAoKIyMjIFZpc3VhbCBDb21wYXJpc29uIHdpdGggUmVMVQoKV2hpbGUgUmVMVSBzaW1wbHkgb3V0cHV0cyB6ZXJvIGZvciBhbGwgbmVnYXRpdmUgaW5wdXRzLCBFTFUgcHJvdmlkZXMgYSBzbW9vdGggbmVnYXRpdmUgcmVzcG9uc2U6CgotIEZvciB4ID4gMDogQm90aCBSZUxVIGFuZCBFTFUgb3V0cHV0IHgKLSBGb3IgeCA8PSAwOiAKICAtIFJlTFUgb3V0cHV0cyAwCiAgLSBFTFUgb3V0cHV0cyAkXGFscGhhKGVeeCAtIDEpJCwgd2hpY2ggc21vb3RobHkgYXBwcm9hY2hlcyAtYWxwaGEKCkVMVSBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIGluIGRlZXAgbmV1cmFsIG5ldHdvcmtzIHdoZXJlIHlvdSB3YW50IHRvIG1haW50YWluIHNvbWUgb2YgdGhlIGJlbmVmaXRzIG9mIFJlTFUgd2hpbGUgYWRkcmVzc2luZyBpdHMgbGltaXRhdGlvbnMgcmVnYXJkaW5nIG5lZ2F0aXZlIGlucHV0cy4=",
    "solution_code": "import math\n\nclass Solution:\n    def elu(self, x: float, alpha: float = 1.0) -> float:\n        \"\"\"\n        Compute the ELU activation function.\n\n        Args:\n            x (float): Input value\n            alpha (float): ELU parameter for negative values (default: 1.0)\n\n        Returns:\n            float: ELU activation value\n        \"\"\"\n        return round(x if x > 0 else alpha * (math.exp(x) - 1),4)",
    "test_cases": [
      {
        "test": [
          0
        ],
        "expected_output": 0.0
      },
      {
        "test": [
          1
        ],
        "expected_output": 1.0
      }
    ],
    "function_name": "elu"
  },
  {
    "id": 98,
    "title": "Implement the PReLU Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBQUmVMVSAoUGFyYW1ldHJpYyBSZUxVKSBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHZhcmlhbnQgb2YgdGhlIFJlTFUgYWN0aXZhdGlvbiBmdW5jdGlvbiB0aGF0IGludHJvZHVjZXMgYSBsZWFybmFibGUgcGFyYW1ldGVyIGZvciBuZWdhdGl2ZSBpbnB1dHMuIFlvdXIgdGFzayBpcyB0byBjb21wdXRlIHRoZSBQUmVMVSBhY3RpdmF0aW9uIHZhbHVlIGZvciBhIGdpdmVuIGlucHV0Lg==",
    "starter_code": "\n\nclass Solution:\n    def prelu(self, x: float, alpha: float = 0.25) -> float:\n    \t\"\"\"\n    \tImplements the PReLU (Parametric ReLU) activation function.\n\n    \tArgs:\n    \t\tx: Input value\n    \t\talpha: Slope parameter for negative values (default: 0.25)\n\n    \tReturns:\n    \t\tfloat: PReLU activation value\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "prelu(-2.0, alpha=0.25)",
    "example_output": "-0.5",
    "example_reasoning": "For x = -2.0 and alpha = 0.25, the PReLU activation is calculated as $PReLU(x) = \\alpha x = 0.25 \\times -2.0 = -0.5$.",
    "learn_content": "IyMjIFVuZGVyc3RhbmRpbmcgdGhlIFBSZUxVIChQYXJhbWV0cmljIFJlTFUpIEFjdGl2YXRpb24gRnVuY3Rpb24KClRoZSBQUmVMVSAoUGFyYW1ldHJpYyBSZWN0aWZpZWQgTGluZWFyIFVuaXQpIGlzIGFuIGFkdmFuY2VkIHZhcmlhbnQgb2YgdGhlIFJlTFUgYWN0aXZhdGlvbiBmdW5jdGlvbiB0aGF0IGludHJvZHVjZXMgYSBsZWFybmFibGUgcGFyYW1ldGVyIGZvciBuZWdhdGl2ZSBpbnB1dHMuIFRoaXMgbWFrZXMgaXQgbW9yZSBmbGV4aWJsZSB0aGFuIHN0YW5kYXJkIFJlTFUgYW5kIGhlbHBzIHByZXZlbnQgdGhlICJkeWluZyBSZUxVIiBwcm9ibGVtLgoKIyMjIyBNYXRoZW1hdGljYWwgRGVmaW5pdGlvbgoKVGhlIFBSZUxVIGZ1bmN0aW9uIGlzIGRlZmluZWQgYXM6CgokJApQUmVMVSh4KSA9IFxiZWdpbntjYXNlc30KeCAmIFx0ZXh0e2lmIH0geCA+IDAgXFwKXGFscGhhIHggJiBcdGV4dHtvdGhlcndpc2V9ClxlbmR7Y2FzZXN9CiQkCgpXaGVyZToKLSAkeCQgaXMgdGhlIGlucHV0IHZhbHVlCi0gJFxhbHBoYSQgaXMgYSBsZWFybmFibGUgcGFyYW1ldGVyICh0eXBpY2FsbHkgaW5pdGlhbGl6ZWQgdG8gYSBzbWFsbCB2YWx1ZSBsaWtlIDAuMjUpCgojIyMjIEtleSBDaGFyYWN0ZXJpc3RpY3MKCjEuICoqQWRhcHRpdmUgU2xvcGUqKjogVW5saWtlIFJlTFUgd2hpY2ggaGFzIGEgemVybyBzbG9wZSBmb3IgbmVnYXRpdmUgaW5wdXRzLCBQUmVMVSBsZWFybnMgdGhlIG9wdGltYWwgbmVnYXRpdmUgc2xvcGUgcGFyYW1ldGVyICgkXGFscGhhJCkgZHVyaW5nIHRyYWluaW5nLgoKMi4gKipPdXRwdXQgUmFuZ2UqKjogCiAgIC0gRm9yICR4ID4gMCQ6IE91dHB1dCBlcXVhbHMgaW5wdXQgKCR5ID0geCQpCiAgIC0gRm9yICR4IFxsZXEgMCQ6IE91dHB1dCBpcyBzY2FsZWQgYnkgJFxhbHBoYSQgKCR5ID0gXGFscGhhIHgkKQoKMy4gKipBZHZhbnRhZ2VzKio6CiAgIC0gSGVscHMgcHJldmVudCB0aGUgImR5aW5nIFJlTFUiIHByb2JsZW0KICAgLSBNb3JlIGZsZXhpYmxlIHRoYW4gc3RhbmRhcmQgUmVMVQogICAtIENhbiBpbXByb3ZlIG1vZGVsIHBlcmZvcm1hbmNlIHRocm91Z2ggbGVhcm5lZCBwYXJhbWV0ZXIKICAgLSBNYWludGFpbnMgdGhlIGNvbXB1dGF0aW9uYWwgZWZmaWNpZW5jeSBvZiBSZUxVCgo0LiAqKlNwZWNpYWwgQ2FzZXMqKjoKICAgLSBXaGVuICRcYWxwaGEgPSAwJCwgUFJlTFUgYmVjb21lcyBSZUxVCiAgIC0gV2hlbiAkXGFscGhhID0gMSQsIFBSZUxVIGJlY29tZXMgYSBsaW5lYXIgZnVuY3Rpb24KICAgLSBXaGVuICRcYWxwaGEkIGlzIHNtYWxsIChlLmcuLCAwLjAxKSwgUFJlTFUgYmVoYXZlcyBzaW1pbGFybHkgdG8gTGVha3kgUmVMVQoKUFJlTFUgaXMgcGFydGljdWxhcmx5IHVzZWZ1bCBpbiBkZWVwIG5ldXJhbCBuZXR3b3JrcyB3aGVyZSB0aGUgb3B0aW1hbCBuZWdhdGl2ZSBzbG9wZSBtaWdodCB2YXJ5IGFjcm9zcyBkaWZmZXJlbnQgbGF5ZXJzIG9yIGNoYW5uZWxzLg==",
    "solution_code": "\n\nclass Solution:\n    def prelu(self, x: float, alpha: float = 0.25) -> float:\n        \"\"\"\n        Implements the PReLU (Parametric ReLU) activation function.\n\n        Args:\n            x: Input value\n            alpha: Slope parameter for negative values (default: 0.25)\n\n        Returns:\n            float: PReLU activation value\n        \"\"\"\n        return x if x > 0 else alpha * x",
    "test_cases": [
      {
        "test": [
          2.0
        ],
        "expected_output": 2.0
      },
      {
        "test": [
          0.0
        ],
        "expected_output": 0.0
      }
    ],
    "function_name": "prelu"
  },
  {
    "id": 99,
    "title": "Implement the Softplus Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBTb2Z0cGx1cyBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHNtb290aCBhcHByb3hpbWF0aW9uIG9mIHRoZSBSZUxVIGZ1bmN0aW9uLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgU29mdHBsdXMgdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQsIGhhbmRsaW5nIGVkZ2UgY2FzZXMgdG8gcHJldmVudCBudW1lcmljYWwgb3ZlcmZsb3cgb3IgdW5kZXJmbG93Lg==",
    "starter_code": "\n\nclass Solution:\n    def softplus(self, x: float) -> float:\n    \t\"\"\"\n    \tCompute the softplus activation function.\n\n    \tArgs:\n    \t\tx: Input value\n\n    \tReturns:\n    \t\tThe softplus value: log(1 + e^x)\n    \t\"\"\"\n    \t# Your code here\n    \tpass\n    \t return round(val,4)",
    "example_input": "softplus(2)",
    "example_output": "2.1269",
    "example_reasoning": "For x = 2, the Softplus activation is calculated as $\\log(1 + e^x)$.",
    "learn_content": "IyMjIFVuZGVyc3RhbmRpbmcgdGhlIFNvZnRwbHVzIEFjdGl2YXRpb24gRnVuY3Rpb24KClRoZSBTb2Z0cGx1cyBhY3RpdmF0aW9uIGZ1bmN0aW9uIGlzIGEgc21vb3RoIGFwcHJveGltYXRpb24gb2YgdGhlIFJlTFUgZnVuY3Rpb24uIEl0J3MgdXNlZCBpbiBuZXVyYWwgbmV0d29ya3Mgd2hlcmUgYSBzbW9vdGhlciB0cmFuc2l0aW9uIGFyb3VuZCB6ZXJvIGlzIGRlc2lyZWQuIFVubGlrZSBSZUxVIHdoaWNoIGhhcyBhIHNoYXJwIHRyYW5zaXRpb24gYXQgeD0wLCBTb2Z0cGx1cyBwcm92aWRlcyBhIG1vcmUgZ3JhZHVhbCBjaGFuZ2UuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KClRoZSBTb2Z0cGx1cyBmdW5jdGlvbiBpcyBtYXRoZW1hdGljYWxseSBkZWZpbmVkIGFzOgoKJCQKU29mdHBsdXMoeCkgPSBcbG9nKDEgKyBlXngpCiQkCgpXaGVyZToKLSAkeCQgaXMgdGhlIGlucHV0IHRvIHRoZSBmdW5jdGlvbgotICRlJCBpcyBFdWxlcidzIG51bWJlciAoYXBwcm94aW1hdGVseSAyLjcxODI4KQotICRcbG9nJCBpcyB0aGUgbmF0dXJhbCBsb2dhcml0aG0KCiMjIyBDaGFyYWN0ZXJpc3RpY3MKCjEuICoqT3V0cHV0IFJhbmdlKio6IAogICAtIFRoZSBvdXRwdXQgaXMgYWx3YXlzIHBvc2l0aXZlOiAkKDAsIFxpbmZ0eSkkCiAgIC0gVW5saWtlIFJlTFUsIFNvZnRwbHVzIG5ldmVyIG91dHB1dHMgZXhhY3RseSB6ZXJvCgoyLiAqKlNtb290aG5lc3MqKjoKICAgLSBTb2Z0cGx1cyBpcyBjb250aW51b3VzbHkgZGlmZmVyZW50aWFibGUKICAgLSBUaGUgdHJhbnNpdGlvbiBhcm91bmQgeD0wIGlzIHNtb290aCwgdW5saWtlIFJlTFUncyBzaGFycCAiZWxib3ciCgozLiAqKlJlbGF0aW9uc2hpcCB0byBSZUxVKio6CiAgIC0gU29mdHBsdXMgY2FuIGJlIHNlZW4gYXMgYSBzbW9vdGggYXBwcm94aW1hdGlvbiBvZiBSZUxVCiAgIC0gQXMgeCBiZWNvbWVzIHZlcnkgbmVnYXRpdmUsIFNvZnRwbHVzIGFwcHJvYWNoZXMgMAogICAtIEFzIHggYmVjb21lcyB2ZXJ5IHBvc2l0aXZlLCBTb2Z0cGx1cyBhcHByb2FjaGVzIHgKCjQuICoqRGVyaXZhdGl2ZSoqOgogICAtIFRoZSBkZXJpdmF0aXZlIG9mIFNvZnRwbHVzIGlzIHRoZSBsb2dpc3RpYyBzaWdtb2lkIGZ1bmN0aW9uOgogICAkJAogICBcZnJhY3tkfXtkeH1Tb2Z0cGx1cyh4KSA9IFxmcmFjezF9ezEgKyBlXnsteH19CiAgICQkCgojIyMgVXNlIENhc2VzCi0gV2hlbiBzbW9vdGggZ3JhZGllbnRzIGFyZSBpbXBvcnRhbnQgZm9yIG9wdGltaXphdGlvbgotIEluIG5ldXJhbCBuZXR3b3JrcyB3aGVyZSBhIGNvbnRpbnVvdXMgYXBwcm94aW1hdGlvbiBvZiBSZUxVIGlzIG5lZWRlZAotIFNpdHVhdGlvbnMgd2hlcmUgc3RyaWN0bHkgcG9zaXRpdmUgb3V0cHV0cyBhcmUgcmVxdWlyZWQgd2l0aCBzbW9vdGggdHJhbnNpdGlvbnM=",
    "solution_code": "import math\n\nclass Solution:\n    def softplus(self, x: float) -> float:\n        \"\"\"\n        Compute the softplus activation function.\n\n        Args:\n            x: Input value\n\n        Returns:\n            The softplus value: log(1 + e^x)\n        \"\"\"\n        # To prevent overflow for large positive values\n        if x > 100:\n            return x\n        # To prevent underflow for large negative values\n        if x < -100:\n            return 0.0\n\n        return round (math.log(1.0 + math.exp(x)),4)",
    "test_cases": [
      {
        "test": [
          0
        ],
        "expected_output": 0.6931
      },
      {
        "test": [
          100
        ],
        "expected_output": 100.0
      }
    ],
    "function_name": "softplus"
  },
  {
    "id": 100,
    "title": "Implement the Softsign Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBTb2Z0c2lnbiBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHNtb290aCBhY3RpdmF0aW9uIGZ1bmN0aW9uIHVzZWQgaW4gbmV1cmFsIG5ldHdvcmtzLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgU29mdHNpZ24gdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQsIGVuc3VyaW5nIHRoZSBvdXRwdXQgaXMgYm91bmRlZCBiZXR3ZWVuIC0xIGFuZCAxLg==",
    "starter_code": "\n\nclass Solution:\n    def softsign(self, x: float) -> float:\n    \t\"\"\"\n    \tImplements the Softsign activation function.\n\n    \tArgs:\n    \t\tx (float): Input value\n\n    \tReturns:\n    \t\tfloat: The Softsign of the input\t\"\"\"\n    \t# Your code here\n    \tpass\n    \treturn round(val,4)",
    "example_input": "softsign(1)",
    "example_output": "0.5",
    "example_reasoning": "For x = 1, the Softsign activation is calculated as $\\frac{x}{1 + |x|}$.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyB0aGUgU29mdHNpZ24gQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIFNvZnRzaWduIGFjdGl2YXRpb24gZnVuY3Rpb24gaXMgYSBzbW9vdGgsIG5vbi1saW5lYXIgYWN0aXZhdGlvbiBmdW5jdGlvbiB1c2VkIGluIG5ldXJhbCBuZXR3b3Jrcy4gSXTigJlzIHNpbWlsYXIgdG8gdGhlIGh5cGVyYm9saWMgdGFuZ2VudCAodGFuaCkgZnVuY3Rpb24gYnV0IHdpdGggZGlmZmVyZW50IHByb3BlcnRpZXMsIHBhcnRpY3VsYXJseSBpbiBpdHMgdGFpbHMgd2hpY2ggYXBwcm9hY2ggdGhlaXIgbGltaXRzIG1vcmUgc2xvd2x5LgoKIyMjIE1hdGhlbWF0aWNhbCBEZWZpbml0aW9uCgpUaGUgU29mdHNpZ24gZnVuY3Rpb24gaXMgbWF0aGVtYXRpY2FsbHkgZGVmaW5lZCBhczoKCiQkClNvZnRzaWduKHgpID0gXGZyYWN7eH17MSArIHx4fH0KJCQKCldoZXJlOgotICR4JCBpcyB0aGUgaW5wdXQgdG8gdGhlIGZ1bmN0aW9uCi0gJHx4fCQgcmVwcmVzZW50cyB0aGUgYWJzb2x1dGUgdmFsdWUgb2YgJHgkCgojIyMgQ2hhcmFjdGVyaXN0aWNzCgotICoqT3V0cHV0IFJhbmdlOioqIFRoZSBvdXRwdXQgaXMgYm91bmRlZCBiZXR3ZWVuIC0xIGFuZCAxLCBhcHByb2FjaGluZyB0aGVzZSB2YWx1ZXMgYXN5bXB0b3RpY2FsbHkgYXMgJHgkIGFwcHJvYWNoZXMgJFxwbSBcaW5mdHkkLgotICoqU2hhcGU6KiogVGhlIGZ1bmN0aW9uIGhhcyBhbiBTLXNoYXBlZCBjdXJ2ZSwgc2ltaWxhciB0byB0YW5oIGJ1dCB3aXRoIGEgc21vb3RoZXIgYXBwcm9hY2ggdG8gaXRzIGFzeW1wdG90ZXMuCi0gKipHcmFkaWVudDoqKiBUaGUgZ3JhZGllbnQgaXMgc21vb3RoZXIgYW5kIG1vcmUgZ3JhZHVhbCBjb21wYXJlZCB0byB0YW5oLCB3aGljaCBjYW4gaGVscCBwcmV2ZW50IHZhbmlzaGluZyBncmFkaWVudCBwcm9ibGVtcyBpbiBkZWVwIG5ldHdvcmtzLgotICoqU3ltbWV0cnk6KiogVGhlIGZ1bmN0aW9uIGlzIHN5bW1ldHJpYyBhcm91bmQgdGhlIG9yaWdpbiAkKDAsMCkkLgoKIyMjIEtleSBQcm9wZXJ0aWVzCgotICoqQm91bmRlZCBPdXRwdXQ6KiogVW5saWtlIFJlTFUsIFNvZnRzaWduIG5hdHVyYWxseSBib3VuZHMgaXRzIG91dHB1dCBiZXR3ZWVuIC0xIGFuZCAxLgotICoqU21vb3RobmVzczoqKiBUaGUgZnVuY3Rpb24gaXMgY29udGludW91cyBhbmQgZGlmZmVyZW50aWFibGUgZXZlcnl3aGVyZS4KLSAqKk5vIFNhdHVyYXRpb246KiogVGhlIGdyYWRpZW50cyBhcHByb2FjaCB6ZXJvIG1vcmUgc2xvd2x5IHRoYW4gaW4gdGFuaCBvciBzaWdtb2lkIGZ1bmN0aW9ucy4KLSAqKlplcm8tQ2VudGVyZWQ6KiogVGhlIGZ1bmN0aW9uIGNyb3NzZXMgdGhyb3VnaCB0aGUgb3JpZ2luLCBtYWtpbmcgaXQgbmF0dXJhbGx5IHplcm8tY2VudGVyZWQuCgpUaGlzIGFjdGl2YXRpb24gZnVuY3Rpb24gY2FuIGJlIHBhcnRpY3VsYXJseSB1c2VmdWwgaW4gc2NlbmFyaW9zIHdoZXJlIHlvdSBuZWVkIGJvdW5kZWQgb3V0cHV0cyB3aXRoIG1vcmUgZ3JhZHVhbCBzYXR1cmF0aW9uIGNvbXBhcmVkIHRvIHRhbmggb3Igc2lnbW9pZCBmdW5jdGlvbnMuCg==",
    "solution_code": "\n\nclass Solution:\n    def softsign(self, x: float) -> float:\n        \"\"\"\n        Implements the Softsign activation function.\n\n        Args:\n            x (float): Input value\n\n        Returns:\n            float: The Softsign of the input, calculated as x/(1 + |x|)\n        \"\"\"\n        return round(x / (1 + abs(x)), 4)",
    "test_cases": [
      {
        "test": [
          0
        ],
        "expected_output": 0.0
      },
      {
        "test": [
          1
        ],
        "expected_output": 0.5
      }
    ],
    "function_name": "softsign"
  },
  {
    "id": 101,
    "title": "Implement the GRPO Objective Function",
    "difficulty": "hard",
    "category": "reinforcement learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBHUlBPIChHZW5lcmFsaXplZCBSZWxhdGl2ZSBQb2xpY3kgT3B0aW1pemF0aW9uKSBvYmplY3RpdmUgZnVuY3Rpb24gdXNlZCB0byBvcHRpbWl6ZSBwb2xpY3kgcGFyYW1ldGVycyBpbiByZWluZm9yY2VtZW50IGxlYXJuaW5nLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgR1JQTyBvYmplY3RpdmUgZ2l2ZW4gdGhlIGxpa2VsaWhvb2QgcmF0aW9zLCBhZHZhbnRhZ2UgZXN0aW1hdGVzLCBvbGQgcG9saWN5IHByb2JhYmlsaXRpZXMsIHJlZmVyZW5jZSBwb2xpY3kgcHJvYmFiaWxpdGllcywgYW5kIGFwcGx5IHRoZSBjbGlwcGluZyBtZWNoYW5pc20gYW5kIEtMIGRpdmVyZ2VuY2UgcGVuYWx0eSBjb3JyZWN0bHkgdG8gbWFpbnRhaW4gdHJhaW5pbmcgc3RhYmlsaXR5Lg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def grpo_objective(self, rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \t\"\"\"\n    \tCompute the GRPO objective function.\n\n    \tArgs:\n    \t\trhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n    \t\tA: List of advantage estimates (A_i).\n    \t\tpi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n    \t\tpi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n    \t\tepsilon: Clipping parameter (eps).\n    \t\tbeta: KL divergence penalty coefficient (beta).\n\n    \tReturns:\n    \t\tThe computed GRPO objective value.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)",
    "example_output": "1.032749",
    "example_reasoning": "The function calculates the GRPO objective by first clipping the likelihood ratios, computing the minimum terms, averaging them, and then subtracting the KL divergence penalty scaled by beta.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def grpo_objective(self, rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n        \"\"\"\n        Compute the GRPO objective function.\n\n        Args:\n            rhos: List of likelihood ratios (\u03c1_i) = \u03c0_theta(o_i | q) / \u03c0_theta_old(o_i | q).\n            A: List of advantage estimates (A_i).\n            pi_theta_old: List representing the old policy probabilities \u03c0_theta_old(o_i | q).\n            pi_theta_ref: List representing the reference policy probabilities \u03c0_ref(o_i | q).\n            epsilon: Clipping parameter (\u03f5).\n            beta: KL divergence penalty coefficient (\u03b2).\n\n        Returns:\n            The computed GRPO objective value.\n        \"\"\"\n        G = len(rhos)\n        if not (len(A) == len(pi_theta_old) == len(pi_theta_ref) == G):\n            raise ValueError(\"All input lists must have the same length.\")\n\n        # Compute clipped likelihood ratios\n        clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n\n        # Compute the minimum terms for the objective\n        unclipped = np.array(rhos) * np.array(A)\n        clipped = clipped_rhos * np.array(A)\n        min_terms = np.minimum(unclipped, clipped)\n        average_min = np.mean(min_terms)\n\n        # Compute pi_theta from rhos and pi_theta_old\n        pi_theta = np.array(rhos) * np.array(pi_theta_old)\n\n        # Normalize pi_theta and pi_theta_ref to ensure they are valid probability distributions\n        pi_theta /= np.sum(pi_theta)\n        pi_theta_ref /= np.sum(pi_theta_ref)\n\n        # Compute KL divergence D_KL(pi_theta || pi_theta_ref)\n        kl_divergence = np.sum(pi_theta * np.log(pi_theta / pi_theta_ref + 1e-10))  # Added epsilon to avoid log(0)\n\n        # Compute the final objective\n        objective = average_min - beta * kl_divergence\n\n        return objective",
    "test_cases": [
      {
        "test": [
          "print(round(grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01),6))"
        ],
        "expected_output": 1.032749
      },
      {
        "test": [
          "print(round(grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05),6))"
        ],
        "expected_output": 0.999743
      }
    ],
    "function_name": "grpo_objective"
  },
  {
    "id": 102,
    "title": "Implement the Swish Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBTd2lzaCBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHNlbGYtZ2F0ZWQgYWN0aXZhdGlvbiBmdW5jdGlvbiB0aGF0IGhhcyBzaG93biBzdXBlcmlvciBwZXJmb3JtYW5jZSBpbiBkZWVwIG5ldXJhbCBuZXR3b3JrcyBjb21wYXJlZCB0byBSZUxVLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgU3dpc2ggdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQu",
    "starter_code": "\n\nclass Solution:\n    def swish(self, x: float) -> float:\n    \t\"\"\"\n    \tImplements the Swish activation function.\n\n    \tArgs:\n    \t\tx: Input value\n\n    \tReturns:\n    \t\tThe Swish activation value\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "swish(1)",
    "example_output": "0.7311",
    "example_reasoning": "For x = 1, the Swish activation is calculated as $Swish(x) = x \\times \\sigma(x)$, where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. Substituting the value, $Swish(1) = 1 \\times \\frac{1}{1 + e^{-1}} = 0.7311$.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyB0aGUgU3dpc2ggQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIFN3aXNoIGFjdGl2YXRpb24gZnVuY3Rpb24gaXMgYSBtb2Rlcm4gc2VsZi1nYXRlZCBhY3RpdmF0aW9uIGZ1bmN0aW9uIGludHJvZHVjZWQgYnkgcmVzZWFyY2hlcnMgYXQgR29vZ2xlIEJyYWluLiBJdCBoYXMgYmVlbiBzaG93biB0byBwZXJmb3JtIGJldHRlciB0aGFuIFJlTFUgaW4gbWFueSBkZWVwIG5ldHdvcmtzLCBwYXJ0aWN1bGFybHkgaW4gZGVlcGVyIGFyY2hpdGVjdHVyZXMuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KClRoZSBTd2lzaCBmdW5jdGlvbiBpcyBkZWZpbmVkIGFzOgoKJCRTd2lzaCh4KSA9IHggXHRpbWVzIFxzaWdtYSh4KSQkCgp3aGVyZSAkXHNpZ21hKHgpJCBpcyB0aGUgc2lnbW9pZCBmdW5jdGlvbiBkZWZpbmVkIGFzOgoKJCRcc2lnbWEoeCkgPSBcZnJhY3sxfXsxICsgZV57LXh9fSQkCgojIyMgQ2hhcmFjdGVyaXN0aWNzCgotICoqT3V0cHV0IFJhbmdlKio6IFVubGlrZSBSZUxVIHdoaWNoIGhhcyBhIHJhbmdlIG9mICRbMCwgXGluZnR5KSQsIFN3aXNoIGhhcyBhIHJhbmdlIG9mICQoLVxpbmZ0eSwgXGluZnR5KSQKLSAqKlNtb290aG5lc3MqKjogU3dpc2ggaXMgc21vb3RoIGFuZCBub24tbW9ub3RvbmljLCBtYWtpbmcgaXQgZGlmZmVyZW50aWFibGUgZXZlcnl3aGVyZQotICoqU2hhcGUqKjogVGhlIGZ1bmN0aW9uIGhhcyBhIHNsaWdodCBkaXAgYmVsb3cgMCBmb3IgbmVnYXRpdmUgdmFsdWVzLCB0aGVuIGN1cnZlcyB1cCBzbW9vdGhseSBmb3IgcG9zaXRpdmUgdmFsdWVzCi0gKipQcm9wZXJ0aWVzKio6CiAgLSBGb3IgbGFyZ2UgcG9zaXRpdmUgeDogU3dpc2goeCkgfiB4IChzaW1pbGFyIHRvIGxpbmVhciBmdW5jdGlvbikKICAtIEZvciBsYXJnZSBuZWdhdGl2ZSB4OiBTd2lzaCh4KSB+IDAgKHNpbWlsYXIgdG8gUmVMVSkKICAtIEhhcyBhIG1pbmltYWwgdmFsdWUgYXJvdW5kIHggfiAtMS4yOAoKIyMjIEFkdmFudGFnZXMKCi0gU21vb3RoIGZ1bmN0aW9uIHdpdGggbm8gaGFyZCB6ZXJvIHRocmVzaG9sZCBsaWtlIFJlTFUKLSBTZWxmLWdhdGVkIG5hdHVyZSBhbGxvd3MgZm9yIG1vcmUgY29tcGxleCByZWxhdGlvbnNoaXBzCi0gT2Z0ZW4gcHJvdmlkZXMgYmV0dGVyIHBlcmZvcm1hbmNlIGluIGRlZXAgbmV1cmFsIG5ldHdvcmtzCi0gUmVkdWNlcyB0aGUgdmFuaXNoaW5nIGdyYWRpZW50IHByb2JsZW0gY29tcGFyZWQgdG8gc2lnbW9pZA==",
    "solution_code": "import math\n\nclass Solution:\n    def swish(self, x: float) -> float:\n        \"\"\"\n        Implements the Swish activation function.\n\n        Args:\n            x: Input value\n\n        Returns:\n            The Swish activation value\n        \"\"\"\n        return x * (1 / (1 + math.exp(-x)))",
    "test_cases": [
      {
        "test": [
          "print(round(swish(0),4))"
        ],
        "expected_output": 0.0
      },
      {
        "test": [
          "print(round(swish(1),4))"
        ],
        "expected_output": 0.7311
      }
    ],
    "function_name": "swish"
  },
  {
    "id": 103,
    "title": "Implement the SELU Activation Function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBTRUxVIChTY2FsZWQgRXhwb25lbnRpYWwgTGluZWFyIFVuaXQpIGFjdGl2YXRpb24gZnVuY3Rpb24sIGEgc2VsZi1ub3JtYWxpemluZyB2YXJpYW50IG9mIEVMVS4gWW91ciB0YXNrIGlzIHRvIGNvbXB1dGUgdGhlIFNFTFUgdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQgd2hpbGUgZW5zdXJpbmcgbnVtZXJpY2FsIHN0YWJpbGl0eS4=",
    "starter_code": "\n\nclass Solution:\n    def selu(self, x: float) -> float:\n    \t\"\"\"\n    \tImplements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    \tArgs:\n    \t\tx: Input value\n\n    \tReturns:\n    \t\tSELU activation value\n    \t\"\"\"\n    \talpha = 1.6732632423543772\n    \tscale = 1.0507009873554804\n    \t# Your code here\n    \tpass",
    "example_input": "selu(-1.0)",
    "example_output": "-1.1113",
    "example_reasoning": "For x = -1.0, the SELU activation is calculated using the formula $SELU(x) = \\lambda \\alpha (e^x - 1)$. Substituting the values of $\\lambda$ and $\\alpha$, we get $SELU(-1.0) = 1.0507 \\times 1.6733 \\times (e^{-1.0} - 1) = -1.1113$.",
    "learn_content": "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",
    "solution_code": "import math\n\nclass Solution:\n    def selu(self, x: float) -> float:\n        \"\"\"\n        Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n        Args:\n            x: Input value\n\n        Returns:\n            SELU activation value\n        \"\"\"\n        alpha = 1.6732632423543772\n        scale = 1.0507009873554804\n        return round(scale * x if x > 0 else scale * alpha * (math.exp(x) - 1), 4)",
    "test_cases": [
      {
        "test": [
          "print(round(selu(1.0), 4))"
        ],
        "expected_output": 1.0507
      },
      {
        "test": [
          "print(round(selu(0.0), 4))"
        ],
        "expected_output": 0.0
      }
    ],
    "function_name": "selu"
  },
  {
    "id": 104,
    "title": "Binary Classification with Logistic Regression",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBwcmVkaWN0aW9uIGZ1bmN0aW9uIGZvciBiaW5hcnkgY2xhc3NpZmljYXRpb24gdXNpbmcgTG9naXN0aWMgUmVncmVzc2lvbi4gWW91ciB0YXNrIGlzIHRvIGNvbXB1dGUgY2xhc3MgcHJvYmFiaWxpdGllcyB1c2luZyB0aGUgc2lnbW9pZCBmdW5jdGlvbiBhbmQgcmV0dXJuIGJpbmFyeSBwcmVkaWN0aW9ucyBiYXNlZCBvbiBhIHRocmVzaG9sZCBvZiAwLjUu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def predict_logistic(self, X: np.ndarray, weights: np.ndarray, bias: float) -> np.ndarray:\n    \t\"\"\"\n    \tImplements binary classification prediction using Logistic Regression.\n\n    \tArgs:\n    \t\tX: Input feature matrix (shape: N x D)\n    \t\tweights: Model weights (shape: D)\n    \t\tbias: Model bias\n\n    \tReturns:\n    \t\tBinary predictions (0 or 1)\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)",
    "example_output": "[1 1 0 0]",
    "example_reasoning": "Each sample's linear combination is computed using $z = Xw + b$. The sigmoid function is applied, and the output is thresholded at 0.5, resulting in binary predictions.",
    "learn_content": "IyMgQmluYXJ5IENsYXNzaWZpY2F0aW9uIHdpdGggTG9naXN0aWMgUmVncmVzc2lvbgoKTG9naXN0aWMgUmVncmVzc2lvbiBpcyBhIGZ1bmRhbWVudGFsIGFsZ29yaXRobSBmb3IgYmluYXJ5IGNsYXNzaWZpY2F0aW9uLiBHaXZlbiBpbnB1dCBmZWF0dXJlcyBhbmQgbGVhcm5lZCBtb2RlbCBwYXJhbWV0ZXJzICh3ZWlnaHRzIGFuZCBiaWFzKSwgeW91ciB0YXNrIGlzIHRvIGltcGxlbWVudCB0aGUgcHJlZGljdGlvbiBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIGNsYXNzIHByb2JhYmlsaXRpZXMuCgojIyMgTWF0aGVtYXRpY2FsIEJhY2tncm91bmQKClRoZSBsb2dpc3RpYyByZWdyZXNzaW9uIG1vZGVsIG1ha2VzIHByZWRpY3Rpb25zIHVzaW5nIHRoZSBzaWdtb2lkIGZ1bmN0aW9uOgoKJCRcc2lnbWEoeikgPSBcZnJhY3sxfXsxICsgZV57LXp9fSQkCgp3aGVyZSB6IGlzIHRoZSBsaW5lYXIgY29tYmluYXRpb24gb2YgZmVhdHVyZXMgYW5kIHdlaWdodHMgcGx1cyBiaWFzOgoKJCR6ID0gXG1hdGhiZnt3fV5UXG1hdGhiZnt4fSArIGIgPSBcc3VtX3tpPTF9XntufSB3X2l4X2kgKyBiJCQKCiMjIyBJbXBsZW1lbnRhdGlvbiBSZXF1aXJlbWVudHMKCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0aGF0OgoKLSBUYWtlcyBhIGJhdGNoIG9mIHNhbXBsZXMgJFxtYXRoYmZ7WH0kIChzaGFwZTogTiB4IEQpLCB3ZWlnaHRzICRcbWF0aGJme3d9JCAoc2hhcGU6IEQpLCBhbmQgYmlhcyBiCi0gQ29tcHV0ZXMgJHogPSBcbWF0aGJme1h9XG1hdGhiZnt3fSArIGIkIGZvciBhbGwgc2FtcGxlcwotIEFwcGxpZXMgdGhlIHNpZ21vaWQgZnVuY3Rpb24gdG8gZ2V0IHByb2JhYmlsaXRpZXMKLSBSZXR1cm5zIGJpbmFyeSBwcmVkaWN0aW9ucyBpLmUuLCAwIG9yIDEgdXNpbmcgYSB0aHJlc2hvbGQgb2YgMC41CgojIyMgSW1wb3J0YW50IENvbnNpZGVyYXRpb25zCgotIEhhbmRsZSBudW1lcmljYWwgc3RhYmlsaXR5IGluIHNpZ21vaWQgY29tcHV0YXRpb24KLSBFbnN1cmUgZWZmaWNpZW50IHZlY3Rvcml6ZWQgb3BlcmF0aW9ucyB1c2luZyBudW1weQotIFJldHVybiBiaW5hcnkgcHJlZGljdGlvbnMgKDAgb3IgMSkKCiMjIyBIaW50CgpUbyBwcmV2ZW50IG92ZXJmbG93IGluIHRoZSBleHBvbmVudGlhbCBjYWxjdWxhdGlvbiBvZiB0aGUgc2lnbW9pZCBmdW5jdGlvbiwgdXNlIGBucC5jbGlwYCB0byBsaW1pdCB6IHZhbHVlczoKCmBgYHB5dGhvbgp6ID0gbnAuY2xpcCh6LCAtNTAwLCA1MDApCmBgYApUaGlzIGVuc3VyZXMgbnVtZXJpY2FsIHN0YWJpbGl0eSB3aGVuIGRlYWxpbmcgd2l0aCBsYXJnZSBpbnB1dCB2YWx1ZXMu",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def predict_logistic(self, X: np.ndarray, weights: np.ndarray, bias: float) -> np.ndarray:\n        \"\"\"\n        Implements binary classification prediction using Logistic Regression.\n\n        Args:\n            X: Input feature matrix (shape: N \u00d7 D)\n            weights: Model weights (shape: D)\n            bias: Model bias\n\n        Returns:\n            Binary predictions (0 or 1)\n        \"\"\"\n        z = np.dot(X, weights) + bias\n        z = np.clip(z, -500, 500)  # Prevent overflow in exp\n        probabilities = 1 / (1 + np.exp(-z))\n        return (probabilities >= 0.5).astype(int)",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              1
            ],
            [
              2,
              2
            ],
            [
              -1,
              -1
            ],
            [
              -2,
              -2
            ]
          ],
          [
            1,
            1
          ],
          0
        ],
        "expected_output": "[1 1 0 0]"
      },
      {
        "test": [
          [
            [
              0,
              0
            ],
            [
              0.1,
              0.1
            ],
            [
              -0.1,
              -0.1
            ]
          ],
          [
            1,
            1
          ],
          0
        ],
        "expected_output": "[1 1 0]"
      }
    ],
    "function_name": "predict_logistic"
  },
  {
    "id": 105,
    "title": "Train Softmax Regression with Gradient Descent",
    "difficulty": "hard",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZ3JhZGllbnQgZGVzY2VudC1iYXNlZCB0cmFpbmluZyBhbGdvcml0aG0gZm9yIFNvZnRtYXggcmVncmVzc2lvbi4gWW91ciB0YXNrIGlzIHRvIGNvbXB1dGUgbW9kZWwgcGFyYW1ldGVycyB1c2luZyBDcm9zcyBFbnRyb3B5IGxvc3MgYW5kIHJldHVybiB0aGUgb3B0aW1pemVkIGNvZWZmaWNpZW50cyBhbG9uZyB3aXRoIGNvbGxlY3RlZCBsb3NzIHZhbHVlcyBvdmVyIGl0ZXJhdGlvbnMuIE1ha2Ugc3VyZSB0byByb3VuZCB5b3VyIHNvbHV0aW9uIHRvIDQgZGVjaW1hbCBwbGFjZXM=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def train_softmaxreg(self, X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \t\"\"\"\n    \tGradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)",
    "example_output": "([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])",
    "example_reasoning": "The function iteratively updates the Softmax regression parameters using gradient descent and collects loss values over iterations.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def train_softmaxreg(self, X: np.ndarray, y: np.ndarray, \n                     learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n        '''        \n        Gradient-descent training algorithm for softmax regression, that collects mean-reduced\n        CE losses, accuracies.\n        Returns\n        -------\n        B : list[float]\n            CxM updated parameter vector rounded to 4 floating points\n        losses : list[float]\n            collected values of a Cross Entropy rounded to 4 floating points\n        '''\n\n        def softmax(self, z):\n            return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n\n        def accuracy(self, y_pred, y_true):\n            return (np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1)).sum() / len(y_true)\n\n        def ce_loss(self, y_pred, y_true):\n            true_labels_idx = np.argmax(y_true, axis=1)\n            return -np.sum(np.log(y_pred)[list(range(len(y_pred))),true_labels_idx])\n\n        y = y.astype(int)\n        C = y.max()+1 # we assume that classes start from 0\n        y = np.eye(C)[y]\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n        B = np.zeros((X.shape[1], C))\n        accuracies, losses = [], []\n\n        for epoch in range(iterations):\n            y_pred = softmax(X @ B)\n            B -= learning_rate * X.T @ (y_pred - y)\n            losses.append(round(ce_loss(y_pred, y), 4))\n            accuracies.append(round(accuracy(y_pred, y), 4))\n\n        return B.T.round(4).tolist(), losses",
    "test_cases": [
      {
        "test": [
          [
            [
              2.5257,
              2.3333,
              1.773,
              0.4106,
              -1.6648
            ],
            [
              1.5101,
              1.3023,
              1.3198,
              1.3608,
              0.4638
            ],
            [
              -2.0969,
              -1.3596,
              -1.0403,
              -2.2548,
              -0.3235
            ],
            [
              -0.9666,
              -0.6068,
              -0.7201,
              -1.7325,
              -1.1281
            ],
            [
              -0.3809,
              -0.2485,
              0.1878,
              0.5235,
              1.3072
            ],
            [
              0.5482,
              0.3315,
              0.1067,
              0.3069,
              -0.3755
            ],
            [
              -3.0339,
              -2.0196,
              -0.6546,
              -0.9033,
              2.8918
            ],
            [
              0.286,
              -0.1265,
              -0.522,
              0.283,
              -0.5865
            ],
            [
              -0.2626,
              0.7601,
              1.8409,
              -0.2324,
              1.8071
            ],
            [
              0.3028,
              -0.4023,
              -1.2955,
              -0.1422,
              -1.7812
            ]
          ],
          [
            2,
            3,
            0,
            0,
            1,
            3,
            0,
            1,
            2,
            1
          ],
          0.03,
          10
        ],
        "expected_output": "([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7201, 9.3163, 8.4942, 7.9132, 7.4598, 7.0854, 6.7653, 6.4851, 6.2358])"
      },
      {
        "test": [
          [
            [
              0.5,
              -1.2
            ],
            [
              -0.3,
              1.1
            ],
            [
              0.8,
              -0.6
            ]
          ],
          [
            0,
            1,
            2
          ],
          0.01,
          10
        ],
        "expected_output": "([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])"
      }
    ],
    "function_name": "train_softmaxreg"
  },
  {
    "id": 106,
    "title": "Train Logistic Regression with Gradient Descent",
    "difficulty": "hard",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZ3JhZGllbnQgZGVzY2VudC1iYXNlZCB0cmFpbmluZyBhbGdvcml0aG0gZm9yIGxvZ2lzdGljIHJlZ3Jlc3Npb24uIFlvdXIgdGFzayBpcyB0byBjb21wdXRlIG1vZGVsIHBhcmFtZXRlcnMgdXNpbmcgQmluYXJ5IENyb3NzIEVudHJvcHkgbG9zcyBhbmQgcmV0dXJuIHRoZSBvcHRpbWl6ZWQgY29lZmZpY2llbnRzIGFsb25nIHdpdGggY29sbGVjdGVkIGxvc3MgdmFsdWVzIG92ZXIgaXRlcmF0aW9ucyhyb3VuZCB0byB0aGUgNHRoIGRlY2ltYWwpLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def train_logreg(self, X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \t\"\"\"\n    \tGradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "train_logreg(np.array([[1.0, 0.5], [-0.5, -1.5], [2.0, 1.5], [-2.0, -1.0]]), np.array([1, 0, 1, 0]), 0.01, 20)",
    "example_output": "([0.0037, 0.0246, 0.0202], [2.7726, 2.7373, 2.7024, 2.6678, 2.6335, 2.5995, 2.5659, 2.5327, 2.4997, 2.4671, 2.4348, 2.4029, 2.3712, 2.3399, 2.3089, 2.2783, 2.2480, 2.2180, 2.1882, 2.1588])",
    "example_reasoning": "The function iteratively updates the logistic regression parameters using gradient descent and collects loss values over iterations.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def train_logreg(self, X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n        \"\"\"\n        Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n        \"\"\"\n        def sigmoid(self, x):\n            return 1 / (1 + np.exp(-x))\n\n        y = y.reshape(-1, 1)\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n        B = np.zeros((X.shape[1], 1))\n        losses = []\n\n        for _ in range(iterations):\n            y_pred = sigmoid(X @ B)\n            B -= learning_rate * X.T @ (y_pred - y)\n            loss = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n            losses.append(round(loss, 4))\n\n        return B.flatten().round(4).tolist(), losses",
    "test_cases": [
      {
        "test": [
          [
            [
              0.7674,
              -0.2341,
              -0.2341,
              1.5792
            ],
            [
              -1.4123,
              0.3142,
              -1.0128,
              -0.908
            ],
            [
              -0.4657,
              0.5425,
              -0.4694,
              -0.4634
            ],
            [
              -0.5622,
              -1.9132,
              0.2419,
              -1.7249
            ],
            [
              -1.4247,
              -0.2257,
              1.4656,
              0.0675
            ],
            [
              1.8522,
              -0.2916,
              -0.6006,
              -0.6017
            ],
            [
              0.3756,
              0.1109,
              -0.5443,
              -1.1509
            ],
            [
              0.1968,
              -1.9596,
              0.2088,
              -1.3281
            ],
            [
              1.523,
              -0.1382,
              0.4967,
              0.6476
            ],
            [
              -1.2208,
              -1.0577,
              -0.0134,
              0.8225
            ]
          ],
          [
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0
          ],
          0.001,
          10
        ],
        "expected_output": "([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223])"
      },
      {
        "test": [
          [
            [
              0.76743473,
              1.57921282,
              -0.46947439
            ],
            [
              -0.23415337,
              1.52302986,
              -0.23413696
            ],
            [
              0.11092259,
              -0.54438272,
              -1.15099358
            ],
            [
              -0.60063869,
              0.37569802,
              -0.29169375
            ],
            [
              -1.91328024,
              0.24196227,
              -1.72491783
            ],
            [
              -1.01283112,
              -0.56228753,
              0.31424733
            ],
            [
              -0.1382643,
              0.49671415,
              0.64768854
            ],
            [
              -0.46341769,
              0.54256004,
              -0.46572975
            ],
            [
              -1.4123037,
              -0.90802408,
              1.46564877
            ],
            [
              0.0675282,
              -0.2257763,
              -1.42474819
            ]
          ],
          [
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0
          ],
          0.1,
          10
        ],
        "expected_output": "([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258])"
      }
    ],
    "function_name": "train_logreg"
  },
  {
    "id": 107,
    "title": "Implement Masked Self-Attention",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IG1hc2tlZCBzZWxmLWF0dGVudGlvbiwgYSB2YXJpYXRpb24gb2YgdGhlIGF0dGVudGlvbiBtZWNoYW5pc20gdXNlZCBpbiBzZXF1ZW5jZSBtb2RlbGluZyB0YXNrcyBzdWNoIGFzIHRleHQgZ2VuZXJhdGlvbi4gWW91ciB0YXNrIGlzIHRvIGNvbXB1dGUgbWFza2VkIHNlbGYtYXR0ZW50aW9uIHVzaW5nIHF1ZXJ5IChRKSwga2V5IChLKSwgdmFsdWUgKFYpIG1hdHJpY2VzIGFuZCBhbiBhdHRlbnRpb24gbWFzay4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def compute_qkv(self, X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \t\"\"\"\n    \tCompute Query (Q), Key (K), and Value (V) matrices.\n    \t\"\"\"\n    \treturn np.dot(X, W_q), np.dot(X, W_k), np.dot(X, W_v)\n\n    def masked_attention(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \t\"\"\"\n    \tCompute masked self-attention.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "masked_attention(Q, K, V, mask)",
    "example_output": "[[547. 490. 399. 495. 485. 439. 645. 393.]\n [547. 490. 399. 495. 485. 439. 645. 393.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]]",
    "example_reasoning": "The function computes self-attention by applying a mask to restrict information flow, ensuring causal dependencies are maintained.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def compute_qkv(self, X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n        Q = np.dot(X, W_q)\n        K = np.dot(X, W_k)\n        V = np.dot(X, W_v)\n        return Q, K, V\n\n    def masked_attention(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n        d_k = Q.shape[1]\n        scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n        scores = scores + mask  # Apply mask\n        attention_weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        attention_weights = attention_weights / np.sum(attention_weights, axis=1, keepdims=True)\n        return np.matmul(attention_weights, V)",
    "test_cases": [
      {
        "test": [
          "np.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(masked_attention(Q, K, V, mask))"
        ],
        "expected_output": "[[547. 490. 399. 495. 485. 439. 645. 393.]\n [547. 490. 399. 495. 485. 439. 645. 393.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]]"
      },
      {
        "test": [
          "np.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(masked_attention(Q, K, V, mask))"
        ],
        "expected_output": "[[ 52.  63.  48.  71.]\n [103. 109.  46.  99.]\n [103. 109.  46.  99.]\n [103. 109.  46.  99.]]"
      }
    ],
    "function_name": "compute_qkv"
  },
  {
    "id": 108,
    "title": "Measure Disorder in Apple Colors",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSBkaXNvcmRlciBpbiBhIGJhc2tldCBvZiBhcHBsZXMgYmFzZWQgb24gdGhlaXIgY29sb3JzLCB3aGVyZSBlYWNoIGFwcGxlIGNvbG9yIGlzIHJlcHJlc2VudGVkIGJ5IGFuIGludGVnZXIuIFRoZSBkaXNvcmRlciBtdXN0IGJlIDAgaWYgYWxsIGFwcGxlcyBhcmUgdGhlIHNhbWUgY29sb3IgYW5kIG11c3QgaW5jcmVhc2UgYXMgdGhlIHZhcmlldHkgb2YgY29sb3JzIGluY3JlYXNlcy4gSW4gcGFydGljdWxhcjoKLSBbMCwwLDAsMF0gc2hvdWxkIHlpZWxkIDAuCi0gWzEsMSwwLDBdIHNob3VsZCBoYXZlIGEgaGlnaGVyIGRpc29yZGVyIHRoYW4gWzAsMCwwLDBdLgotIFswLDEsMiwzXSBzaG91bGQgaGF2ZSBhIGhpZ2hlciBkaXNvcmRlciB0aGFuIFsxLDEsMCwwXS4KLSBbMCwwLDEsMSwyLDIsMywzXSBzaG91bGQgaGF2ZSBhIGhpZ2hlciBkaXNvcmRlciB0aGFuIFswLDAsMCwwLDAsMSwyLDNdLgoKWW91IG1heSB1c2UgYW55IG1ldGhvZCB0byBtZWFzdXJlIGRpc29yZGVyIGFzIGxvbmcgYXMgdGhlc2UgcHJvcGVydGllcyBhcmUgc2F0aXNmaWVkLg==",
    "starter_code": "\n\nclass Solution:\n    def disorder(self, apples: list) -> float:\n    \t\"\"\"\n    \tCompute the disorder in a basket of apples.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "disorder([1,1,0,0])",
    "example_output": "0.5 #or any value from -inf till +inf",
    "example_reasoning": "In the basket [1,1,0,0], there are two distinct colors each appearing with equal frequency (0.5).",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def disorder(self, apples: list) -> float:\n        \"\"\"\n        Calculates a measure of disorder in a basket of apples based on their colors.\n        One valid approach is to use the Gini impurity, defined as:\n          G = 1 - sum((count/total)^2 for each color)\n        This method returns 0 for a basket with all apples of the same color and increases as the variety of colors increases.\n        While this implementation uses the Gini impurity, any method that satisfies the following properties is acceptable:\n          1. A single color results in a disorder of 0.\n          2. Baskets with more distinct colors yield a higher disorder score.\n          3. The ordering constraints are maintained.\n        \"\"\"\n        if not apples:\n            return 0.0\n        total = len(apples)\n        counts = {}\n        for color in apples:\n            counts[color] = counts.get(color, 0) + 1\n        impurity = 1.0\n        for count in counts.values():\n            p = count / total\n            impurity -= p * p\n        return round(impurity, 4)",
    "test_cases": [
      {
        "test": [
          "print(disorder([0,0,0,0])<disorder([1,0,0,0]))"
        ],
        "expected_output": "True"
      },
      {
        "test": [
          "print(disorder([0,0,0,0,0,1,2,3])<disorder([0,0,1,1,2,2,3,3]))"
        ],
        "expected_output": "True"
      }
    ],
    "function_name": "disorder"
  },
  {
    "id": 109,
    "title": "Implement Layer Normalization for Sequence Data",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gcGVyZm9ybSBMYXllciBOb3JtYWxpemF0aW9uIG9uIGFuIGlucHV0IHRlbnNvci4gR2l2ZW4gYSAzRCBhcnJheSByZXByZXNlbnRpbmcgYmF0Y2hfc2l6ZSwgc2VxdWVuY2UgbGVuZ3RoLCBhbmQgZmVhdHVyZSBkaW1lbnNpb25zLCBub3JtYWxpemUgdGhlIGRhdGEgYWNyb3NzIHRoZSBmZWF0dXJlIGRpbWVuc2lvbiBmb3IgZWFjaCBzZXF1ZW5jZSwgdGhlbiBhcHBseSBzY2FsaW5nIGFuZCBzaGlmdGluZyBwYXJhbWV0ZXJzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def layer_normalization(self, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \t\"\"\"\n    \tPerform Layer Normalization.\n    \t\"\"\"\n    \t# Your code here\n    \tpass",
    "example_input": "np.random.seed(42); X = np.random.randn(2, 2, 3); gamma = np.ones(3).reshape(1, 1, -1); beta = np.zeros(3).reshape(1, 1, -1); layer_normalization(X, gamma, beta)",
    "example_output": "[[[ 0.47373971 -1.39079736  0.91705765]\n  [ 1.41420326 -0.70711154 -0.70709172]]\n [[ 1.13192477  0.16823009 -1.30015486]\n  [ 1.4141794  -0.70465482 -0.70952458]]]",
    "example_reasoning": "The function computes the mean and variance across the feature dimension (d_model=3) for each sequence, normalizes the input, then applies gamma=1 and beta=0, resulting in a normalized output with zero mean and unit variance scaled as is.",
    "learn_content": "## Understanding Layer Normalization

Layer Normalization (LN) is a technique commonly used in natural language processing (NLP) tasks to normalize the activations within each individual sample (or instance) across all the features (or dimensions). Unlike **Batch Normalization**, which normalizes across the batch dimension, Layer Normalization normalizes across the feature (or channel) dimension. This makes it particularly useful for sequence-based tasks, where the sequence length varies or where the batch size is small, as is often the case in NLP applications.

### Concepts

Layer Normalization operates on the principle of normalizing each input sample independently, across its feature dimensions. This ensures that all the features in a single instance (or sequence) are scaled and shifted to have similar statistics, which helps stabilize the training process and improve the model's ability to learn.

The process of Layer Normalization consists of the following steps:

1. **Compute the Mean and Variance for Each Sample:** For each input sequence, compute the mean and variance across the feature dimensions.
2. **Normalize the Inputs:** Normalize the activations by subtracting the mean and dividing by the standard deviation.
3. **Apply Scale and Shift:** After normalization, apply a learned scale (gamma) and shift (beta) to allow the model to restore the original distribution of the data, if necessary.

### Structure of Layer Normalization for (Batch Size, Sequence Length, Feature Dimension) Input

For an input tensor with the shape **(batch_size, seq_len, d_model)** (where:
- **batch_size**: number of sequences in a batch,
- **seq_len**: sequence length (number of tokens or time steps),
- **d_model**: number of features (model dimension)),
Layer Normalization operates over the **d_model** (features) dimension, normalizing each sequence independently.

#### 1. Mean and Variance Calculation for Each Sample

- For each individual sequence in the batch (for each **b** in **batch_size**), the **mean** $\mu_b$ and **variance** $\sigma_b^2$ are computed over the features (or channels) for that particular sequence. Importantly, this computation does not involve the batch dimension, meaning each sequence is normalized independently.

  $$ 
  \mu_b = \frac{1}{d_{\text{model}}} \sum_{i=1}^{d_{\text{model}}} x_{b,i}
  $$

  $$
  \sigma_b^2 = \frac{1}{d_{\text{model}}} \sum_{i=1}^{d_{\text{model}}} (x_{b,i} - \mu_b)^2
  $$

  Where:
  - $x_{b,i}$ is the activation at batch index $b$ and feature index $i$ (across the sequence length).
  - $d_{\text{model}}$ is the model dimension (the number of features).

#### 2. Normalization

Once the mean $\mu_b$ and variance $\sigma_b^2$ have been computed for each sequence, the next step is to **normalize** the input by subtracting the mean and dividing by the standard deviation (square root of variance plus a small constant $\epsilon$ for numerical stability):

$$
\hat{x}_{b,i} = \frac{x_{b,i} - \mu_b}{\sqrt{\sigma_b^2 + \epsilon}}
$$

Where:
- $\hat{x}_{b,i}$ is the normalized value of the $i$-th feature for the $b$-th sequence.
- $\epsilon$ is a small constant added to the variance for numerical stability.

#### 3. Scale and Shift

After normalization, the next step is to apply a **scale** ($\gamma$) and **shift** ($\beta$) to the normalized activations for each sequence. These are learned parameters that allow the model to adjust the output distribution for each sequence:

$$
y_{b,i} = \gamma \hat{x}_{b,i} + \beta
$$

Where:
- $\gamma$ is the scaling factor for the feature $i$.
- $\beta$ is the shifting factor for the feature $i$.

#### 4. Training and Inference

- **During Training**: For each sequence, the mean and variance are computed over the feature dimensions and used for normalization.
- **During Inference**: The model uses the running averages of the statistics (mean and variance) computed during training to ensure consistent behavior during inference.

### Why Use Layer Normalization in NLP?

Layer Normalization is especially useful in NLP and sequence-based tasks because of the following reasons:

- **Independence from Batch Size**: Layer Normalization operates independently for each sample (sequence), which means it does not depend on the batch size. This is important in NLP tasks where the batch size can vary, or be small, which would make Batch Normalization less effective.

- **Variable Sequence Lengths**: NLP models often work with sequences of varying lengths. Layer Normalization normalizes over the feature dimension, making it easier to handle sequences of different lengths without the need for special adjustments.

- **Training Stability**: Layer Normalization helps stabilize the training process by ensuring that the activations within each sequence are normalized, preventing the network from becoming sensitive to the scale of the inputs and improving gradient flow.

### Why Not Use Batch Normalization in NLP?

Batch Normalization (BN) normalizes over the batch dimension, which works well when the batch size is large and fixed. However, in NLP tasks, there are a few reasons why **Batch Normalization** is less commonly used:

- **Batch Size Variability**: In NLP, the batch size can vary across training and inference steps. A smaller or variable batch size can lead to poor estimates of the mean and variance during Batch Normalization, which can degrade performance.

- **Sequence Length Variability**: In NLP, the length of input sequences can vary greatly (e.g., sentences of different lengths). Batch Normalization requires that the statistics be computed over the batch, which makes it difficult to apply across sequences of varying lengths without padding or truncation.

- **Dependence on Batch Statistics**: Since Batch Normalization relies on batch statistics, it can cause issues when used in tasks with smaller or highly variable batch sizes, such as in NLP, where each sequence may not represent a meaningful distribution of activations across the batch.

### Key Points

- **Normalization Over Features**: Layer Normalization normalizes across the feature dimensions (model dimension), rather than across the batch dimension, making it ideal for NLP tasks where the batch size may vary or be small.

- **Sequence-Based Normalization**: By normalizing each sequence independently, Layer Normalization ensures that the activations within a single sequence are normalized, without needing information from other sequences in the batch.

- **Stabilizing Training**: Layer Normalization improves the gradient flow and ensures that activations within each sequence are consistent, which stabilizes training and helps prevent vanishing or exploding gradients.

- **Better for Small or Variable Batch Sizes**: Layer Normalization works well with smaller batch sizes, which are often used in NLP tasks like language modeling, machine translation, and text classification.

### Summary

Layer Normalization is particularly effective in NLP tasks because it normalizes each sequence independently, ensuring that each sample has a consistent activation distribution. It is preferable over Batch Normalization in cases where the batch size is small or variable, and when sequences have different lengths, making it a popular choice for sequence-based models like transformers, BERT, and GPT.
",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def layer_normalization(self, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n        \"\"\"\n        Perform Layer Normalization.\n        \"\"\"\n        mean = np.mean(X, axis=-1, keepdims=True)\n        variance = np.var(X, axis=-1, keepdims=True)\n        X_norm = (X - mean) / np.sqrt(variance + epsilon)\n        norm_X = gamma * X_norm + beta\n        return norm_X",
    "test_cases": [
      {
        "test": [
          "np.random.seed(42); X = np.random.randn(2, 2, 3); gamma = np.ones(3).reshape(1, 1, -1); beta = np.zeros(3).reshape(1, 1, -1); print(np.round(layer_normalization(X, gamma, beta), 3))"
        ],
        "expected_output": [
          [
            [
              0.474,
              -1.391,
              0.917
            ],
            [
              1.414,
              -0.707,
              -0.707
            ]
          ],
          [
            [
              1.132,
              0.168,
              -1.3
            ],
            [
              1.414,
              -0.705,
              -0.71
            ]
          ]
        ]
      },
      {
        "test": [
          "np.random.seed(42); X = np.random.randn(2, 3, 4); gamma = np.ones(4).reshape(1, 1, -1); beta = np.zeros(4).reshape(1, 1, -1); print(np.round(layer_normalization(X, gamma, beta), 3))"
        ],
        "expected_output": [
          [
            [
              -0.229,
              -1.3,
              0.026,
              1.502
            ],
            [
              -0.926,
              -0.926,
              1.46,
              0.392
            ],
            [
              -0.585,
              1.732,
              -0.571,
              -0.576
            ]
          ],
          [
            [
              1.401,
              -1.05,
              -0.836,
              0.486
            ],
            [
              -0.4,
              1.657,
              -0.238,
              -1.019
            ],
            [
              1.454,
              -0.191,
              0.094,
              -1.357
            ]
          ]
        ]
      }
    ],
    "function_name": "layer_normalization"
  },
  {
    "id": 110,
    "title": "Evaluate Translation Quality with METEOR Score",
    "difficulty": "medium",
    "category": "nlp",
    "problem_description": "RGV2ZWxvcCBhIGZ1bmN0aW9uIHRvIGNvbXB1dGUgdGhlIE1FVEVPUiBzY29yZSBmb3IgZXZhbHVhdGluZyBtYWNoaW5lIHRyYW5zbGF0aW9uIHF1YWxpdHkuIEdpdmVuIGEgcmVmZXJlbmNlIHRyYW5zbGF0aW9uIGFuZCBhIGNhbmRpZGF0ZSB0cmFuc2xhdGlvbiwgY2FsY3VsYXRlIHRoZSBzY29yZSBiYXNlZCBvbiB1bmlncmFtIG1hdGNoZXMsIHByZWNpc2lvbiwgcmVjYWxsLCBGLW1lYW4sIGFuZCBhIHBlbmFsdHkgZm9yIHdvcmQgb3JkZXIgZnJhZ21lbnRhdGlvbi4=",
    "starter_code": "import numpy as np\nfrom collections import Counter\n\nclass Solution:\n    def meteor_score(self, reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \t# Your code here\n    \tpass",
    "example_input": "meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky')",
    "example_output": "0.625",
    "example_reasoning": "The function identifies 4 unigram matches ('rain', 'gently'/'gentle', 'from', 'sky'), computes precision (4/6) and recall (4/5), calculates an F-mean, and then apply a small penalty for two chunks.",
    "learn_content": "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",
    "solution_code": "from collections import Counter\n\nclass Solution:\n    \"import numpy as np\n\n    def meteor_score(self, reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n        if not reference or not candidate:\n            raise ValueError(\"Reference and candidate cannot be empty\")\n\n        # Tokenize and count\n        ref_tokens = reference.lower().split()\n        cand_tokens = candidate.lower().split()\n\n        # Counter for unigram for reference and candidate \n        ref_counts = Counter(ref_tokens) \n        cand_counts = Counter(cand_tokens)\n\n        # Calculate matches\n        num_matches = sum((ref_counts & cand_counts).values()) # Number of matching words in candidate and reference \n        ref_len = len(ref_tokens)\n        cand_len = len(cand_tokens)  \n\n        # Unigram Precision and Recall \n        precision = num_matches / cand_len if cand_len > 0 else 0 # Avoiding Division by zero\n        recall = num_matches / ref_len if ref_len > 0 else 0 # Avoiding Division by zero \n\n        if num_matches == 0:\n            return 0.0\n\n        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n\n        # Chunk calculation \n        matched_positions = []\n        ref_positions = {}  # Store positions of words in reference\n        used_positions = set()  # Track already used indices\n\n        # Populate reference positions for word alignment tracking\n        for i, word in enumerate(ref_tokens):\n            ref_positions.setdefault(word, []).append(i)\n\n        # Determine the sequence of matched positions in reference\n        for word in cand_tokens:\n            if word in ref_positions:\n                for pos in ref_positions[word]:\n                    if pos not in used_positions:\n                        matched_positions.append(pos)\n                        used_positions.add(pos)\n                        break  # Ensure each match is used only once\n\n        # Count chunks by detecting breaks in position sequence\n        num_chunks = 1 if matched_positions else 0\n        for i in range(1, len(matched_positions)):\n            if matched_positions[i] != matched_positions[i - 1] + 1:\n                num_chunks += 1  # Break in sequence \u2192 new chunk\n\n        # Fragmentation penalty\n        penalty = gamma * ((num_chunks / num_matches) ** beta) if num_matches > 0 else 0\n\n        # Final score\n        return round(fmean * (1 - penalty), 3) # Rounding to 3 Decimal places \n",
    "test_cases": [
      {
        "test": [
          "print(round(meteor_score('The dog barks at the moon', 'The dog barks at the moon'),3))"
        ],
        "expected_output": 0.998
      },
      {
        "test": [
          "print(round(meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky'),3))"
        ],
        "expected_output": 0.625
      }
    ],
    "function_name": "meteor_score"
  },
  {
    "id": 111,
    "title": "Compute Pointwise Mutual Information",
    "difficulty": "medium",
    "category": "nlp",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgUG9pbnR3aXNlIE11dHVhbCBJbmZvcm1hdGlvbiAoUE1JKSBnaXZlbiB0aGUgam9pbnQgb2NjdXJyZW5jZSBjb3VudCBvZiB0d28gZXZlbnRzLCB0aGVpciBpbmRpdmlkdWFsIGNvdW50cywgYW5kIHRoZSB0b3RhbCBudW1iZXIgb2Ygc2FtcGxlcy4gUE1JIG1lYXN1cmVzIGhvdyBtdWNoIHRoZSBhY3R1YWwgam9pbnQgb2NjdXJyZW5jZSBvZiBldmVudHMgZGlmZmVycyBmcm9tIHdoYXQgd2Ugd291bGQgZXhwZWN0IGJ5IGNoYW5jZS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def compute_pmi(self, joint_counts, total_counts_x, total_counts_y, total_samples):\n    \t# Implement PMI calculation here\n    \tpass",
    "example_input": "compute_pmi(50, 200, 300, 1000)",
    "example_output": "-0.263",
    "example_reasoning": "The PMI calculation compares the actual joint probability (50/1000 = 0.05) to the product of the individual probabilities (200/1000 * 300/1000 = 0.06). Thus, PMI = log\u2082(0.05 / (0.2 * 0.3)) \u2248 -0.263, indicating the events co-occur slightly less than expected by chance.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def compute_pmi(self, joint_counts, total_counts_x, total_counts_y, total_samples):\n\n        if not all(isinstance(x, int) and x >= 0 for x in [joint_counts, total_counts_x, total_counts_y, total_samples]):\n            raise ValueError(\"All inputs must be non-negative integers.\")\n\n        if total_samples == 0:\n            raise ValueError(\"Total samples cannot be zero.\")\n\n        if joint_counts > min(total_counts_x, total_counts_y):\n            raise ValueError(\"Joint counts cannot exceed individual counts.\")\n\n        p_x = total_counts_x / total_samples\n        p_y = total_counts_y / total_samples\n        p_xy = joint_counts / total_samples\n\n        if p_xy == 0:\n            return float('-inf')\n\n        pmi = np.log2(p_xy / (p_x * p_y))\n\n        return round(pmi, 3)",
    "test_cases": [
      {
        "test": [
          10,
          50,
          50,
          200
        ],
        "expected_output": -0.322
      },
      {
        "test": [
          100,
          500,
          500,
          1000
        ],
        "expected_output": -1.322
      }
    ],
    "function_name": "compute_pmi"
  },
  {
    "id": 112,
    "title": "Min-Max Normalization of Feature Values",
    "difficulty": "easy",
    "category": "data preprocessing",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBNaW4tTWF4IE5vcm1hbGl6YXRpb24gb24gYSBsaXN0IG9mIGludGVnZXJzLCBzY2FsaW5nIGFsbCB2YWx1ZXMgdG8gdGhlIHJhbmdlIFswLCAxXS4gTWluLU1heCBub3JtYWxpemF0aW9uIGhlbHBzIGVuc3VyZSB0aGF0IGFsbCBmZWF0dXJlcyBjb250cmlidXRlIGVxdWFsbHkgdG8gYSBtb2RlbCBieSBzY2FsaW5nIHRoZW0gdG8gYSBjb21tb24gcmFuZ2Uu",
    "starter_code": "\n\nclass Solution:\n    def min_max(self, x: list[int]) -> list[float]:\n    \t# Your code here\n    \tpass",
    "example_input": "min_max([1, 2, 3, 4, 5])",
    "example_output": "[0.0, 0.25, 0.5, 0.75, 1.0]",
    "example_reasoning": "The minimum value is 1 and the maximum is 5. Each value is scaled using the formula (x - min) / (max - min).",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyBNaW4tTWF4IE5vcm1hbGl6YXRpb24KCk1pbi1NYXggTm9ybWFsaXphdGlvbiBpcyBhIHRlY2huaXF1ZSB1c2VkIHRvIHJlc2NhbGUgbnVtZXJpY2FsIGRhdGEgdG8gdGhlIHJhbmdlICRbMCwgMV0kLgoKVGhlIGZvcm11bGEgdXNlZCBpczoKCiQkClgnID0gXGZyYWN7WCAtIFhfe1xtaW59fXtYX3tcbWF4fSAtIFhfe1xtaW59fQokJAoKIyMjIFdoeSBOb3JtYWxpemU/CgotIEVuc3VyZXMgYWxsIGZlYXR1cmVzIGhhdmUgZXF1YWwgaW1wb3J0YW5jZSByZWdhcmRsZXNzIG9mIHRoZWlyIG9yaWdpbmFsIHNjYWxlLgotIENvbW1vbmx5IHVzZWQgaW4gcHJlcHJvY2Vzc2luZyBmb3IgbWFjaGluZSBsZWFybmluZyBhbGdvcml0aG1zIHN1Y2ggYXMgay1uZWFyZXN0IG5laWdoYm9ycywgbmV1cmFsIG5ldHdvcmtzLCBhbmQgZ3JhZGllbnQgZGVzY2VudC1iYXNlZCBtb2RlbHMuCgojIyMgU3BlY2lhbCBDYXNlCgpJZiBhbGwgdGhlIGVsZW1lbnRzIGluIHRoZSBpbnB1dCBhcmUgaWRlbnRpY2FsLCB0aGVuICRYX3tcbWF4fSA9IFhfe1xtaW59JC4gSW4gdGhhdCBjYXNlLCByZXR1cm4gYW4gYXJyYXkgb2YgemVyb3MuCgojIyMgRXhhbXBsZQoKR2l2ZW4gdGhlIGlucHV0IGxpc3QgYFsxLCAyLCAzLCA0LCA1XWA6CgotIE1pbmltdW06ICQxJAotIE1heGltdW06ICQ1JAotIFRoZSBub3JtYWxpemVkIHZhbHVlcyBhcmU6CgokJApcYmVnaW57YWxpZ25lZH0KJlxmcmFjezEgLSAxfXs0fSA9IDAuMCBcXAomXGZyYWN7MiAtIDF9ezR9ID0gMC4yNSBcXAomXGZyYWN7MyAtIDF9ezR9ID0gMC41IFxcCiZcZnJhY3s0IC0gMX17NH0gPSAwLjc1IFxcCiZcZnJhY3s1IC0gMX17NH0gPSAxLjAKXGVuZHthbGlnbmVkfQokJAoKVGhlIHJlc3VsdCBpcyBgWzAuMCwgMC4yNSwgMC41LCAwLjc1LCAxLjBdYC4KClJlbWVtYmVyIHRvIHJvdW5kIHRoZSByZXN1bHQgdG8gKio0IGRlY2ltYWwgcGxhY2VzKiou",
    "solution_code": "\n\nclass Solution:\n    def min_max(self, x: list[int]) -> list[float]:\n        largest = max(x)\n        smallest = min(x)\n        if largest == smallest:\n            return [0.0] * len(x)\n        for i in range(len(x)):\n            x[i] = round((x[i] - smallest) / (largest - smallest), 4)\n        return x",
    "test_cases": [
      {
        "test": [
          [
            1,
            2,
            3,
            4,
            5
          ]
        ],
        "expected_output": [
          0.0,
          0.25,
          0.5,
          0.75,
          1.0
        ]
      },
      {
        "test": [
          [
            30,
            45,
            56,
            70,
            88
          ]
        ],
        "expected_output": [
          0.0,
          0.2586,
          0.4483,
          0.6897,
          1.0
        ]
      }
    ],
    "function_name": "min_max"
  },
  {
    "id": 113,
    "title": "Implement a Simple Residual Block with Shortcut Connection",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjcmVhdGVzIGEgc2ltcGxlIHJlc2lkdWFsIGJsb2NrIHVzaW5nIE51bVB5LiBUaGUgYmxvY2sgc2hvdWxkIHRha2UgYSAxRCBpbnB1dCBhcnJheSwgcHJvY2VzcyBpdCB0aHJvdWdoIHR3byB3ZWlnaHQgbGF5ZXJzICh1c2luZyBtYXRyaXggbXVsdGlwbGljYXRpb24pLCBhcHBseSBSZUxVIGFjdGl2YXRpb25zLCBhbmQgYWRkIHRoZSBvcmlnaW5hbCBpbnB1dCB2aWEgYSBzaG9ydGN1dCBjb25uZWN0aW9uIGJlZm9yZSBhIGZpbmFsIFJlTFUgYWN0aXZhdGlvbi4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def residual_block(self, x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> np.ndarray:\n    \t# Your code here\n    \tpass",
    "example_input": "x = np.array([1.0, 2.0]), w1 = np.array([[1.0, 0.0], [0.0, 1.0]]), w2 = np.array([[0.5, 0.0], [0.0, 0.5]])",
    "example_output": "[1.5, 3.0]",
    "example_reasoning": "The input x is [1.0, 2.0]. First, compute w1 @ x = [1.0, 2.0], apply ReLU to get [1.0, 2.0]. Then, compute w2 @ [1.0, 2.0] = [0.5, 1.0]. Add the shortcut x to get [0.5 + 1.0, 1.0 + 2.0] = [1.5, 3.0]. Final ReLU gives [1.5, 3.0].",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def residual_block(self, x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> np.ndarray:\n        # First weight layer\n        y = np.dot(w1, x)\n        # First ReLU\n        y = np.maximum(0, y)\n        # Second weight layer\n        y = np.dot(w2, y)\n        # Add shortcut connection (x + F(x))\n        y = y + x\n        # Final ReLU\n        y = np.maximum(0, y)\n        return y",
    "test_cases": [
      {
        "test": [
          "x = np.array([1.0, 2.0])\nw1 = np.array([[1.0, 0.0], [0.0, 1.0]])\nw2 = np.array([[0.5, 0.0], [0.0, 0.5]])\nprint(residual_block(x, w1, w2))"
        ],
        "expected_output": [
          1.5,
          3.0
        ]
      },
      {
        "test": [
          "x = np.array([-1.0, 2.0])\nw1 = np.array([[1.0, 0.0], [0.0, 1.0]])\nw2 = np.array([[0.5, 0.0], [0.0, 0.5]])\nprint(residual_block(x, w1, w2))"
        ],
        "expected_output": [
          0.0,
          3.0
        ]
      }
    ],
    "function_name": "residual_block"
  },
  {
    "id": 114,
    "title": "Implement Global Average Pooling",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBHbG9iYWwgQXZlcmFnZSBQb29saW5nIG9uIGEgM0QgTnVtUHkgYXJyYXkgcmVwcmVzZW50aW5nIGZlYXR1cmUgbWFwcyBmcm9tIGEgY29udm9sdXRpb25hbCBsYXllci4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGFuIGlucHV0IG9mIHNoYXBlIChoZWlnaHQsIHdpZHRoLCBjaGFubmVscykgYW5kIHJldHVybiBhIDFEIGFycmF5IG9mIHNoYXBlIChjaGFubmVscywpLCB3aGVyZSBlYWNoIGVsZW1lbnQgaXMgdGhlIGF2ZXJhZ2Ugb2YgYWxsIHZhbHVlcyBpbiB0aGUgY29ycmVzcG9uZGluZyBmZWF0dXJlIG1hcC4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def global_avg_pool(self, x: np.ndarray) -> np.ndarray:\n    \t# Your code here\n    \tpass",
    "example_input": "x = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])",
    "example_output": "[5.5, 6.5, 7.5]",
    "example_reasoning": "For each channel, compute the average of all elements. For channel 0: (1+4+7+10)/4 = 5.5, for channel 1: (2+5+8+11)/4 = 6.5, for channel 2: (3+6+9+12)/4 = 7.5.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def global_avg_pool(self, x: np.ndarray) -> np.ndarray:\n        return np.mean(x, axis=(0, 1))",
    "test_cases": [
      {
        "test": [
          "x = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nprint(global_avg_pool(x))"
        ],
        "expected_output": [
          5.5,
          6.5,
          7.5
        ]
      },
      {
        "test": [
          "x = np.array([[[100, 200]]])\nprint(global_avg_pool(x))"
        ],
        "expected_output": [
          100.0,
          200.0
        ]
      }
    ],
    "function_name": "global_avg_pool"
  },
  {
    "id": 115,
    "title": "Implement Batch Normalization for BCHW Input",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBCYXRjaCBOb3JtYWxpemF0aW9uIG9uIGEgNEQgTnVtUHkgYXJyYXkgcmVwcmVzZW50aW5nIGEgYmF0Y2ggb2YgZmVhdHVyZSBtYXBzIGluIHRoZSBCQ0hXIGZvcm1hdCAoYmF0Y2gsIGNoYW5uZWxzLCBoZWlnaHQsIHdpZHRoKS4gVGhlIGZ1bmN0aW9uIHNob3VsZCBub3JtYWxpemUgdGhlIGlucHV0IGFjcm9zcyB0aGUgYmF0Y2ggYW5kIHNwYXRpYWwgZGltZW5zaW9ucyBmb3IgZWFjaCBjaGFubmVsLCB0aGVuIGFwcGx5IHNjYWxlIChnYW1tYSkgYW5kIHNoaWZ0IChiZXRhKSBwYXJhbWV0ZXJzLiBVc2UgdGhlIHByb3ZpZGVkIGVwc2lsb24gdmFsdWUgdG8gZW5zdXJlIG51bWVyaWNhbCBzdGFiaWxpdHku",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def batch_normalization(self, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \t# Your code here\n    \tpass",
    "example_input": "B, C, H, W = 2, 2, 2, 2; np.random.seed(42); X = np.random.randn(B, C, H, W); gamma = np.ones(C).reshape(1, C, 1, 1); beta = np.zeros(C).reshape(1, C, 1, 1)",
    "example_output": "[[[[ 0.42859934, -0.51776438], [ 0.65360963,  1.95820707]], [[ 0.02353721,  0.02355215], [ 1.67355207,  0.93490043]]], [[[-1.01139563,  0.49692747], [-1.00236882, -1.00581468]], [[ 0.45676349, -1.50433085], [-1.33293647, -0.27503802]]]]",
    "example_reasoning": "The input X is a 2x2x2x2 array. For each channel, compute the mean and variance across the batch (B), height (H), and width (W) dimensions. Normalize X using (X - mean) / sqrt(variance + epsilon), then scale by gamma and shift by beta. The output matches the expected normalized values.",
    "learn_content": "## Understanding Batch Normalization

Batch Normalization (BN) is a widely used technique that helps to accelerate the training of deep neural networks and improve model performance. By normalizing the inputs to each layer so that they have a mean of zero and a variance of one, BN stabilizes the learning process, speeds up convergence, and introduces regularization, which can reduce the need for other forms of regularization like dropout.

### Concepts

Batch Normalization operates on the principle of reducing **internal covariate shift**, which occurs when the distribution of inputs to a layer changes during training as the model weights get updated. This can slow down training and make hyperparameter tuning more challenging. By normalizing the inputs, BN reduces this problem, allowing the model to train faster and more reliably.

The process of Batch Normalization consists of the following steps:

1. **Compute the Mean and Variance:** For each mini-batch, compute the mean and variance of the activations for each feature (dimension).
2. **Normalize the Inputs:** Normalize the activations using the computed mean and variance.
3. **Apply Scale and Shift:** After normalization, apply a learned scale (gamma) and shift (beta) to restore the model's ability to represent the data's original distribution.
4. **Training and Inference:** During training, the mean and variance are computed from the current mini-batch. During inference, a running average of the statistics from the training phase is used.

### Structure of Batch Normalization for BCHW Input

For an input tensor with the shape **BCHW**, where:
- **B**: batch size,
- **C**: number of channels,
- **H**: height,
- **W**: width,
the Batch Normalization process operates on specific dimensions based on the task's requirement.

#### 1. Mean and Variance Calculation

- In **Batch Normalization**, we typically normalize the activations **across the batch** and **over the spatial dimensions (height and width)** for each **channel**. This means we calculate the mean and variance **per channel** (C) for the **batch and spatial dimensions** (H, W).

For each channel $c$, we compute the **mean** $\mu_c$ and **variance** $\sigma_c^2$ over the mini-batch and spatial dimensions:

$$
\mu_c = \frac{1}{B \cdot H \cdot W} \sum_{i=1}^{B} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{i,c,h,w}
$$

$$
\sigma_c^2 = \frac{1}{B \cdot H \cdot W} \sum_{i=1}^{B} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{i,c,h,w} - \mu_c)^2
$$

Where:
- $x_{i,c,h,w}$ is the input activation at batch index $i$, channel $c$, height $h$, and width $w$.
- $B$ is the batch size.
- $H$ and $W$ are the spatial dimensions (height and width).
- $C$ is the number of channels.

The mean and variance are computed **over all spatial positions (H, W)** and **across all samples in the batch (B)** for each **channel (C)**.

#### 2. Normalization

Once the mean $\mu_c$ and variance $\sigma_c^2$ have been computed for each channel, the next step is to **normalize** the input. The normalization is done by subtracting the mean and dividing by the standard deviation (plus a small constant $\epsilon$ for numerical stability):

$$
\hat{x}_{i,c,h,w} = \frac{x_{i,c,h,w} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}
$$

Where:
- $\hat{x}_{i,c,h,w}$ is the normalized activation for the input at batch index $i$, channel $c$, height $h$, and width $w$.
- $\epsilon$ is a small constant to avoid division by zero (for numerical stability).

#### 3. Scale and Shift

After normalization, the next step is to apply a **scale** ($\gamma_c$) and **shift** ($\beta_c$) to the normalized activations for each channel. These learned parameters allow the model to adjust the output distribution of each feature, preserving the flexibility of the original activations.

$$
y_{i,c,h,w} = \gamma_c \hat{x}_{i,c,h,w} + \beta_c
$$

Where:
- $\gamma_c$ is the scaling factor for channel $c$.
- $\beta_c$ is the shifting factor for channel $c$.

#### 4. Training and Inference

- **During Training**: The mean and variance are computed for each mini-batch and used for normalization across the batch and spatial dimensions for each channel.
- **During Inference**: The model uses a running average of the statistics (mean and variance) that were computed during training to ensure consistent behavior when deployed.

### Key Points

- **Normalization Across Batch and Spatial Dimensions**: In Batch Normalization for **BCHW** input, the normalization is done **across the batch (B) and spatial dimensions (H, W)** for each **channel (C)**. This ensures that each feature channel has zero mean and unit variance, making the training process more stable.

- **Channel-wise Normalization**: Batch Normalization normalizes the activations independently for each **channel (C)** because different channels in convolutional layers often have different distributions and should be treated separately.

- **Numerical Stability**: The small constant $\epsilon$ is added to the variance to avoid numerical instability when dividing by the square root of variance, especially when the variance is very small.

- **Improved Gradient Flow**: By reducing internal covariate shift, Batch Normalization allows the gradients to flow more easily during backpropagation, helping the model train faster and converge more reliably.

- **Regularization Effect**: Batch Normalization introduces noise into the training process because it relies on the statistics of a mini-batch. This noise acts as a form of regularization, which can prevent overfitting and improve generalization.

### Why Normalize Over Batch and Spatial Dimensions?

- **Across Batch**: Normalizing across the batch helps to stabilize the input distribution across all samples in a mini-batch. This allows the model to avoid the problem of large fluctuations in the input distribution as weights are updated.

- **Across Spatial Dimensions**: In convolutional networks, the spatial dimensions (height and width) are highly correlated, and normalizing over these dimensions ensures that the activations are distributed consistently throughout the spatial field, helping to maintain a stable learning process.

- **Channel-wise Normalization**: Each channel can have its own distribution of values, and normalization per channel ensures that each feature map is scaled and shifted independently, allowing the model to learn representations that are not overly sensitive to specific channels' scaling.

By normalizing across the batch and spatial dimensions and applying a per-channel transformation, Batch Normalization helps reduce internal covariate shift and speeds up training, leading to faster convergence and better overall model performance.",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def batch_normalization(self, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n        # Compute mean and variance across the batch and spatial dimensions\n        mean = np.mean(X, axis=(0, 2, 3), keepdims=True)  # Mean over (B, H, W)\n        variance = np.var(X, axis=(0, 2, 3), keepdims=True)  # Variance over (B, H, W)\n        # Normalize\n        X_norm = (X - mean) / np.sqrt(variance + epsilon)\n        # Scale and shift\n        norm_X = gamma * X_norm + beta\n        return norm_X",
    "test_cases": [
      {
        "test": [
          "B, C, H, W = 2, 2, 2, 2\nnp.random.seed(42)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C).reshape(1, C, 1, 1)\nbeta = np.zeros(C).reshape(1, C, 1, 1)\nactual_output = batch_normalization(X, gamma, beta)\nexpected_output = np.array([[[[ 0.42859934, -0.51776438], [ 0.65360963,  1.95820707]], [[ 0.02353721,  0.02355215], [ 1.67355207,  0.93490043]]], [[[-1.01139563,  0.49692747], [-1.00236882, -1.00581468]], [[ 0.45676349, -1.50433085], [-1.33293647, -0.27503802]]]])\nprint(actual_output)"
        ],
        "expected_output": [
          [
            [
              [
                0.42859934,
                -0.51776438
              ],
              [
                0.65360963,
                1.95820707
              ]
            ],
            [
              [
                0.02353721,
                0.02355215
              ],
              [
                1.67355207,
                0.93490043
              ]
            ]
          ],
          [
            [
              [
                -1.01139563,
                0.49692747
              ],
              [
                -1.00236882,
                -1.00581468
              ]
            ],
            [
              [
                0.45676349,
                -1.50433085
              ],
              [
                -1.33293647,
                -0.27503802
              ]
            ]
          ]
        ]
      },
      {
        "test": [
          "B, C, H, W = 2, 2, 2, 2\nnp.random.seed(101)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C).reshape(1, C, 1, 1)\nbeta = np.zeros(C).reshape(1, C, 1, 1)\nactual_output = batch_normalization(X, gamma, beta)\nexpected_output = np.array([[[[ 1.81773164,  0.16104096], [ 0.38406453,  0.06197112]], [[ 1.00432932, -0.37139956], [-1.12098938,  0.94031919]]], [[[-1.94800122,  0.25029395], [ 0.08188579, -0.80898678]], [[ 0.34878049, -0.99452891], [-1.24171594,  1.43520478]]]])\nprint(actual_output)"
        ],
        "expected_output": [
          [
            [
              [
                1.81773164,
                0.16104096
              ],
              [
                0.38406453,
                0.06197112
              ]
            ],
            [
              [
                1.00432932,
                -0.37139956
              ],
              [
                -1.12098938,
                0.94031919
              ]
            ]
          ],
          [
            [
              [
                -1.94800122,
                0.25029395
              ],
              [
                0.08188579,
                -0.80898678
              ]
            ],
            [
              [
                0.34878049,
                -0.99452891
              ],
              [
                -1.24171594,
                1.43520478
              ]
            ]
          ]
        ]
      }
    ],
    "function_name": "batch_normalization"
  },
  {
    "id": 116,
    "title": "Derivative of a Polynomial",
    "difficulty": "easy",
    "category": "calculus",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgZGVyaXZhdGl2ZSBvZiBhIHBvbHlub21pYWwgdGVybSBvZiB0aGUgZm9ybSBgYyAqIHhebmAgYXQgYSBnaXZlbiBwb2ludCBgeGAsIHdoZXJlIGBjYCBpcyBhIGNvZWZmaWNpZW50IGFuZCBgbmAgaXMgdGhlIGV4cG9uZW50LiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgdmFsdWUgb2YgdGhlIGRlcml2YXRpdmUsIGFjY291bnRpbmcgZm9yIHRoZSBjb2VmZmljaWVudCBpbiB0aGUgcG93ZXIgcnVsZS4gVGhpcyBpcyB1c2VmdWwgZm9yIHVuZGVyc3RhbmRpbmcgaG93IHBvbHlub21pYWxzIGNoYW5nZSBhdCBzcGVjaWZpYyBwb2ludHMgaW4gbWFjaGluZSBsZWFybmluZyBvcHRpbWl6YXRpb24gcHJvYmxlbXMu",
    "starter_code": "\n\nclass Solution:\n    def poly_term_derivative(self, c: float, x: float, n: float) -> float:\n        # Your code here\n        pass",
    "example_input": "poly_term_derivative(2.0, 3.0, 2.0)",
    "example_output": "12.0",
    "example_reasoning": "For the term 2 * x^2, the derivative is 2 * 2 * x^(2-1) = 4 * x. At x = 3, this evaluates to 4 * 3 = 12.0.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def poly_term_derivative(self, c: float, x: float, n: float) -> float:\n        if n == 0.0:\n            return 0.0\n        return round(c * n * (x ** (n - 1)), 4)",
    "test_cases": [
      {
        "test": [
          2.0,
          3.0,
          2.0
        ],
        "expected_output": 12.0
      },
      {
        "test": [
          1.5,
          4.0,
          0.0
        ],
        "expected_output": 0.0
      }
    ],
    "function_name": "poly_term_derivative"
  },
  {
    "id": 117,
    "title": "Compute Orthonormal Basis for 2D Vectors",
    "difficulty": "medium",
    "category": "linear algebra",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjb21wdXRlcyBhbiBvcnRob25vcm1hbCBiYXNpcyBmb3IgdGhlIHN1YnNwYWNlIHNwYW5uZWQgYnkgYSBsaXN0IG9mIDJEIHZlY3RvcnMgdXNpbmcgdGhlIEdyYW0tU2NobWlkdCBwcm9jZXNzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBsaXN0IG9mIDJEIHZlY3RvcnMgYW5kIGEgdG9sZXJhbmNlIHZhbHVlICh0b2wpIHRvIGRldGVybWluZSBsaW5lYXIgaW5kZXBlbmRlbmNlLCByZXR1cm5pbmcgYSBsaXN0IG9mIG9ydGhvbm9ybWFsIHZlY3RvcnMgKHVuaXQgbGVuZ3RoIGFuZCBvcnRob2dvbmFsIHRvIGVhY2ggb3RoZXIpIHRoYXQgc3BhbiB0aGUgc2FtZSBzdWJzcGFjZS4gVGhpcyBpcyBhIGZ1bmRhbWVudGFsIGNvbmNlcHQgaW4gbGluZWFyIGFsZ2VicmEgd2l0aCBhcHBsaWNhdGlvbnMgaW4gbWFjaGluZSBsZWFybmluZywgc3VjaCBhcyBmZWF0dXJlIG9ydGhvZ29uYWxpemF0aW9uLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def orthonormal_basis(self, vectors: list[list[float]], tol: float = 1e-10) -> list[np.ndarray]:\n        # Your code here\n        pass",
    "example_input": "orthonormal_basis([[1, 0], [1, 1]])",
    "example_output": "[array([1., 0.]), array([0., 1.])]",
    "example_reasoning": "Start with [1, 0], normalize to [1, 0]. For [1, 1], subtract its projection onto [1, 0] (which is [1, 0]), leaving [0, 1]. Check if norm > 1e-10 (it is 1), then normalize to [0, 1]. The result is an orthonormal basis.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyB0aGUgR3JhbS1TY2htaWR0IFByb2Nlc3MKClRoZSBHcmFtLVNjaG1pZHQgcHJvY2VzcyB0cmFuc2Zvcm1zIGEgc2V0IG9mIHZlY3RvcnMgaW50byBhbiBvcnRob25vcm1hbCBiYXNpcyB2ZWN0b3JzIHRoYXQgYXJlIG9ydGhvZ29uYWwgKHBlcnBlbmRpY3VsYXIpIGFuZCBoYXZlIHVuaXQgbGVuZ3RoIGZvciB0aGUgc3Vic3BhY2UgdGhleSBzcGFuLgoKIyMjIE1hdGhlbWF0aWNhbCBEZWZpbml0aW9uCgpHaXZlbiB2ZWN0b3JzICR2XzEsIHZfMiwgXGxkb3RzJCwgdGhlIHByb2Nlc3MgY29uc3RydWN0cyBhbiBvcnRob25vcm1hbCBzZXQgJHVfMSwgdV8yLCBcbGRvdHMkIGFzIGZvbGxvd3M6CjEuICR1XzEgPSBcZnJhY3t2XzF9e1x8dl8xXHx9JCAobm9ybWFsaXplIHRoZSBmaXJzdCB2ZWN0b3IpLgoyLiBGb3Igc3Vic2VxdWVudCB2ZWN0b3JzICR2X2skOgogICAtIFN1YnRyYWN0IHByb2plY3Rpb25zOiAkJHdfayA9IHZfayAtIFxzdW1fe2k9MX1ee2stMX0gXHRleHR7cHJvan1fe3VfaX0odl9rKSwkJCB3aGVyZSAkXHRleHR7cHJvan1fe3VfaX0odl9rKSA9ICh2X2sgXGNkb3QgdV9pKSB1X2kkLgogICAtIE5vcm1hbGl6ZTogJCR1X2sgPSBcZnJhY3t3X2t9e1x8d19rXHx9LCQkIGlmICRcfHdfa1x8ID4gXHRleHR7dG9sfSQuCgojIyMgV2h5IE9ydGhvbm9ybWFsIEJhc2VzPwoKLSBPcnRob2dvbmFsIHZlY3RvcnMgc2ltcGxpZnkgY29tcHV0YXRpb25zIChlLmcuLCB0aGVpciBkb3QgcHJvZHVjdCBpcyB6ZXJvKS4KLSBVbml0IGxlbmd0aCBlbnN1cmVzIGVxdWFsIHNjYWxpbmcsIHVzZWZ1bCBpbiAkUENBJCwgJFFSJCBkZWNvbXBvc2l0aW9uLCBhbmQgbmV1cmFsIG5ldHdvcmsgb3B0aW1pemF0aW9uLgoKIyMjIFNwZWNpYWwgQ2FzZQoKSWYgYSB2ZWN0b3IncyBub3JtIGlzIGxlc3MgdGhhbiBvciBlcXVhbCB0byAkXHRleHR7dG9sfSQgKGRlZmF1bHQgJDFlLTEwJCksIGl0J3MgY29uc2lkZXJlZCBsaW5lYXJseSBkZXBlbmRlbnQgYW5kIGV4Y2x1ZGVkIGZyb20gdGhlIGJhc2lzLgoKIyMjIEV4YW1wbGUKCkZvciB2ZWN0b3JzIGBbWzEsIDBdLCBbMSwgMV1dYCB3aXRoICRcdGV4dHt0b2x9ID0gMWUtMTAkOgoxLiAkdl8xID0gWzEsIDBdJCwgJFx8dl8xXHwgPSAxJCwgc28gJHVfMSA9IFsxLCAwXSQuCjIuICR2XzIgPSBbMSwgMV0kLCBwcm9qZWN0aW9uIG9uICR1XzEkOiAkKHZfMiBcY2RvdCB1XzEpIHVfMSA9IDEgXGNkb3QgWzEsIDBdID0gWzEsIDBdJC4KICAgLSAkd18yID0gWzEsIDFdIC0gWzEsIDBdID0gWzAsIDFdJC4KICAgLSAkXHx3XzJcfCA9IDEgPiAxZS0xMCQsIHNvICR1XzIgPSBbMCwgMV0kLgoKUmVzdWx0OiBgW1sxLCAwXSwgWzAsIDFdXWAsIHJvdW5kZWQgdG8gNCBkZWNpbWFsIHBsYWNlcy4=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def orthonormal_basis(self, vectors: list[list[float]], tol: float = 1e-10) -> list[np.ndarray]:\n        basis = []\n        for v in vectors:\n            v = np.array(v, dtype=float)\n            for b in basis:\n                v = v - np.dot(v, b) * b\n            norm = np.sqrt(np.dot(v, v))\n            if norm > tol:\n                v = v / norm\n                basis.append(v)\n        return basis",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              0
            ],
            [
              1,
              1
            ]
          ]
        ],
        "expected_output": "[array([1., 0.]), array([0., 1.])]"
      },
      {
        "test": [
          [
            [
              2,
              0
            ],
            [
              4,
              0
            ]
          ]
        ],
        "expected_output": "[array([1., 0.])]"
      }
    ],
    "function_name": "orthonormal_basis"
  },
  {
    "id": 118,
    "title": "Compute the Cross Product of Two 3D Vectors",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgY3Jvc3MgcHJvZHVjdCBvZiB0d28gMy1kaW1lbnNpb25hbCB2ZWN0b3JzLiBUaGUgY3Jvc3MgcHJvZHVjdCBvZiB0d28gdmVjdG9ycyByZXN1bHRzIGluIGEgdGhpcmQgdmVjdG9yIHRoYXQgaXMgcGVycGVuZGljdWxhciB0byBib3RoIGFuZCBmb2xsb3dzIHRoZSByaWdodC1oYW5kIHJ1bGUuIFRoaXMgY29uY2VwdCBpcyBmdW5kYW1lbnRhbCBpbiBwaHlzaWNzLCBlbmdpbmVlcmluZywgYW5kIDNEIGdyYXBoaWNzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def cross_product(self, a, b):\n        # Your code here\n        pass",
    "example_input": "cross_product([1, 0, 0], [0, 1, 0])",
    "example_output": "[0, 0, 1]",
    "example_reasoning": "The cross product of two orthogonal unit vectors [1, 0, 0] and [0, 1, 0] is [0, 0, 1], pointing in the positive z-direction as per the right-hand rule.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyB0aGUgQ3Jvc3MgUHJvZHVjdAoKVGhlICoqY3Jvc3MgcHJvZHVjdCoqIG9mIHR3byB2ZWN0b3JzICRcdmVje2F9JCBhbmQgJFx2ZWN7Yn0kIGluIDNEIHNwYWNlIGlzIGEgdmVjdG9yIHRoYXQgaXMgcGVycGVuZGljdWxhciB0byBib3RoICRcdmVje2F9JCBhbmQgJFx2ZWN7Yn0kLgoKIyMjIFByb3BlcnRpZXMKLSBEZWZpbmVkIG9ubHkgaW4gMyBkaW1lbnNpb25zLgotIFRoZSByZXN1bHQgJFx2ZWN7Y30gPSBcdmVje2F9IFx0aW1lcyBcdmVje2J9JCBpcyBwZXJwZW5kaWN1bGFyIHRvIGJvdGggJFx2ZWN7YX0kIGFuZCAkXHZlY3tifSQuCi0gRm9sbG93cyB0aGUgcmlnaHQtaGFuZCBydWxlLgoKIyMjIE1hdGhlbWF0aWNhbCBGb3JtdWxhCgpHaXZlbjoKLSAkXHZlY3thfSA9IFthXzEsIGFfMiwgYV8zXSQKLSAkXHZlY3tifSA9IFtiXzEsIGJfMiwgYl8zXSQKClRoZSBjcm9zcyBwcm9kdWN0IGlzOgoKJCQKXHZlY3thfSBcdGltZXMgXHZlY3tifSA9IFthXzIgYl8zIC0gYV8zIGJfMixcIGFfMyBiXzEgLSBhXzEgYl8zLFwgYV8xIGJfMiAtIGFfMiBiXzFdCiQkCgojIyMgVXNlIENhc2VzCi0gQ2FsY3VsYXRpbmcgbm9ybWFscyBpbiAzRCBncmFwaGljcy4KLSBEZXRlcm1pbmluZyB0b3JxdWUgYW5kIGFuZ3VsYXIgbW9tZW50dW0gaW4gcGh5c2ljcy4KLSBWZXJpZnlpbmcgb3J0aG9nb25hbGl0eSBpbiBtYWNoaW5lIGxlYXJuaW5nIGdlb21ldHJ5LgoKIyMjIEV4YW1wbGUKRm9yICRcdmVje2F9ID0gWzEsIDAsIDBdJCBhbmQgJFx2ZWN7Yn0gPSBbMCwgMSwgMF0kOgoKJCQKXHZlY3thfSBcdGltZXMgXHZlY3tifSA9IFswLCAwLCAxXQokJAoKVGhlIHJlc3VsdCBwb2ludHMgaW4gdGhlICR6JC1heGlzIGRpcmVjdGlvbiwgY29uZmlybWluZyBwZXJwZW5kaWN1bGFyaXR5IHRvIGJvdGggJFx2ZWN7YX0kIGFuZCAkXHZlY3tifSQu",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def cross_product(self, a, b):\n        \"\"\"\n        Compute the cross product of two 3D vectors a and b.\n        Parameters:\n            a (array-like): A 3-element vector.\n            b (array-like): A 3-element vector.\n        Returns:\n            numpy.ndarray: The cross product vector.\n        \"\"\"\n        a = np.array(a)\n        b = np.array(b)\n\n        if a.shape != (3,) or b.shape != (3,):\n            raise ValueError(\"Both input vectors must be of length 3.\")\n\n        cross = np.array([\n            a[1] * b[2] - a[2] * b[1],\n            a[2] * b[0] - a[0] * b[2],\n            a[0] * b[1] - a[1] * b[0]\n        ])\n        return cross",
    "test_cases": [
      {
        "test": [
          [
            1,
            0,
            0
          ],
          [
            0,
            1,
            0
          ]
        ],
        "expected_output": [
          0,
          0,
          1
        ]
      },
      {
        "test": [
          [
            0,
            1,
            0
          ],
          [
            0,
            0,
            1
          ]
        ],
        "expected_output": [
          1,
          0,
          0
        ]
      }
    ],
    "function_name": "cross_product"
  },
  {
    "id": 119,
    "title": "Solve System of Linear Equations Using Cramer's Rule",
    "difficulty": "medium",
    "category": "linear algebra for machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gc29sdmUgYSBzeXN0ZW0gb2YgbGluZWFyIGVxdWF0aW9ucyAkQXggPSBiJCB1c2luZyBDcmFtZXIncyBSdWxlLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBzcXVhcmUgY29lZmZpY2llbnQgbWF0cml4ICRBJCBhbmQgYSBjb25zdGFudCB2ZWN0b3IgJGIkLCBhbmQgcmV0dXJuIHRoZSBzb2x1dGlvbiB2ZWN0b3IgJHgkLiBJZiB0aGUgc3lzdGVtIGhhcyBubyB1bmlxdWUgc29sdXRpb24gKGkuZS4sIHRoZSBkZXRlcm1pbmFudCBvZiAkQSQgaXMgemVybyksIHJldHVybiAtMS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def cramers_rule(self, A, b):\n        # Your code here\n        pass",
    "example_input": "A = [[2, -1, 3], [4, 2, 1], [-6, 1, -2]], b = [5, 10, -3]",
    "example_output": "[0.1667 3.3333 2.6667]",
    "example_reasoning": "We compute the determinant of A and then replace each column with vector b to compute the determinants of modified matrices. These are then used in the formula $x_i = \\frac{\\det(A_i)}{\\det(A)}$ to get the solution.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyBDcmFtZXIncyBSdWxlCgpDcmFtZXIncyBSdWxlIGlzIGEgbWV0aG9kIHRvIHNvbHZlIGEgc3lzdGVtIG9mIGxpbmVhciBlcXVhdGlvbnMgJEF4ID0gYiQgdXNpbmcgZGV0ZXJtaW5hbnRzLgoKIyMjIFJlcXVpcmVtZW50cwotIFRoZSBjb2VmZmljaWVudCBtYXRyaXggJEEkIG11c3QgYmUgc3F1YXJlICgkbiBcdGltZXMgbiQpLgotIFRoZSBkZXRlcm1pbmFudCBvZiAkQSQsICRcZGV0KEEpJCwgbXVzdCBiZSBub24temVybyBmb3IgYSB1bmlxdWUgc29sdXRpb24gdG8gZXhpc3QuCgojIyMgRm9ybXVsYQpGb3IgZWFjaCB2YXJpYWJsZSAkeF9pJCwgcmVwbGFjZSB0aGUgJGkkLXRoIGNvbHVtbiBvZiAkQSQgd2l0aCB2ZWN0b3IgJGIkIGFuZCBjb21wdXRlOgoKJCQKeF9pID0gXGZyYWN7XGRldChBX2kpfXtcZGV0KEEpfQokJAoKV2hlcmU6Ci0gJEFfaSQgaXMgdGhlIG1hdHJpeCBmb3JtZWQgYnkgcmVwbGFjaW5nIHRoZSAkaSQtdGggY29sdW1uIG9mICRBJCB3aXRoICRiJAotICRcZGV0KEEpJCBpcyB0aGUgZGV0ZXJtaW5hbnQgb2YgdGhlIG9yaWdpbmFsIG1hdHJpeCAkQSQKCiMjIyBTdGVwcwoxLiBDb21wdXRlICRcZGV0KEEpJC4gSWYgaXQncyAwLCByZXR1cm4gLTEuCjIuIEZvciBlYWNoIHZhcmlhYmxlICR4X2kkOgogICAtIFJlcGxhY2UgY29sdW1uICRpJCBpbiAkQSQgd2l0aCAkYiQKICAgLSBDb21wdXRlICRcZGV0KEFfaSkkCiAgIC0gQ29tcHV0ZSAkeF9pID0gXGZyYWN7XGRldChBX2kpfXtcZGV0KEEpfSQKCiMjIyBFeGFtcGxlCkdpdmVuOgoKJCQKQSA9IFxiZWdpbntibWF0cml4fSAyICYgLTEgJiAzIFxcIDQgJiAyICYgMSBcXCAtNiAmIDEgJiAtMiBcZW5ke2JtYXRyaXh9LCBccXVhZCBiID0gXGJlZ2lue2JtYXRyaXh9IDUgXFwgMTAgXFwgLTMgXGVuZHtibWF0cml4fQokJAoKMS4gJFxkZXQoQSkgPSAtMzYuMCQKMi4gUmVwbGFjZSBlYWNoIGNvbHVtbiB3aXRoICRiJDoKCi0gJFxkZXQoQV8xKSA9IC02LjAkCi0gJFxkZXQoQV8yKSA9IC0xMjAuMCQKLSAkXGRldChBXzMpID0gLTk2LjAkCgpUaGVuLAoKJCQKeCA9IFxsZWZ0WyBcZnJhY3stNn17LTM2fSwgXGZyYWN7LTEyMH17LTM2fSwgXGZyYWN7LTk2fXstMzZ9IFxyaWdodF0gPSBbMC4xNjY3LCAzLjMzMzMsIDIuNjY2N10KJCQKCiMjIyBBcHBsaWNhdGlvbnMKLSBTb2x2aW5nIHNtYWxsIHN5c3RlbXMgb2YgZXF1YXRpb25zCi0gVXNlZnVsIGluIHRoZW9yZXRpY2FsIGxpbmVhciBhbGdlYnJhCi0gTm90IHByYWN0aWNhbCBmb3IgbGFyZ2UgbWF0cmljZXMgZHVlIHRvIGNvbXB1dGF0aW9uYWwgY29zdA==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def cramers_rule(self, A, b):\n        A = np.array(A, dtype=float)\n        b = np.array(b, dtype=float)\n\n        n, m = A.shape\n        if n != m or b.shape[0] != n:\n            return -1\n\n        det_A = np.linalg.det(A)\n        if np.isclose(det_A, 0):\n            return -1\n\n        x = np.zeros(n)\n        for i in range(n):\n            A_mod = A.copy()\n            A_mod[:, i] = b\n            det_A_mod = np.linalg.det(A_mod)\n            x[i] = det_A_mod / det_A\n\n        return x",
    "test_cases": [
      {
        "test": [
          "print(np.round(cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]), 4))"
        ],
        "expected_output": [
          0.1667,
          3.3333,
          2.6667
        ]
      },
      {
        "test": [
          "print(np.round(cramers_rule([[1, 2], [3, 4]], [5, 6]), 4))"
        ],
        "expected_output": [
          -4.0,
          4.5
        ]
      }
    ],
    "function_name": "cramers_rule"
  },
  {
    "id": 120,
    "title": "Bhattacharyya Distance Between Two Distributions",
    "difficulty": "easy",
    "category": "statistics",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBCaGF0dGFjaGFyeXlhIGRpc3RhbmNlIGJldHdlZW4gdHdvIHByb2JhYmlsaXR5IGRpc3RyaWJ1dGlvbnMuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSB0d28gbGlzdHMgcmVwcmVzZW50aW5nIGRpc2NyZXRlIHByb2JhYmlsaXR5IGRpc3RyaWJ1dGlvbnMgYHBgIGFuZCBgcWAsIGFuZCByZXR1cm4gdGhlIEJoYXR0YWNoYXJ5eWEgZGlzdGFuY2Ugcm91bmRlZCB0byA0IGRlY2ltYWwgcGxhY2VzLiBJZiB0aGUgaW5wdXRzIGhhdmUgZGlmZmVyZW50IGxlbmd0aHMgb3IgYXJlIGVtcHR5LCByZXR1cm4gMC4wLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def bhattacharyya_distance(self, p: list[float], q: list[float]) -> float:\n        # Your code here\n        pass",
    "example_input": "p = [0.1, 0.2, 0.3, 0.4], q = [0.4, 0.3, 0.2, 0.1]",
    "example_output": "0.1166",
    "example_reasoning": "The Bhattacharyya coefficient is calculated as the sum of element-wise square roots of the product of p and q, giving BC = 0.8898. The distance is then -log(0.8898) = 0.1166.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyBCaGF0dGFjaGFyeXlhIERpc3RhbmNlCgoqKkJoYXR0YWNoYXJ5eWEgRGlzdGFuY2UgKEJEKSoqIGlzIGEgY29uY2VwdCBpbiBzdGF0aXN0aWNzIHVzZWQgdG8gbWVhc3VyZSB0aGUgKipzaW1pbGFyaXR5Kiogb3IgKipvdmVybGFwKiogYmV0d2VlbiB0d28gcHJvYmFiaWxpdHkgZGlzdHJpYnV0aW9ucyAqKlAoeCkqKiBhbmQgKipRKHgpKiogb24gdGhlIHNhbWUgZG9tYWluICoqeCoqLiAgCgpUaGlzIGRpZmZlcnMgZnJvbSAqKktMIERpdmVyZ2VuY2UqKiwgd2hpY2ggbWVhc3VyZXMgdGhlICoqbG9zcyBvZiBpbmZvcm1hdGlvbioqIHdoZW4gcHJvamVjdGluZyBvbmUgcHJvYmFiaWxpdHkgZGlzdHJpYnV0aW9uIG9udG8gYW5vdGhlciAocmVmZXJlbmNlIGRpc3RyaWJ1dGlvbikuICAKCiMjIyAqKkJoYXR0YWNoYXJ5eWEgRGlzdGFuY2UgRm9ybXVsYSoqClRoZSBCaGF0dGFjaGFyeXlhIGRpc3RhbmNlIGlzIGRlZmluZWQgYXM6ICAKCiQkCkJDIChQLCBRKSA9IFxzdW0gXHNxcnR7UChYKSBcY2RvdCBRKFgpfQokJAoKJCQKQkQgKFAsIFEpID0gLVxsbihCQyAoUCwgUSkpCiQkCgp3aGVyZSAqKkJDIChQLCBRKSoqIGlzIHRoZSAqKkJoYXR0YWNoYXJ5eWEgY29lZmZpY2llbnQqKi4gIAoKIyMjICoqS2V5IFByb3BlcnRpZXMqKgoxLiAqKkJEIGlzIGFsd2F5cyBub24tbmVnYXRpdmUqKjogIAogICAkJCBCRCBcZ2VxIDAgJCQKMi4gKipTeW1tZXRyaWMgaW4gbmF0dXJlKio6ICAKICAgJCQgQkQgKFAsIFEpID0gQkQgKFEsIFApICQkCjMuICoqQXBwbGljYXRpb25zKio6ICAKICAgLSBSaXNrIGFzc2Vzc21lbnQgIAogICAtIFN0b2NrIHByZWRpY3Rpb25zICAKICAgLSBGZWF0dXJlIHNjYWxpbmcgIAogICAtIENsYXNzaWZpY2F0aW9uIHByb2JsZW1zICAKCiMjIyAqKkV4YW1wbGUgQ2FsY3VsYXRpb24qKgpDb25zaWRlciB0d28gcHJvYmFiaWxpdHkgZGlzdHJpYnV0aW9ucyAqKlAoeCkqKiBhbmQgKipRKHgpKio6ICAKCiQkClAoeCkgPSBbMC4xLCAwLjIsIDAuMywgMC40XSwgXHF1YWQgUSh4KSA9IFswLjQsIDAuMywgMC4yLCAwLjFdCiQkCgoxLiAqKkJoYXR0YWNoYXJ5eWEgQ29lZmZpY2llbnQqKjogIAoKJCQKQkMgKFAsIFEpID0gXHN1bSBcc3FydHtQKFgpIFxjZG90IFEoWCl9ID0gMC44ODk4CiQkCgoyLiAqKkJoYXR0YWNoYXJ5eWEgRGlzdGFuY2UqKjogIAoKJCQKQkQgKFAsIFEpID0gLVxsbihCQyAoUCwgUSkpID0gLVxsbigwLjg4OTgpID0gMC4xMTY2CiQkCgpUaGlzIGlsbHVzdHJhdGVzIGhvdyBCRCBxdWFudGlmaWVzIHRoZSAqKm92ZXJsYXAqKiBiZXR3ZWVuIHR3byBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb25zLiAg",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def bhattacharyya_distance(self, p : list[float], q : list[float]) -> float:\n        if len(p) != len(q):\n            return 0.0\n\n        p, q = np.array(p), np.array(q)\n        BC = np.sum(np.sqrt(p * q))\n        DB = -np.log(BC)\n        return round(DB, 4)",
    "test_cases": [
      {
        "test": [
          "print(round(bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]),4))"
        ],
        "expected_output": 0.1166
      },
      {
        "test": [
          "print(round(bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]),4))"
        ],
        "expected_output": 0.0541
      }
    ],
    "function_name": "bhattacharyya_distance"
  },
  {
    "id": 121,
    "title": "Vector Element-wise Sum",
    "difficulty": "easy",
    "category": "linear algebra",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgZWxlbWVudC13aXNlIHN1bSBvZiB0d28gdmVjdG9ycy4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gYSBuZXcgdmVjdG9yIHJlcHJlc2VudGluZyB0aGUgcmVzdWx0aW5nIHN1bSBpZiB0aGUgb3BlcmF0aW9uIGlzIHZhbGlkLCBvciAtMSBpZiB0aGUgdmVjdG9ycyBoYXZlIGluY29tcGF0aWJsZSBkaW1lbnNpb25zLiBUd28gdmVjdG9ycyAobGlzdHMpIGNhbiBiZSBzdW1tZWQgZWxlbWVudC13aXNlIG9ubHkgaWYgdGhleSBhcmUgb2YgdGhlIHNhbWUgbGVuZ3RoLg==",
    "starter_code": "\n\nclass Solution:\n    def vector_sum(self, a: list[int|float], b: list[int|float]) -> list[int|float]:\n    \t# Return the element-wise sum of vectors 'a' and 'b'.\n    \t# If vectors have different lengths, return -1.\n    \tpass",
    "example_input": "a = [1, 3], b = [4, 5]",
    "example_output": "[5, 8]",
    "example_reasoning": "Element-wise sum: [1+4, 3+5] = [5, 8].",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyBWZWN0b3IgRWxlbWVudC13aXNlIFN1bQoKSW4gbGluZWFyIGFsZ2VicmEsIHRoZSAqKmVsZW1lbnQtd2lzZSBzdW0qKiAoYWxzbyBrbm93biBhcyB2ZWN0b3IgYWRkaXRpb24pIGludm9sdmVzIGFkZGluZyBjb3JyZXNwb25kaW5nIGVudHJpZXMgb2YgdHdvIHZlY3RvcnMuCgojIyMgVmVjdG9yIE5vdGF0aW9uCkdpdmVuIHR3byB2ZWN0b3JzICRhJCBhbmQgJGIkIG9mIHRoZSBzYW1lIGRpbWVuc2lvbiAkbiQ6CgokJAphID0gXGJlZ2lue3BtYXRyaXh9IGFfMSBcXCBhXzIgXFwgXHZkb3RzIFxcIGFfbiBcZW5ke3BtYXRyaXh9LCBccXVhZCBiID0gXGJlZ2lue3BtYXRyaXh9IGJfMSBcXCBiXzIgXFwgXHZkb3RzIFxcIGJfbiBcZW5ke3BtYXRyaXh9CiQkCgpUaGUgZWxlbWVudC13aXNlIHN1bSBpcyBkZWZpbmVkIGFzOgoKJCQKYSArIGIgPSBcYmVnaW57cG1hdHJpeH0gYV8xICsgYl8xIFxcIGFfMiArIGJfMiBcXCBcdmRvdHMgXFwgYV9uICsgYl9uIFxlbmR7cG1hdHJpeH0KJCQKCiMjIyBLZXkgUmVxdWlyZW1lbnQKVmVjdG9ycyAkYSQgYW5kICRiJCBtdXN0IGJlIG9mIHRoZSBzYW1lIGxlbmd0aCAkbiQgZm9yIHRoZSBvcGVyYXRpb24gdG8gYmUgdmFsaWQuIElmIHRoZWlyIGxlbmd0aHMgZGlmZmVyLCBlbGVtZW50LXdpc2UgYWRkaXRpb24gaXMgbm90IGRlZmluZWQuCgojIyMgRXhhbXBsZQpMZXQ6CgokJAphID0gWzEsIDIsIDNdLCBccXVhZCBiID0gWzQsIDUsIDZdCiQkCgpUaGVuOgoKJCQKYSArIGIgPSBbMSs0LCAyKzUsIDMrNl0gPSBbNSwgNywgOV0KJCQKClRoaXMgc2ltcGxlIG9wZXJhdGlvbiBpcyBmb3VuZGF0aW9uYWwgaW4gbWFueSBhcHBsaWNhdGlvbnMgc3VjaCBhcyB2ZWN0b3IgYXJpdGhtZXRpYywgbmV1cmFsIG5ldHdvcmsgY29tcHV0YXRpb25zLCBhbmQgbGluZWFyIHRyYW5zZm9ybWF0aW9ucy4=",
    "solution_code": "\n\nclass Solution:\n    def vector_sum(self, a: list[int|float], b: list[int|float]) -> list[int|float]:\n        if len(a) != len(b):\n            return -1\n        return [a[i] + b[i] for i in range(len(a))]",
    "test_cases": [
      {
        "test": [
          [
            1,
            2,
            3
          ],
          [
            4,
            5,
            6
          ]
        ],
        "expected_output": [
          5,
          7,
          9
        ]
      },
      {
        "test": [
          [
            1,
            2
          ],
          [
            1,
            2,
            3
          ]
        ],
        "expected_output": -1
      }
    ],
    "function_name": "vector_sum"
  },
  {
    "id": 122,
    "title": "Policy Gradient with REINFORCE",
    "difficulty": "hard",
    "category": "reinforcement learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBwb2xpY3kgZ3JhZGllbnQgZXN0aW1hdG9yIHVzaW5nIHRoZSBSRUlORk9SQ0UgYWxnb3JpdGhtLiBUaGUgcG9saWN5IGlzIHBhcmFtZXRlcml6ZWQgYnkgYSAyRCBOdW1QeSBhcnJheSBgdGhldGFgIG9mIHNoYXBlIGAobnVtX3N0YXRlcywgbnVtX2FjdGlvbnMpYC4gVGhlIHBvbGljeSBmb3IgZWFjaCBzdGF0ZSBpcyBjb21wdXRlZCB2aWEgc29mdG1heCBvdmVyIGB0aGV0YVtzLCA6XWAuIEdpdmVuIGEgbGlzdCBvZiBlcGlzb2RlcyAoZWFjaCBhIGxpc3Qgb2YgKHN0YXRlLCBhY3Rpb24sIHJld2FyZCkgdHVwbGVzKSwgY29tcHV0ZSB0aGUgYXZlcmFnZSBncmFkaWVudCBvZiB0aGUgbG9nLXBvbGljeSBtdWx0aXBsaWVkIGJ5IHRoZSByZXR1cm4gYXQgZWFjaCB0aW1lIHN0ZXAu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def compute_policy_gradient(self, theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> np.ndarray:\n        \"\"\"\n        Estimate the policy gradient using REINFORCE.\n\n        Args:\n            theta: (num_states x num_actions) policy parameters.\n            episodes: List of episodes, where each episode is a list of (state, action, reward).\n\n        Returns:\n            Average policy gradient (same shape as theta).\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "theta = np.zeros((2,2)); episodes = [[(0,1,0), (1,0,1)], [(0,0,0)]]",
    "example_output": "[[-0.25, 0.25], [0.25, -0.25]]",
    "example_reasoning": "Episode 1 contributes a positive gradient from reward 1 at t=1; episode 2 adds zero. Result is averaged across episodes.",
    "learn_content": "IyMgUkVJTkZPUkNFIGFuZCBQb2xpY3kgR3JhZGllbnQgRXN0aW1hdGlvbgoKVGhlIFJFSU5GT1JDRSBhbGdvcml0aG0gY29tcHV0ZXMgZ3JhZGllbnRzIG9mIHRoZSBleHBlY3RlZCByZXR1cm4gd2l0aCByZXNwZWN0IHRvIHBvbGljeSBwYXJhbWV0ZXJzIHVzaW5nIE1vbnRlIENhcmxvIHNhbXBsZXMgb2YgZXBpc29kZXMuCgojIyMgU29mdG1heCBQb2xpY3kKR2l2ZW4gJFx0aGV0YSQgd2l0aCBzaGFwZSAobnVtX3N0YXRlcywgbnVtX2FjdGlvbnMpLCB3ZSBkZWZpbmUgdGhlIHByb2JhYmlsaXR5IG9mIGFjdGlvbiAkYSQgaW4gc3RhdGUgJHMkIGFzOgoKJCQKXHBpKGEgXG1pZCBzOyBcdGhldGEpID0gXGZyYWN7XGV4cChcdGhldGFbcywgYV0pfXtcc3VtX3thJ30gXGV4cChcdGhldGFbcywgYSddKX0KJCQKCiMjIyBSRUlORk9SQ0UgR3JhZGllbnQKRm9yIGFuIGVwaXNvZGUgd2l0aCB0cmFuc2l0aW9ucyAkKHNfdCwgYV90LCByX3QpJCBhbmQgcmV0dXJucyAkR190ID0gXHN1bV97az10fV5UIHJfayQ6CgokJApcbmFibGFfXHRoZXRhIEooXHRoZXRhKSBcYXBwcm94IFxzdW1fdCBcbmFibGFfXHRoZXRhIFxsb2cgXHBpKGFfdCBcbWlkIHNfdCkgXGNkb3QgR190CiQkCgojIyMgTG9nLVBvbGljeSBHcmFkaWVudApUbyBjb21wdXRlICRcbmFibGFfXHRoZXRhIFxsb2cgXHBpKGFfdCBcbWlkIHNfdCkkOgoKLSBGb3IgJFx0aGV0YVtzX3QsIGFfdF0kOiAkMSAtIFxwaShhX3QgXG1pZCBzX3QpJAotIEZvciAkXHRoZXRhW3NfdCwgYSddJCwgd2hlcmUgJGEnIFxuZXEgYV90JDogJC1ccGkoYScgXG1pZCBzX3QpJAotIEFsbCBvdGhlciBlbnRyaWVzOiAwCgojIyMgRmluYWwgRXN0aW1hdGUKRm9yIG11bHRpcGxlIGVwaXNvZGVzOgoKJCQKXGhhdHtcbmFibGF9X1x0aGV0YSBKKFx0aGV0YSkgPSBcZnJhY3sxfXtOfSBcc3VtX3tpPTF9Xk4gXHN1bV97dD0wfV57VF9pfSBcbmFibGFfXHRoZXRhIFxsb2cgXHBpKGFfdF5pIFxtaWQgc190XmkpIFxjZG90IEdfdF5pCiQkCgpUaGlzIGFsZ29yaXRobSB3b3JrcyBldmVuIHdpdGhvdXQgdmFsdWUgZnVuY3Rpb24gZXN0aW1hdGlvbiwgbWFraW5nIGl0IGEgZm91bmRhdGlvbmFsIG1ldGhvZCBpbiBwb2xpY3ktYmFzZWQgcmVpbmZvcmNlbWVudCBsZWFybmluZy4K",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def compute_policy_gradient(self, theta, episodes):\n        def softmax(self, x):\n            x = x - np.max(x)\n            exps = np.exp(x)\n            return exps / np.sum(exps)\n\n        grad = np.zeros_like(theta)\n        for episode in episodes:\n            rewards = [step[2] for step in episode]\n            returns = np.cumsum(rewards[::-1])[::-1]\n            for t, (s, a, _), G in zip(range(len(episode)), episode, returns):\n                probs = softmax(theta[s])\n                grad_log_pi = np.zeros_like(theta)\n                grad_log_pi[s, :] = -probs\n                grad_log_pi[s, a] += 1\n                grad += grad_log_pi * G\n        return grad / len(episodes)",
    "test_cases": [
      {
        "test": [
          "theta = np.zeros((2,2)); episodes = [[(0,1,0), (1,0,1)], [(0,0,0)]]; print(np.round(compute_policy_gradient(theta, episodes), 4))"
        ],
        "expected_output": "[[-0.25  0.25]\n [ 0.25 -0.25]]"
      },
      {
        "test": [
          "theta = np.zeros((2,2)); episodes = [[(0,0,0)], [(0,1,0), (1,1,0)]]; print(np.round(compute_policy_gradient(theta, episodes), 4))"
        ],
        "expected_output": "[[0. 0.]\n [0. 0.]]"
      }
    ],
    "function_name": "compute_policy_gradient"
  },
  {
    "id": 123,
    "title": "Calculate Computational Efficiency of MoE",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "Q2FsY3VsYXRlIHRoZSBjb21wdXRhdGlvbmFsIGNvc3Qgc2F2aW5ncyBvZiBhbiBNb0UgbGF5ZXIgY29tcGFyZWQgdG8gYSBkZW5zZSBsYXllciwgYXMgZGlzY3Vzc2VkIGluIHRoZSBwYXBlciAnT3V0cmFnZW91c2x5IExhcmdlIE5ldXJhbCBOZXR3b3JrczogVGhlIFNwYXJzZWx5LUdhdGVkIE1peHR1cmUtb2YtRXhwZXJ0cyBMYXllci4nIEdpdmVuIHRoZSBudW1iZXIgb2YgZXhwZXJ0cywgc3BhcnNpdHkgKG51bWJlciBvZiBhY3RpdmUgZXhwZXJ0cyksIGFuZCBpbnB1dC9vdXRwdXQgZGltZW5zaW9ucywgY29tcHV0ZSB0aGUgZmxvYXRpbmctcG9pbnQgb3BlcmF0aW9ucyAoRkxPUHMpIGZvciBib3RoIGFuZCBkZXRlcm1pbmUgdGhlIHNhdmluZ3MgcGVyY2VudGFnZS4=",
    "starter_code": "\n\nclass Solution:\n    def compute_efficiency(self, n_experts, k_active, d_in, d_out):\n        \"\"\"\n        Calculate computational savings of MoE vs. dense layer.\n\n        Args:\n            n_experts: Total number of experts\n            k_active: Number of active experts (sparsity)\n            d_in: Input dimension\n            d_out: Output dimension\n\n        Returns:\n            Percentage savings in FLOPs\n        \"\"\"\n        pass",
    "example_input": "compute_efficiency(1000, 2, 512, 512)",
    "example_output": "99.8",
    "example_reasoning": "Dense layer FLOPs: 1000 * 512 * 512 = 262,144,000. MoE FLOPs: 2 * 512 * 512 = 524,288. Savings:  ((262,144,000 - 524,288) / 262,144,000) x 100 \u2248 99.8%.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def compute_efficiency(self, n_experts, k_active, d_in, d_out):\n        dense_flops = n_experts * d_in * d_out\n        moe_flops = k_active * d_in * d_out\n        savings = (dense_flops - moe_flops) / dense_flops * 100\n        return round(savings, 1)",
    "test_cases": [
      {
        "test": [
          "print(round(compute_efficiency(1000, 2, 512, 512),1))"
        ],
        "expected_output": 99.8
      },
      {
        "test": [
          "print(round(compute_efficiency(10, 2, 256, 256),1))"
        ],
        "expected_output": 80.0
      }
    ],
    "function_name": "compute_efficiency"
  },
  {
    "id": 124,
    "title": "Implement the Noisy Top-K Gating Function",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBOb2lzeSBUb3AtSyBnYXRpbmcgbWVjaGFuaXNtIHVzZWQgaW4gTWl4dHVyZS1vZi1FeHBlcnRzIChNb0UpIG1vZGVscy4gR2l2ZW4gYW4gaW5wdXQgbWF0cml4LCB3ZWlnaHQgbWF0cmljZXMsIHByZS1zYW1wbGVkIG5vaXNlLCBhbmQgYSBzcGFyc2l0eSBjb25zdHJhaW50IGssIGNvbXB1dGUgdGhlIGZpbmFsIGdhdGluZyBwcm9iYWJpbGl0aWVzIG1hdHJpeC4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def noisy_topk_gating(self, \n        X: np.ndarray,\n        W_g: np.ndarray,\n        W_noise: np.ndarray,\n        N: np.ndarray,\n        k: int\n    ) -> np.ndarray:\n        \"\"\"\n        Args:\n            X: Input data, shape (batch_size, features)\n            W_g: Gating weight matrix, shape (features, num_experts)\n            W_noise: Noise weight matrix, shape (features, num_experts)\n            N: Noise samples, shape (batch_size, num_experts)\n            k: Number of experts to keep per example\n        Returns:\n            Gating probabilities, shape (batch_size, num_experts)\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "X = [[1.0, 2.0]]\nW_g = [[1.0, 0.0], [0.0, 1.0]]\nW_noise = [[0.5, 0.5], [0.5, 0.5]]\nN = [[1.0, -1.0]]\nk = 2",
    "example_output": "[[0.917, 0.0825]]",
    "example_reasoning": "This example demonstrates that the gating function produces a sparse softmax output, favoring the higher gate after noise perturbation.",
    "learn_content": "IyMgTm9pc3kgVG9wLUsgR2F0aW5nCgpOb2lzeSBUb3AtSyBHYXRpbmcgaXMgYSBzcGFyc2Ugc2VsZWN0aW9uIG1lY2hhbmlzbSB1c2VkIGluIE1peHR1cmUtb2YtRXhwZXJ0cyAoTW9FKSBtb2RlbHMuIEl0IHJvdXRlcyBpbnB1dCB0b2tlbnMgdG8gYSBzdWJzZXQgb2YgYXZhaWxhYmxlIGV4cGVydHMsIGVuaGFuY2luZyBlZmZpY2llbmN5IGFuZCBtb2RlbCBjYXBhY2l0eS4KCiMjIyBPdmVydmlldwoKVGhlIGNvcmUgaWRlYSBpcyB0byBhZGQgbGVhcm5lZCBub2lzZSB0byB0aGUgZ2F0aW5nIGxvZ2l0cyBhbmQgdGhlbiBzZWxlY3Qgb25seSB0aGUgdG9wLWsgZXhwZXJ0cyBmb3IgZWFjaCBpbnB1dC4gVGhpcyBlbmNvdXJhZ2VzIGV4cGxvcmF0aW9uIGFuZCBoZWxwcyBiYWxhbmNlIGxvYWQgYWNyb3NzIGV4cGVydHMuCgojIyMgU3RlcC1ieS1TdGVwIEJyZWFrZG93bgoKMS4gKipDb21wdXRlIFJhdyBHYXRlIFNjb3JlcyoqICAKICAgRmlyc3QsIGNvbXB1dGUgdHdvIGxpbmVhciBwcm9qZWN0aW9ucyBvZiB0aGUgaW5wdXQ6CiAgICQkCiAgIEhfe1x0ZXh0e2Jhc2V9fSA9IFggV19nCiAgICQkCiAgICQkCiAgIEhfe1x0ZXh0e25vaXNlfX0gPSBYIFdfe1x0ZXh0e25vaXNlfX0KICAgJCQKCjIuICoqQXBwbHkgTm9pc2Ugd2l0aCBTb2Z0cGx1cyBTY2FsaW5nKiogIAogICBBZGQgcHJlLXNhbXBsZWQgR2F1c3NpYW4gbm9pc2UsIHNjYWxlZCBieSBhIHNvZnRwbHVzIHRyYW5zZm9ybWF0aW9uOgogICAkJAogICBIID0gSF97XHRleHR7YmFzZX19ICsgTiBcb2RvdCBcdGV4dHtTb2Z0cGx1c30oSF97XHRleHR7bm9pc2V9fSkKICAgJCQKCjMuICoqVG9wLUsgTWFza2luZyoqICAKICAgS2VlcCBvbmx5IHRoZSB0b3AtayBlbGVtZW50cyBpbiBlYWNoIHJvdyAoaS5lLiwgcGVyIGlucHV0KSwgc2V0dGluZyB0aGUgcmVzdCB0byAkLVxpbmZ0eSQ6CiAgICQkCiAgIEgnID0gXHRleHR7VG9wS30oSCwgaykKICAgJCQKCjQuICoqU29mdG1heCBPdmVyIFRvcC1LKiogIAogICBOb3JtYWxpemUgdGhlIHRvcC1rIHNjb3JlcyBpbnRvIGEgdmFsaWQgcHJvYmFiaWxpdHkgZGlzdHJpYnV0aW9uOgogICAkJAogICBHID0gXHRleHR7U29mdG1heH0oSCcpCiAgICQkCgojIyMgV29ya2VkIEV4YW1wbGUKCkxldDoKLSAkWCA9IFtbMS4wLCAyLjBdXSQKLSAkV19nID0gW1sxLjAsIDAuMF0sIFswLjAsIDEuMF1dJAotICRXX3tcdGV4dHtub2lzZX19ID0gW1swLjUsIDAuNV0sIFswLjUsIDAuNV1dJAotICROID0gW1sxLjAsIC0xLjBdXSQKLSAkayA9IDIkCgpTdGVwLWJ5LXN0ZXA6Ci0gJEhfe1x0ZXh0e2Jhc2V9fSA9IFsxLjAsIDIuMF0kCi0gJEhfe1x0ZXh0e25vaXNlfX0gPSBbMS41LCAxLjVdJAotICRcdGV4dHtTb2Z0cGx1c30oSF97XHRleHR7bm9pc2V9fSkgXGFwcHJveCBbMS44MDQsIDEuODA0XSQKLSAkSCA9IFsxLjAgKyAxLjgwNCwgMi4wIC0gMS44MDRdID0gWzIuODA0LCAwLjE5Nl0kCi0gU29mdG1heCBvdmVyIHRoZXNlIGdpdmVzOiAkWzAuOTE3LCAwLjA4MjVdJAoKIyMjIEJlbmVmaXRzCgotICoqQ29tcHV0YXRpb25hbCBFZmZpY2llbmN5Kio6IEFjdGl2YXRlcyBvbmx5IGsgZXhwZXJ0cyBwZXIgaW5wdXQuCi0gKipMb2FkIEJhbGFuY2luZyoqOiBJbmplY3RlZCBub2lzZSBlbmNvdXJhZ2VzIGRpdmVyc2l0eSBpbiBleHBlcnQgc2VsZWN0aW9uLgotICoqSW1wcm92ZWQgR2VuZXJhbGl6YXRpb24qKjogQWN0cyBhcyBhIHJlZ3VsYXJpemVyIHZpYSBub2lzZS1iYXNlZCBnYXRpbmcuCgpUaGlzIHRlY2huaXF1ZSBpcyB1c2VkIGluIGxhcmdlIHNwYXJzZSBtb2RlbHMgbGlrZSBHU2hhcmQgYW5kIFN3aXRjaCBUcmFuc2Zvcm1lcnMu",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def noisy_topk_gating(self, \n        X: np.ndarray,\n        W_g: np.ndarray,\n        W_noise: np.ndarray,\n        N: np.ndarray,\n        k: int\n    ) -> np.ndarray:\n        H_base = X @ W_g\n        H_noise = X @ W_noise\n        softplus = np.log1p(np.exp(H_noise))\n        H = H_base + N * softplus\n\n        def top_k_masked(self, row, k):\n            mask = np.full_like(row, -np.inf)\n            top_idx = np.argsort(row)[-k:]\n            mask[top_idx] = row[top_idx]\n            return mask\n\n        masked_H = np.vstack([top_k_masked(row, k) for row in H])\n        exps = np.exp(masked_H - np.max(masked_H, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(0)\nX = np.array([[1.0, 2.0]])\nW_g = np.array([[1.0, 0.0], [0.0, 1.0]])\nW_noise = np.zeros((2,2))\nN = np.zeros((1,2))\nprint(noisy_topk_gating(X, W_g, W_noise, N, k=1))"
        ],
        "expected_output": "[[0. 1.]]"
      },
      {
        "test": [
          "import numpy as np\nX = np.array([[1.0, 2.0]])\nW_g = np.array([[1.0, 0.0], [0.0, 1.0]])\nW_noise = np.array([[0.5, 0.5], [0.5, 0.5]])\nN = np.array([[1.0, -1.0]])\nprint(np.round(noisy_topk_gating(X, W_g, W_noise, N, k=2), 4))"
        ],
        "expected_output": [
          [
            0.917,
            0.083
          ]
        ]
      }
    ],
    "function_name": "noisy_topk_gating"
  },
  {
    "id": 125,
    "title": "Implement a Sparse Mixture of Experts Layer",
    "difficulty": "hard",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgTWl4dHVyZS1vZi1FeHBlcnRzIChNb0UpIGxheWVyIHVzaW5nIHNvZnRtYXggZ2F0aW5nIGFuZCB0b3AtayByb3V0aW5nLiBHaXZlbiBhbiBpbnB1dCB0ZW5zb3IsIGEgc2V0IG9mIGV4cGVydCB3ZWlnaHQgbWF0cmljZXMsIGEgZ2F0aW5nIHdlaWdodCBtYXRyaXgsIGFuZCBwYXJhbWV0ZXJzIHNwZWNpZnlpbmcgdGhlIG51bWJlciBvZiBleHBlcnRzIGFuZCB0aGUgdmFsdWUgb2YgaywgY29tcHV0ZSB0aGUgZmluYWwgTW9FIG91dHB1dCBieSBzZWxlY3RpbmcgdGhlIHRvcC1rIGV4cGVydHMgcGVyIHRva2VuLCBhcHBseWluZyB0aGVpciB0cmFuc2Zvcm1hdGlvbnMsIGFuZCBhZ2dyZWdhdGluZyB0aGUgcmVzdWx0cyB3ZWlnaHRlZCBieSB0aGUgbm9ybWFsaXplZCBnYXRpbmcgcHJvYmFiaWxpdGllcy4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def moe(self, x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> np.ndarray:\n        \"\"\"\n        Args:\n            x: Input tensor of shape (n_batch, l_seq, d_model)\n            We: Expert weights of shape (n_experts, d_model, d_model)\n            Wg: Gating weights of shape (d_model, n_experts)\n            n_experts: Number of experts\n            top_k: Number of experts to route each token to\n        Returns:\n            Output tensor of shape (n_batch, l_seq, d_model)\n        \"\"\"\n        pass",
    "example_input": "x = np.arange(12).reshape(2, 3, 2)\nWe = np.ones((4, 2, 2))\nWg = np.ones((2, 4))\ntop_k = 1",
    "example_output": "[[[1, 1], [5, 5], [9, 9]], [[13, 13], [17, 17], [21, 21]]]",
    "example_reasoning": "Each token is routed to its top expert and processed using a weight matrix of ones. The result matches the input tokens due to identity transformation and weight 1.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def softmax(self, x: np.ndarray, axis: int = -1) -> np.ndarray:\n        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\n    def get_top_k(self, arr: np.ndarray, k: int):\n        idx = np.argpartition(arr, -k)[..., -k:]\n        vals = np.take_along_axis(arr, idx, axis=-1)\n        return idx, vals\n\n    def expert(self, x: np.ndarray, We_i: np.ndarray):\n        # x: [n_tokens, d_model]\n        # We_i: [d_model, d_model]\n        return x @ We_i\n\n    def gate(self, x: np.ndarray, Wg: np.ndarray):\n        # x: [n_batch * l_seq, d_model]\n        # Wg: [n_batch * l_seq, n_experts]\n        return x @ Wg\n\n    def moe(self, x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n        # x: [n_batch, l_seq, d_model]\n        # We: [n_experts, d_model, d_model]\n        # Wg: [n_batch * l_seq, n_experts]\n\n        n_batch, l_seq, d_model = x.shape\n\n        # flatten batch and sequence dimensions for easier indexing\n        # x_flat: [n_batch * l_seq, d_model]\n        x_flat = x.reshape(-1, d_model)\n        n_tokens, _ = x_flat.shape\n\n        gating_logits = gate(x_flat, Wg)\n        gating_weights = softmax(gating_logits, axis=-1)\n\n        topk_idx, topk_weights = get_top_k(gating_weights, top_k)\n        topk_idx_flat = topk_idx.flatten()  # [n_tokens * top_k]\n        # mapping from top K expert indices to token indices: [n_tokens * top_k]\n        token_idx_flat = np.arange(n_tokens).repeat(top_k)\n\n        topk_weights_norm = topk_weights / topk_weights.sum(axis=1, keepdims=True)\n        topk_weights_norm_flat = topk_weights_norm.flatten()\n\n        # prepare result memory for aggregation: [n_tokens, d_model]\n        output_flat = np.zeros_like(x_flat)\n        for i in range(n_experts):\n            mask = topk_idx_flat == i\n            tokens_expert_i = token_idx_flat[mask]\n\n            if tokens_expert_i.size > 0:\n                x_expert_i = x_flat[tokens_expert_i]\n                output_expert_i = expert(x_expert_i, We[i, ...])\n                output_expert_i *= topk_weights_norm_flat[mask, None]\n\n                # scatter add to result memory\n                np.add.at(output_flat, tokens_expert_i, output_expert_i)\n\n        return output_flat.reshape(n_batch, l_seq, d_model)\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nd_model = 2\nn_experts = 4\nl_seq = 3\nn_batch = 2\ntop_k = 2\nx = np.random.rand(n_batch, l_seq, d_model)\nWe = np.random.rand(n_experts, d_model, d_model)\nWg = np.random.rand(d_model, n_experts)\noutput = moe(x, We, Wg, n_experts, top_k)\nprint(np.round(output, 4))"
        ],
        "expected_output": "[[[0.5148 0.4329]\n  [0.5554 0.5447]\n  [0.1285 0.102 ]]\n\n [[0.339  0.3046]\n  [0.5391 0.417 ]\n  [0.3597 0.3262]]]"
      },
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nd_model = 2\nn_experts = 4\nl_seq = 3\nn_batch = 2\ntop_k = 2\nx = np.random.rand(n_batch, l_seq, d_model)\nWe = np.zeros((n_experts, d_model, d_model))\nWg = np.random.rand(d_model, n_experts)\noutput = moe(x, We, Wg, n_experts, top_k)\nprint(output)"
        ],
        "expected_output": "[[[0. 0.]\n  [0. 0.]\n  [0. 0.]]\n\n [[0. 0.]\n  [0. 0.]\n  [0. 0.]]]"
      }
    ],
    "function_name": "moe"
  },
  {
    "id": 126,
    "title": "Implement Group Normalization",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gcGVyZm9ybSBHcm91cCBOb3JtYWxpemF0aW9uIG9uIGEgNEQgaW5wdXQgdGVuc29yIHdpdGggc2hhcGUgKEIsIEMsIEgsIFcpLiBUaGUgZnVuY3Rpb24gc2hvdWxkIG5vcm1hbGl6ZSBvdmVyIHNtYWxsZXIgZ3JvdXBzIG9mIGNoYW5uZWxzLCB0aGVuIGFwcGx5IGEgbGVhcm5lZCBzY2FsZSAoZ2FtbWEpIGFuZCBzaGlmdCAoYmV0YSku",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def group_normalization(self, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5) -> np.ndarray:\n        # Your code here\n        pass",
    "example_input": "X.shape = (2, 2, 2, 2), gamma = [1, 1], beta = [0, 0], num_groups = 2",
    "example_output": "Normalized tensor where each group is independently normalized and scaled by gamma and shifted by beta.",
    "example_reasoning": "First split the channels into groups, compute mean and variance per group, normalize within the group, then scale and shift with gamma and beta.",
    "learn_content": "## Understanding Group Normalization

Group Normalization (GN) is a normalization technique that divides the channels into groups and normalizes the activations within each group. Unlike Batch Normalization, which normalizes over the entire mini-batch, Group Normalization normalizes over groups of channels and is less dependent on the batch size. This makes it particularly useful for tasks with small batch sizes or when using architectures such as segmentation networks where spatial resolution is important.

### Concepts

Group Normalization operates on the principle of normalizing within smaller groups of channels. The process reduces **internal covariate shift** within these groups and helps stabilize training, especially in scenarios where the batch size is small or varies across tasks.

The process of Group Normalization consists of the following steps:

1. **Divide Channels into Groups:** Split the feature channels into several groups. The number of groups is determined by the **n_groups** parameter.
2. **Compute the Mean and Variance within Each Group:** For each group, compute the mean and variance of the activations within the group, across the spatial dimensions and batch.
3. **Normalize the Inputs:** Normalize the activations of each group using the computed mean and variance.
4. **Apply Scale and Shift:** After normalization, apply a learned scale (gamma) and shift (beta) to restore the model's ability to represent the data's original distribution.

### Structure of Group Normalization for BCHW Input

For an input tensor with the shape **BCHW** , where:
- **B**: batch size,
- **C**: number of channels,
- **H**: height,
- **W**: width,
the Group Normalization process operates on specific dimensions based on the task's requirement.

#### 1. Group Division

- The input feature dimension **C** (channels) is divided into several groups. The number of groups is determined by the **n_groups** parameter, and the size of each group is calculated as:

  $$
  \text{groupSize} = \frac{C}{n_{\text{groups}}}
  $$

  Where:
  - **C** is the number of channels.
  - **n_groups** is the number of groups into which the channels are divided.
  - **groupSize** is the number of channels in each group.

  The input tensor is then reshaped to group the channels into the specified groups.

#### 2. Mean and Variance Calculation within Groups

- For each group, the **mean** $\mu_g$ and **variance** $\sigma_g^2$ are computed over the spatial dimensions and across the batch. This normalization helps to stabilize the activations within each group.

  $$ 
  \mu_g = \frac{1}{B \cdot H \cdot W \cdot \text{groupSize}} \sum_{i=1}^{B} \sum_{h=1}^{H} \sum_{w=1}^{W} \sum_{g=1}^{\text{groupSize}} x_{i,g,h,w}
  $$

  $$
  \sigma_g^2 = \frac{1}{B \cdot H \cdot W \cdot \text{groupSize}} \sum_{i=1}^{B} \sum_{h=1}^{H} \sum_{w=1}^{W} \sum_{g=1}^{\text{groupSize}} (x_{i,g,h,w} - \mu_g)^2
  $$

  Where:
  - $x_{i,g,h,w}$ is the activation at batch index $i$, group index $g$, height $h$, and width $w$.
  - $B$ is the batch size.
  - $H$ and $W$ are the spatial dimensions (height and width).
  - $\text{groupSize}$ is the number of channels in each group.

#### 3. Normalization

Once the mean $\mu_g$ and variance $\sigma_g^2$ have been computed for each group, the next step is to **normalize** the input. The normalization is done by subtracting the mean and dividing by the standard deviation (square root of the variance, plus a small constant $\epsilon$ for numerical stability):

$$
\hat{x}_{i,g,h,w} = \frac{x_{i,g,h,w} - \mu_g}{\sqrt{\sigma_g^2 + \epsilon}}
$$

Where:
- $\hat{x}_{i,g,h,w}$ is the normalized activation for the input at batch index $i$, group index $g$, height $h$, and width $w$.
- $\epsilon$ is a small constant to avoid division by zero.

#### 4. Scale and Shift

After normalization, the next step is to apply a **scale** ($\gamma_g$) and **shift** ($\beta_g$) to the normalized activations for each group. These learned parameters allow the model to adjust the output distribution of each group:

$$
y_{i,g,h,w} = \gamma_g \hat{x}_{i,g,h,w} + \beta_g
$$

Where:
- $\gamma_g$ is the scaling factor for group $g$.
- $\beta_g$ is the shifting factor for group $g$.

#### 5. Training and Inference

- **During Training**: The mean and variance are computed for each mini-batch and used for normalization within each group.
- **During Inference**: The model uses running averages of the statistics (mean and variance) that were computed during training to ensure consistent behavior when deployed.

### Key Points

- **Group-wise Normalization**: Group Normalization normalizes within smaller groups of channels instead of normalizing over the entire batch and all channels. This allows for more stable training in cases with small batch sizes.

- **Number of Groups**: The number of groups is a hyperparameter (**n_groups**) that can significantly affect the model’s performance. It is typically set to divide the total number of channels into groups of equal size.

- **Smaller Batch Sizes**: Group Normalization is less dependent on the batch size, making it ideal for situations where batch sizes are small (e.g., segmentation tasks).

- **Numerical Stability**: As with other normalization techniques, a small constant $\epsilon$ is added to the variance to avoid numerical instability when dividing by the square root of variance.

- **Improved Convergence**: Group Normalization can help improve the gradient flow throughout the network, making it easier to train deep networks with small batch sizes. It also helps speed up convergence and stabilize training.

- **Regularization Effect**: Similar to Batch Normalization, Group Normalization introduces a form of regularization through the normalization process. It can reduce overfitting by acting as a noise source during training.

### Why Normalize Over Groups?

- **Group-wise Normalization**: By dividing the channels into smaller groups, Group Normalization ensures that each group has a stable distribution of activations, making it effective even when batch sizes are small.

- **Less Dependency on Batch Size**: Unlike Batch Normalization, Group Normalization does not require large batch sizes to compute accurate statistics. This makes it well-suited for tasks such as image segmentation, where large batch sizes may not be feasible.

- **Channel-wise Learning**: Group Normalization allows each group to learn independently, preserving flexibility while also controlling the complexity of normalization over channels.

By normalizing over smaller groups, Group Normalization can reduce internal covariate shift and allow for faster and more stable training, even in situations where Batch Normalization may be less effective due to small batch sizes.",
    "solution_code": "\n\nclass Solution:\n    def group_normalization(self, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5) -> np.ndarray:\n        '''\n        Perform Group Normalization.\n\n        Args:\n        X: numpy array of shape (B, C, H, W), input data\n        gamma: numpy array of shape (C,), scale parameter\n        beta: numpy array of shape (C,), shift parameter\n        num_groups: number of groups for normalization\n        epsilon: small constant to avoid division by zero\n\n        Returns:\n        norm_X: numpy array of shape (B, C, H, W), normalized output\n        '''\n        batch_size, num_channels, height, width = X.shape\n        group_size = num_channels // num_groups\n\n        # Reshape X into groups\n        X_reshaped = X.reshape(batch_size, num_groups, group_size, height, width)\n\n        # Compute mean and variance for each group\n        mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n        variance = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n\n        X_norm = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n\n        # Reshape back to the original shape\n        X_norm = X_norm.reshape(batch_size, num_channels, height, width)\n\n        # Apply scale and shift\n        norm_X = gamma * X_norm + beta\n        return norm_X",
    "test_cases": [
      {
        "test": [
          "np.random.seed(42)\nB, C, H, W = 2, 2, 2, 2\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C).reshape(1, C, 1, 1)\nbeta = np.zeros(C).reshape(1, C, 1, 1)\nnum_groups = 2\noutput = group_normalization(X, gamma, beta, num_groups)\nprint(np.round(output, 4))"
        ],
        "expected_output": [
          [
            [
              [
                -0.2287,
                -1.2998
              ],
              [
                0.026,
                1.5025
              ]
            ],
            [
              [
                -0.926,
                -0.9259
              ],
              [
                1.46,
                0.3919
              ]
            ]
          ],
          [
            [
              [
                -0.5848,
                1.732
              ],
              [
                -0.5709,
                -0.5762
              ]
            ],
            [
              [
                1.4005,
                -1.0503
              ],
              [
                -0.8361,
                0.486
              ]
            ]
          ]
        ]
      },
      {
        "test": [
          "np.random.seed(42)\nB, C, H, W = 2, 2, 2, 1\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C).reshape(1, C, 1, 1)\nbeta = np.zeros(C).reshape(1, C, 1, 1)\nnum_groups = 2\noutput = group_normalization(X, gamma, beta, num_groups)\nprint(np.round(output, 4))"
        ],
        "expected_output": [
          [
            [
              [
                1.0
              ],
              [
                -1.0
              ]
            ],
            [
              [
                -1.0
              ],
              [
                1.0
              ]
            ]
          ],
          [
            [
              [
                -0.0026
              ],
              [
                0.0026
              ]
            ],
            [
              [
                1.0
              ],
              [
                -1.0
              ]
            ]
          ]
        ]
      }
    ],
    "function_name": "group_normalization"
  },
  {
    "id": 127,
    "title": "Find Captain Redbeard's Hidden Treasure",
    "difficulty": "medium",
    "category": "calculus",
    "problem_description": "Q2FwdGFpbiBSZWRiZWFyZCwgdGhlIG1vc3QgZGFyaW5nIHBpcmF0ZSBvZiB0aGUgc2V2ZW4gc2VhcywgaGFzIHVuY292ZXJlZCBhIG15c3RlcmlvdXMgYW5jaWVudCBtYXAuIEluc3RlYWQgb2YgaXNsYW5kcywgaXQgc2hvd3MgYSBzdHJhbmdlIHdhdnkgY3VydmUsIGFuZCB0aGUgdHJlYXN1cmUgbGllcyBhdCB0aGUgbG93ZXN0IHBvaW50IG9mIHRoZSBsYW5kISAod2F0Y2ggb3V0IGZvciB0aG9zZSB0cmlja3kgbG9jYWwgbWlucykKClRoZSBsYW5kJ3MgaGVpZ2h0IGF0IGFueSBwb2ludCAkeCQgaXMgZ2l2ZW4gYnk6CgpmKHgpID0geF40IC0gM3heMyArIDIKCgoqKllvdXIgTWlzc2lvbjoqKgpJbXBsZW1lbnQgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBmaW5kcyB0aGUgdmFsdWUgb2YgJHgkIHdoZXJlICRmKHgpJCByZWFjaGVzIGl0cyBtaW5pbXVtLCBzdGFydGluZyBmcm9tIGFueSByYW5kb20gaW5pdGlhbCBwb3NpdGlvbi4=",
    "starter_code": "\n\nclass Solution:\n    def find_treasure(self, start_x: float) -> float:\n        \"\"\"\n        Find the x-coordinate where f(x) = x^4 - 3x^3 + 2 is minimized.\n\n      Returns:\n            float: The x-coordinate of the minimum point.\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "start_x = 0.0",
    "example_output": "min float value",
    "example_reasoning": "Cant really give you a example without giving the solution... so ya",
    "learn_content": "IyMgSG93IHRvIEZpbmQgdGhlIE1pbmltdW0gb2YgYSBGdW5jdGlvbgoKVG8gZmluZCB0aGUgbWluaW11bSBvZiBhIGZ1bmN0aW9uIGxpa2UKCiQkCmYoeCkgPSB4XjQgLSAzeF4zICsgMgokJAoKd2UgY2FuIHVzZSBhIHRlY2huaXF1ZSBjYWxsZWQgKipncmFkaWVudCBkZXNjZW50KiouCgojIyMgU3RlcHM6CgoxLiAqKkZpbmQgdGhlIERlcml2YXRpdmUqKgogICAtIFRoZSBkZXJpdmF0aXZlIChzbG9wZSkgdGVsbHMgdXMgd2hpY2ggZGlyZWN0aW9uIHRoZSBmdW5jdGlvbiBpcyBpbmNyZWFzaW5nIG9yIGRlY3JlYXNpbmcuCiAgIC0gRm9yIHRoaXMgcHJvYmxlbSwgdGhlIGRlcml2YXRpdmUgaXM6CiAgICAgJCQKICAgICBmJyh4KSA9IDR4XjMgLSA5eF4yCiAgICAgJCQKCjIuICoqTW92ZSBPcHBvc2l0ZSB0aGUgU2xvcGUqKgogICAtIElmIHRoZSBzbG9wZSBpcyBwb3NpdGl2ZSwgbW92ZSBsZWZ0LgogICAtIElmIHRoZSBzbG9wZSBpcyBuZWdhdGl2ZSwgbW92ZSByaWdodC4KICAgLSBVcGRhdGUgdGhlIHBvc2l0aW9uIGJ5OgogICAgICQkCiAgICAgeF97bmV3fSA9IHhfe29sZH0gLSBcdGV4dHtsZWFybmluZyByYXRlfSBcdGltZXMgZicoeF97b2xkfSkKICAgICAkJAoKMy4gKipSZXBlYXQqKgogICAtIEtlZXAgdXBkYXRpbmcgJHgkIHVudGlsIHRoZSBjaGFuZ2UgaXMgdmVyeSBzbWFsbCAoYmVsb3cgYSB0b2xlcmFuY2UpLgoKIyMjIFdoeSBEb2VzIFRoaXMgV29yaz8KLSBJZiB5b3UgYWx3YXlzIG1vdmUgZG93bmhpbGwgYWxvbmcgdGhlIHNsb3BlLCB5b3UgZXZlbnR1YWxseSByZWFjaCBhIGJvdHRvbSBhIGxvY2FsIG1pbmltdW0uCgojIyMgSW1wb3J0YW50IFRlcm1zCi0gKipMZWFybmluZyBSYXRlKio6IEhvdyBiaWcgYSBzdGVwIHRvIHRha2UgZWFjaCB1cGRhdGUuCi0gKipUb2xlcmFuY2UqKjogSG93IGNsb3NlIHN1Y2Nlc3NpdmUgc3RlcHMgbXVzdCBiZSB0byBzdG9wLgotICoqTG9jYWwgTWluaW11bSoqOiBBIHBvaW50IHdoZXJlIHRoZSBmdW5jdGlvbiB2YWx1ZSBpcyBsb3dlciB0aGFuIG5lYXJieSBwb2ludHMuCgpJbiB0aGlzIHByb2JsZW0sIENhcHRhaW4gUmVkYmVhcmQgZmluZHMgdGhlIGhpZGRlbiB0cmVhc3VyZSBieSBtb3ZpbmcgZG93bmhpbGwgdW50aWwgaGUgcmVhY2hlcyB0aGUgbG93ZXN0IHBvaW50IQ==",
    "solution_code": "\n\nclass Solution:\n    def find_treasure(self, start_x: float) -> float:\n    \tlearning_rate = 0.1, tolerance = 1e-6, max_iters = 10000)    def gradient(x):\n            return 4 * x**3 - 9 * x**2  # derivative of x^4 - 3x^3 + 2\n\n        x = start_x\n        for _ in range(max_iters):\n            grad = gradient(x)\n            new_x = x - learning_rate * grad\n            if abs(new_x - x) < tolerance:\n                break\n            x = new_x\n        return round(x, 4)",
    "test_cases": [
      {
        "test": [
          "print(abs(find_treasure(-1) - 2.25) < 1e-2)"
        ],
        "expected_output": "True"
      },
      {
        "test": [
          "print(abs(find_treasure(1.0) - 2.25) < 1e-2)"
        ],
        "expected_output": "True"
      }
    ],
    "function_name": "find_treasure"
  },
  {
    "id": 128,
    "title": "Dynamic Tanh: Normalization-Free Transformer Activation",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBEeW5hbWljIFRhbmggKER5VCkgZnVuY3Rpb24sIGEgbm9ybWFsaXphdGlvbi1mcmVlIHRyYW5zZm9ybWF0aW9uIGluc3BpcmVkIGJ5IHRoZSBUYW5oIGZ1bmN0aW9uLiBEeVQgcmVwbGFjZXMgbGF5ZXIgbm9ybWFsaXphdGlvbiBpbiBUcmFuc2Zvcm1lciBhcmNoaXRlY3R1cmVzIHdoaWxlIHByZXNlcnZpbmcgc3F1YXNoaW5nIGJlaGF2aW9yIGFuZCBlbmFibGluZyBzdGFibGUgdHJhaW5pbmcu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def dynamic_tanh(self, x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n        # Your code here\n        pass",
    "example_input": "x = np.array([[[0.14115588, 0.00372817, 0.24126647, 0.22183601]]])\ngamma = np.ones((4,))\nbeta = np.zeros((4,))\nalpha = 0.5\nprint(dynamic_tanh(x, alpha, gamma, beta))",
    "example_output": "[[[0.0705, 0.0019, 0.1201, 0.1105]]]",
    "example_reasoning": "Each element in the input is scaled by alpha, passed through tanh, and then scaled by gamma and shifted by beta. This mimics the squashing behavior of layer normalization without explicitly using statistics.",
    "learn_content": "QSBuZXcgc3R1ZHkgKGh0dHBzOi8vYXJ4aXYub3JnL3BkZi8yNTAzLjEwNjIyKSBkZW1vbnN0cmF0ZXMgdGhhdCBsYXllciBub3JtYWxpemF0aW9uLCB0aGF0IGlzIHViaXF1aXRvdXMgaW4gdHJhbnNmb3JtZXJzLCBwcm9kdWNlcyBUYW5oLWxpa2UgUy1zaGFwZXMuIEJ5IGluY29ycG9yYXRpbmcgYSBuZXcgbGF5ZXIgcmVwbGFjZW1lbnQgZm9yIG5vcm1hbGl6YXRpb24gY2FsbGVkICJEeW5hbWljIFRhbmgiIChEeVQgZm9yIHNob3J0KSwgVHJhbnNmb3JtZXJzIHdpdGhvdXQgbm9ybWFsaXphdGlvbiBjYW4gbWF0Y2ggb3IgZXhjZWVkIHRoZSBwZXJmb3JtYW5jZSBvZiB0aGVpciBub3JtYWxpemVkIGNvdW50ZXJwYXJ0cywgbW9zdGx5IHdpdGhvdXQgaHlwZXJwYXJhbWV0ZXIgdHVuaW5nLgoKIyMjIE5vcm1hbGl6YXRpb24gbGF5ZXIKQ29uc2lkZXIgYW4gc3RhbmRhcmQgTkxQIHRhc2ssIHdoZXJlIGFuIGlucHV0ICR4JCBoYXMgYSBzaGFwZSBvZiAkKEIsVCxDKSQsIHdoZXJlICRCJCBpcyB0aGUgYmF0Y2ggc2l6ZSwgJFQkIC0gbnVtYmVyIG9mIHRva2VucyAoc2VxdWVuY2UgbGVuZ3RoKSBhbmQgJEMkIC0gZW1iZWRkaW5nIGRpbWVuc2lvbnMuIFRoZW4gYW4gb3V0cHV0IG9mIGEgbm9ybWFsaXphdGlvbiBsYXllciBpcyBnZW5lcmFsbHkgY29tcHV0ZWQgYXMgJG5vcm0oeCk9XGdhbW1hKFxmcmFje3gtXG11fXtcc3FydHtcc2lnbWFeMitcdmFyZXBzaWxvbn19KStcYmV0YSQsIHdoZXJlICRcZ2FtbWEkIGFuZCAkXGJldGEkIGFyZSBsZWFybmFibGUgcGFyYW1ldGVycyBvZiBzaGFwZSAkKEMsKSQuIERpc3RyaWJ1dGlvbidzIHN0YXRpc3RpY3MgYXJlIGNhbGN1bGF0ZWQgYXMgZm9sbG93czogJFxtdV9rPVxmcmFjezF9e0JUfVxzdW1faV5CXHN1bV9qXlR4X3tpan0kOyAkXHNpZ21hX2teMj1cZnJhY3sxfXtCIFR9IFxzdW1fe2ksIGp9XGxlZnQoeF97aSBqIGt9LVxtdV9rXHJpZ2h0KV4yJAoKIyMjIEh5cGVyYm9sb2ljIHRhbmdlbnQgKFRhbmgpClRhbmggZnVuY3Rpb24gaXMgZGVmaW5lZCBhcyBhIHJhdGlvOiAkdGFuaCh4KT1cZnJhY3tzaW5oKHgpfXtjb3NoKHgpfT1cZnJhY3tleHAoeCktZXhwKC14KX17ZXhwKHgpK2V4cCgteCl9JC4gRXNzZW50aWFsbHkgdGhlIGZ1bmN0aW9uIGFsbG93cyB0cmFuc2Zvcm1hdGlvbiBvZiBhbiBhcmJpdHJhcnkgZG9tYWluIHRvICRbLTEsMV0kLiAKCiMjIyBEeW5hbWljIFRhbmggKER5VCkKVHVybnMgb3V0IHRoYXQgTE4gKGxheWVyIG5vcm1hbGl6YXRpb24pIHByb2R1Y2VzIGRpZmZlcmVudCBwYXJ0cyBvZiBhICR0YW5oKGt4KSQsIHdoZXJlICRrJCBjb250cm9scyB0aGUgY3VydmF0dXJlIG9mIHRoZSB0YW5oIGN1cnZlIGluIHRoZSBjZW50ZXIuIFRoZSBzbWFsbGVyIHRoZSAkayQsIHRoZSBzbW9vdGhlciBpcyB0aGUgY2hhbmdlIGZyb20gJC0xJCB0byAkMSQuIEhlbmNlIHRoZSBzdHVkeSBwcm9wb3NlcyBhIGRyb3AtaW4gcmVwbGFjZW1lbnQgZm9yIExOIGdpdmVuIGFuIGlucHV0IHRlbnNvciAkeCQ6CgokJApEeVQoeCk9XGdhbW1hKnRhbmgoXGFscGhhIHgpK1xiZXRhLAokJAoKd2hlcmU6CiogJFxhbHBoYSQgLSBsZWFybmFibGUgcGFyYW1ldGVyIHRoYXQgYWxsb3dzIHNjYWxpbmcgdGhlIGlucHV0IGRpZmZlcmVudGx5IGJhc2VkIG9uIGl0cyByYW5nZSAodG9rZW5zIHByb2R1Y2luZyAqKnNtYWxsZXIgdmFyaWFuY2UqKiBwcm9kdWNlICoqbGVzcyBzbW9vdGhlciBjdXJ2ZXMqKikuIEF1dGhvcnMgc3VnZ2VzdCBhICoqZGVmYXVsdCB2YWx1ZSoqIG9mICQwLjUkLgoqICRcZ2FtbWEsIFxiZXRhJCAtIGxlYXJuYWJsZSBwYXJhbWV0ZXJzLCB0aGF0IHNjYWxlIG91ciBvdXRwdXQgYmFzZWQgb24gdGhlIGlucHV0LiBBdXRob3JzIHN1Z2dlc3QgaW5pdGlhbGl6aW5nIHRoZXNlIHZlY3RvcnMgd2l0aCBmb2xsb3dpbmcgKipkZWZhdWx0IHZhbHVlcyoqOgogICAgKiAkXGdhbW1hJCBhcyBhbGwtb25lIHZlY3RvciAKICAgICogJFxiZXRhJCBhcyBhbGwtemVybwoKRGVzcGl0ZSBub3QgY2FsY3VsYXRpbmcgc3RhdGlzdGljcywgRHlUIHByZXNlcnZlcyB0aGUgInNxdWFzaGluZyIgZWZmZWN0IG9mIExOIG9uIGV4dHJlbWUgdmFsdWVzIGluIGEgbm9uLWxpbmVhciBmYXNoaW9uLCB3aGlsZSBhbG1vc3QgbGluZWFybHkgdHJhbnNmb3JtaW5nIGNlbnRyYWwgcGFydHMgb2YgdGhlIGlucHV0Lg==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def dynamic_tanh(self, x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n        def tanh(self, x: np.ndarray) -> np.ndarray:\n            return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n\n        x = tanh(alpha * x)\n        return (x * gamma + beta).round(4).tolist()",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nx = np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]])\ngamma = np.ones((1,))\nbeta = np.zeros((1,))\nprint(np.round(dynamic_tanh(x, 0.5, gamma, beta),4))"
        ],
        "expected_output": [
          [
            [
              0.4397
            ]
          ],
          [
            [
              0.4532
            ]
          ],
          [
            [
              0.1789
            ]
          ],
          [
            [
              0.2535
            ]
          ],
          [
            [
              0.3669
            ]
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\nx = np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]])\ngamma = np.ones((3,))\nbeta = np.zeros((3,))\nprint(np.round(dynamic_tanh(x, 0.5, gamma, beta),4))"
        ],
        "expected_output": [
          [
            [
              [
                0.1036,
                0.0847,
                0.0195
              ],
              [
                0.0893,
                0.0548,
                0.1912
              ],
              [
                0.0005,
                0.1772,
                0.0791
              ]
            ]
          ]
        ]
      }
    ],
    "function_name": "dynamic_tanh"
  },
  {
    "id": 129,
    "title": "Calculate Unigram Probability from Corpus",
    "difficulty": "easy",
    "category": "nlp",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSB1bmlncmFtIHByb2JhYmlsaXR5IG9mIGEgZ2l2ZW4gd29yZCBpbiBhIGNvcnB1cyBvZiBzZW50ZW5jZXMuIEluY2x1ZGUgc3RhcnQgYDxzPmAgYW5kIGVuZCBgPC9zPmAgdG9rZW5zIGluIHRoZSBjYWxjdWxhdGlvbi4gVGhlIHByb2JhYmlsaXR5IHNob3VsZCBiZSByb3VuZGVkIHRvIDQgZGVjaW1hbCBwbGFjZXMu",
    "starter_code": "\n\nclass Solution:\n    def unigram_probability(self, corpus: str, word: str) -> float:\n        # Your code here\n        pass",
    "example_input": "corpus = \"<s> Jack I like </s> <s> Jack I do like </s>\", word = \"Jack\"",
    "example_output": "0.1818",
    "example_reasoning": "The corpus has 11 total tokens. 'Jack' appears twice. So, probability = 2 / 11",
    "learn_content": "IyBVbmlncmFtIFByb2JhYmlsaXR5IENhbGN1bGF0aW9uCgotIEluIE5hdHVyYWwgTGFuZ3VhZ2UgUHJvY2Vzc2luZyAoTkxQKSwgYSAgdW5pZ3JhbSBtb2RlbCBpcyB0aGUgc2ltcGxlc3QgZm9ybSBvZiBhIGxhbmd1YWdlIG1vZGVsLiAKLSBJdCBhc3N1bWVzIGVhY2ggd29yZCBpbiBhIHNlbnRlbmNlIGlzIGdlbmVyYXRlZCBpbmRlcGVuZGVudGx5LiAgCgoKLSBUaGUgcHJvYmFiaWxpdHkgb2YgYSB3b3JkIHcgdW5kZXIgdGhlIHVuaWdyYW0gbW9kZWwgaXM6CgokUCh3KSA9IFxmcmFje1x0ZXh0e0NvdW50fSh3KX17XHN1bV97dycgXGluIFZ9IFx0ZXh0e0NvdW50fSh3Jyl9JAoKV2hlcmU6CgotICRcdGV4dHtDb3VudH0odykkID0gTnVtYmVyIG9mIHRpbWVzIHRoZSB3b3JkIHcgYXBwZWFycyBpbiB0aGUgY29ycHVzLgoKLSAkViQgPSBWb2NhYnVsYXJ5IChhbGwgd29yZCB0b2tlbnMgaW4gdGhlIGNvcnB1cykuCgotICRcc3VtX3t3JyBcaW4gVn0gXHRleHR7Q291bnR9KHcnKSQgPSBUb3RhbCBudW1iZXIgb2Ygd29yZCB0b2tlbnMuCi0gUm91bmQgdXB0byB0aGUgNHRoIGRlY2ltYWwgcG9pbnQuCgoKLS0tCgojIyMgU2FtcGxlIENvcnB1cwoKYGBgdGV4dAo8cz4gSSBhbSBKYWNrIDwvcz4KPHM+IEphY2sgSSBhbSA8L3M+CjxzPiBKYWNrIEkgbGlrZSA8L3M+CjxzPiBKYWNrIEkgZG8gbGlrZSA8L3M+CjxzPiBkbyBJIGxpa2UgSmFjayA8L3M+CmBgYAoKTm90ZXMgOiAKLSBcPHM+IDogU3RhcnQgb2YgYSBzZW50ZW5jZQotIFw8L3M+IDogRW5kIG9mIGEgc2VudGVuY2UKLSBOZWVkIHRvIGNvdW50IGJvdGggdGhlIHN0YXJ0IGFuZCBlbm9kIG9mIHNlbnRlbmNlIHRva2VucyB3aGlsZSBjYWxjdWxhdGluZyBwcm9iYWJpbGl0eS4KLSBaZXJvIHByb2JhYmlsaXR5IGlzc3VlcyBhcmUgbm90IGFkZHJlc3NlZCBoZXJlIGFuZCB3aWxsIGJlIGNvdmVyZWQgc2VwYXJhdGVseSB1bmRlciBzbW9vdGhpbmcgdGVjaG5pcXVlcyBpbiAgbGF0ZXIgcHJvYmxlbXMu",
    "solution_code": "\n\nclass Solution:\n    def unigram_probability(self, corpus: str, word: str) -> float:\n        tokens = corpus.split()\n        total_word_count = len(tokens)\n        word_count = tokens.count(word)\n        return round(word_count / total_word_count, 4)",
    "test_cases": [
      {
        "test": [
          "corpus = \"\"\"<s> I am Jack </s> <s> Jack I am </s> <s> Jack I like </s> <s> Jack I do like </s> <s> do I like Jack </s>\"\"\"\nprint(round(unigram_probability(corpus, \"Jack\"),4))"
        ],
        "expected_output": 0.1852
      },
      {
        "test": [
          "corpus = \"\"\"<s> I am Jack </s> <s> Jack I am </s> <s> Jack I like </s> <s> Jack I do like </s> <s> do I like Jack </s>\"\"\"\nprint(round(unigram_probability(corpus, \"like\"),4))"
        ],
        "expected_output": 0.1111
      }
    ],
    "function_name": "unigram_probability"
  },
  {
    "id": 130,
    "title": "Implement a Simple CNN Training Function with Backpropagation",
    "difficulty": "hard",
    "category": "deep learning",
    "problem_description": "Q3JlYXRlIGEgZnVuY3Rpb24gdGhhdCB0cmFpbnMgYSBiYXNpYyBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3JrIChDTk4pIHVzaW5nIGJhY2twcm9wYWdhdGlvbi4gVGhlIG5ldHdvcmsgc2hvdWxkIGluY2x1ZGUgb25lIGNvbnZvbHV0aW9uYWwgbGF5ZXIgd2l0aCBSZUxVIGFjdGl2YXRpb24sIGZvbGxvd2VkIGJ5IGZsYXR0ZW5pbmcgYW5kIGEgZGVuc2UgbGF5ZXIgd2l0aCBzb2Z0bWF4IG91dHB1dCwgYW5kIGEgY3Jvc3MgZW50cm9weSBsb3NzLiBZb3UgbmVlZCB0byBoYW5kbGUgdGhlIGZvcndhcmQgcGFzcywgY29tcHV0ZSB0aGUgbG9zcyBncmFkaWVudHMsIGFuZCB1cGRhdGUgdGhlIHdlaWdodHMgYW5kIGJpYXNlcyB1c2luZyBzdG9jaGFzdGljIGdyYWRpZW50IGRlc2NlbnQuIEVuc3VyZSB0aGUgZnVuY3Rpb24gcHJvY2Vzc2VzIGlucHV0IGRhdGEgYXMgZ3JheXNjYWxlIGltYWdlcyBhbmQgb25lLWhvdCBlbmNvZGVkIGxhYmVscywgYW5kIHJldHVybnMgdGhlIHRyYWluZWQgd2VpZ2h0cyBhbmQgYmlhc2VzIGZvciB0aGUgY29udm9sdXRpb25hbCBhbmQgZGVuc2UgbGF5ZXJzLg==",
    "starter_code": "\n\nclass Solution:\n    def train_simple_cnn_with_backprop(self, X, y, epochs, learning_rate, kernel_size=3, num_filters=1):\n        # Your code here\n        pass",
    "example_input": "import numpy as np; np.random.seed(42); X = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]); y = np.array([[1, 0]]); print(train_simple_cnn_with_backprop(X, y, 1, 0.01, 3, 1))",
    "example_output": "(array([[[ 0.00501739],        [-0.00128214],        [ 0.00662764]],       [[ 0.01543131],        [-0.00209028],        [-0.00203986]],       [[ 0.01614389],        [ 0.00807636],        [-0.00424248]]]), array([5.02517066e-05]), array([[ 0.00635715, -0.00556573]]), array([ 0.00499531, -0.00499531]))",
    "example_reasoning": "The solution processes the input X through a forward pass, where it applies a convolutional layer with ReLU activation, flattens the output, and passes it through a dense layer with softmax to compute predictions and loss based on the one-hot encoded label y. In the backward pass, it calculates gradients using backpropagation for the weights and biases, then updates them using stochastic gradient descent with the specified learning rate, and returns the updated weights after one epoch.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def train_simple_cnn_with_backprop(self, X, y, epochs, learning_rate, kernel_size=3, num_filters=1):\n        '''\n        Trains a simple CNN with one convolutional layer, ReLU activation, flattening, and a dense layer with softmax output using backpropagation.\n\n        Assumes X has shape (n_samples, height, width) for grayscale images and y is one-hot encoded with shape (n_samples, num_classes).\n\n        Parameters:\n        X : np.ndarray, input data\n        y : np.ndarray, one-hot encoded labels\n        epochs : int, number of training epochs\n        learning_rate : float, learning rate for weight updates\n        kernel_size : int, size of the square convolutional kernel\n        num_filters : int, number of filters in the convolutional layer\n\n        Returns:\n        W_conv, b_conv, W_dense, b_dense : Trained weights and biases for the convolutional and dense layers\n        '''\n        n_samples, height, width = X.shape\n        num_classes = y.shape[1]\n\n        # Initialize weights and biases\n        W_conv = np.random.randn(kernel_size, kernel_size, num_filters) * 0.01\n        b_conv = np.zeros(num_filters)\n        output_height = height - kernel_size + 1\n        output_width = width - kernel_size + 1\n        flattened_size = output_height * output_width * num_filters\n        W_dense = np.random.randn(flattened_size, num_classes) * 0.01\n        b_dense = np.zeros(num_classes)\n\n        for epoch in range(epochs):\n            for i in range(n_samples):  # Stochastic Gradient Descent with batch size 1\n                # Forward pass\n                # Convolutional layer\n                Z_conv = np.zeros((output_height, output_width, num_filters))\n                for k in range(num_filters):\n                    for p in range(output_height):\n                        for q in range(output_width):\n                            Z_conv[p, q, k] = np.sum(X[i, p:p+kernel_size, q:q+kernel_size] * W_conv[:, :, k]) + b_conv[k]\n                A_conv = np.maximum(Z_conv, 0)  # ReLU activation\n                A_flat = A_conv.flatten()  # Flatten the output\n\n                # Dense layer\n                Z_dense = np.dot(A_flat, W_dense) + b_dense\n                exp_Z_dense = np.exp(Z_dense - np.max(Z_dense))  # Numerical stability for softmax\n                A_dense = exp_Z_dense / np.sum(exp_Z_dense)\n\n                # Backpropagation\n                # Loss gradient for cross-entropy with softmax\n                dZ_dense = A_dense - y[i]\n\n                # Dense layer gradients\n                dW_dense = np.outer(A_flat, dZ_dense)\n                db_dense = dZ_dense\n                dA_flat = np.dot(dZ_dense, W_dense.T)\n\n                # Reshape and backprop through ReLU\n                dA_conv = dA_flat.reshape(A_conv.shape)\n                dZ_conv = dA_conv * (A_conv > 0).astype(float)\n\n                # Convolutional layer gradients\n                dW_conv = np.zeros_like(W_conv)\n                db_conv = np.zeros(num_filters)\n                for k in range(num_filters):\n                    db_conv[k] = np.sum(dZ_conv[:, :, k])\n                    for ii in range(kernel_size):\n                        for jj in range(kernel_size):\n\n                            dW_conv[ii, jj, k] = np.sum(dZ_conv[:, :, k] * X[i, ii:ii+output_height, jj:jj+output_width])\n\n\n    # Update weights and biases\n                W_conv -= learning_rate * dW_conv\n                b_conv -= learning_rate * db_conv\n                W_dense -= learning_rate * dW_dense\n                b_dense -= learning_rate * db_dense\n\n        return W_conv, b_conv, W_dense, b_dense",
    "test_cases": [
      {
        "test": [
          "import numpy as np; np.random.seed(42); X = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]); y = np.array([[1, 0]]); print(train_simple_cnn_with_backprop(X, y, 1, 0.01, 3, 1))"
        ],
        "expected_output": "(array([[[ 0.00501739],        [-0.00128214],        [ 0.00662764]],       [[ 0.01543131],        [-0.00209028],        [-0.00203986]],       [[ 0.01614389],        [ 0.00807636],        [-0.00424248]]]), array([5.02517066e-05]), array([[ 0.00635715, -0.00556573]]), array([ 0.00499531, -0.00499531]))"
      },
      {
        "test": [
          "import numpy as np; np.random.seed(42); X = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[9, 8, 7], [6, 5, 4], [3, 2, 1]]]); y = np.array([[0, 1], [1, 0]]); print(train_simple_cnn_with_backprop(X, y, 2, 0.01, 3, 1))"
        ],
        "expected_output": "(array([[[ 0.00561561],        [-0.00091884],        [ 0.00675603]],       [[ 0.01532478],        [-0.00243171],        [-0.00261621]],       [[ 0.01533262],        [ 0.00703018],        [-0.00552357]]]), array([-1.80360153e-05]), array([[ 0.00561137, -0.00481995]]), array([ 3.23124658e-05, -3.23124658e-05]))"
      }
    ],
    "function_name": "train_simple_cnn_with_backprop"
  },
  {
    "id": 131,
    "title": "Implement Efficient Sparse Window Attention",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "Q3JlYXRlIGEgZnVuY3Rpb24gbmFtZWQgc3BhcnNlX3dpbmRvd19hdHRlbnRpb24gdGhhdCBjb21wdXRlcyBzcGFyc2UgYXR0ZW50aW9uIG92ZXIgbG9uZyBzZXF1ZW5jZXMgYnkgc2xpZGluZyBhIGZpeGVkLXJhZGl1cyB3aW5kb3cgYWNyb3NzIHRoZSBzZXF1ZW5jZS4KCuKAoiBUaGUgcGFyYW1ldGVyIHdpbmRvd19zaXplIHJlcHJlc2VudHMgdGhlIHJhZGl1cyB3IG9mIHRoZSB3aW5kb3cuCi0gRm9yIGEgdG9rZW4gYXQgaW5kZXggaSwgYXR0ZW5kIG9ubHkgdG8gdG9rZW5zIHdob3NlIGluZGljZXMgYXJlIHdpdGhpbiBtYXgoMCwgaSAtIHcpIHRocm91Z2ggbWluKHNlcV9sZW4gLSAxLCBpICsgdyksIGluY2x1c2l2ZS4KLSBUb2tlbnMgbmVhciB0aGUgYmVnaW5uaW5nIG9yIGVuZCBvZiB0aGUgc2VxdWVuY2Ugc2ltcGx5IGhhdmUgc21hbGxlciB3aW5kb3dzOyBubyBwYWRkaW5nIGlzIGFkZGVkLgoK4oCiIElucHV0cwotIFEsIEssIFY6IE51bVB5IGFycmF5cyB3aXRoIHNoYXBlcyAoc2VxX2xlbiwgZF9rKSBmb3IgUSBhbmQgSywgYW5kIChzZXFfbGVuLCBkX3YpIGZvciBWLgotIHdpbmRvd19zaXplOiBpbnRlZ2VyIHdpbmRvdyByYWRpdXMuCi0gc2NhbGVfZmFjdG9yIChvcHRpb25hbCk6IHZhbHVlIHVzZWQgdG8gc2NhbGUgZG90LXByb2R1Y3Qgc2NvcmVzOyBpZiBOb25lLCBkZWZhdWx0IHRvIHNxcnQoZF9rKS4KCuKAoiBPdXRwdXQKLSBBIE51bVB5IGFycmF5IG9mIHNoYXBlIChzZXFfbGVuLCBkX3YpIGNvbnRhaW5pbmcgdGhlIGF0dGVudGlvbiByZXN1bHRzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def sparse_window_attention(self, Q, K, V, window_size, scale_factor=None):\n        # Your code here\n        pass",
    "example_input": "import numpy as np\nQ = np.array([[1.0], [1.0], [1.0]])\nK = np.array([[1.0], [1.0], [1.0]])\nV = np.array([[1.0], [2.0], [3.0]])\nprint(sparse_window_attention(Q, K, V, 1))",
    "example_output": "[[1.5] [2. ] [2.5]]",
    "example_reasoning": "The sparse_window_attention function processes each query in the input Q by computing attention scores only with keys in K within a window of size 1 (i.e., the current position and one adjacent position on each side), then applies softmax to these scores to derive weights for the corresponding values in V. For the given input arrays, this results in the output where the first element is the weighted average of V[0] and V[1] (yielding 1.5), the second is the average of all elements in V (yielding 2.0), and the third is the average of V[1] and V[2] (yielding 2.5).",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def sparse_window_attention(self, Q, K, V, window_size, scale_factor=None):\n        \"\"\"\n        Computes sparse attention with a sliding window mask to efficiently handle longer context lengths.\n        This implementation uses a loop over the sequence to compute attention only within the specified window,\n        reducing memory usage compared to dense attention.\n\n        Args:\n            Q (np.ndarray): Query matrix of shape (seq_len, d_k)\n            K (np.ndarray): Key matrix of shape (seq_len, d_k)\n            V (np.ndarray): Value matrix of shape (seq_len, d_v)\n            window_size (int): The radius of the attention window (attends to window_size positions on each side).\n            scale_factor (float, optional): Scaling factor for the dot product. If None, uses sqrt(d_k).\n\n        Returns:\n            np.ndarray: Attention output of shape (seq_len, d_v)\n        \"\"\"\n        seq_len = Q.shape[0]\n        d_k = Q.shape[1]\n        if scale_factor is None:\n            scale_factor = np.sqrt(d_k).astype(float)\n        output = np.zeros((seq_len, V.shape[1]), dtype=V.dtype)\n        for i in range(seq_len):\n            start = max(0, i - window_size)\n            end = min(seq_len, i + window_size + 1)\n            local_Q = Q[i:i+1]\n            local_K = K[start:end]\n            local_V = V[start:end]\n            scores = np.dot(local_Q, local_K.T) / scale_factor\n            max_score = np.max(scores, axis=1, keepdims=True)\n            exp_scores = np.exp(scores - max_score)\n            sum_exp = np.sum(exp_scores, axis=1, keepdims=True)\n            attention_weights = exp_scores / sum_exp\n            output[i] = np.dot(attention_weights, local_V)\n        return output",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nQ = np.array([[1.0], [1.0], [1.0]])\nK = np.array([[1.0], [1.0], [1.0]])\nV = np.array([[1.0], [2.0], [3.0]])\nprint(sparse_window_attention(Q, K, V, 1))"
        ],
        "expected_output": [
          [
            1.5
          ],
          [
            2.0
          ],
          [
            2.5
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\nQ = np.array([[4.0]])\nK = np.array([[4.0]])\nV = np.array([[5.0]])\nprint(sparse_window_attention(Q, K, V, 0))"
        ],
        "expected_output": [
          [
            5.0
          ]
        ]
      }
    ],
    "function_name": "sparse_window_attention"
  },
  {
    "id": 132,
    "title": "Simulate Markov Chain Transitions",
    "difficulty": "medium",
    "category": "probability",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gc2ltdWxhdGUgYSBNYXJrb3YgQ2hhaW4uIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBhIDJEIG51bXB5IGFycmF5IHJlcHJlc2VudGluZyB0aGUgdHJhbnNpdGlvbiBtYXRyaXggKHdoZXJlIGVhY2ggcm93IHN1bXMgdG8gMSksIGFuIGludGVnZXIgZm9yIHRoZSBpbml0aWFsIHN0YXRlIGluZGV4LCBhbmQgYW4gaW50ZWdlciBmb3IgdGhlIG51bWJlciBvZiBzdGVwcyB0byBzaW11bGF0ZS4gSXQgc2hvdWxkIHJldHVybiBhIG51bXB5IGFycmF5IGNvbnRhaW5pbmcgdGhlIHNlcXVlbmNlIG9mIHN0YXRlIGluZGljZXMgb3ZlciB0aW1lLCBpbmNsdWRpbmcgdGhlIGluaXRpYWwgc3RhdGUuIFVzZSBudW1weSBmb3IgYXJyYXkgb3BlcmF0aW9ucyBhbmQgcmFuZG9tIHNlbGVjdGlvbnMu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def simulate_markov_chain(self, transition_matrix, initial_state, num_steps):\n        # Your code here\n        pass",
    "example_input": "transition_matrix = np.array([[0.8, 0.2], [0.3, 0.7]]); print(simulate_markov_chain(transition_matrix, 0, 3))",
    "example_output": "[0 0 1 1]",
    "example_reasoning": "The solution simulates a Markov chain by starting with the initial state (0) and iteratively selecting the next state based on the probabilities in the transition matrix. For the given input, this process generates the sequence [0, 0, 1, 1] over three steps, where the first state is the initial one, and subsequent states are chosen such that from state 0, it stays at 0, then transitions to 1, and remains at 1.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def simulate_markov_chain(self, transition_matrix, initial_state, num_steps):\n        \"\"\"\n        Simulates a Markov Chain given a transition matrix, initial state, and number of steps.\n\n        Parameters:\n        transition_matrix : 2D numpy.ndarray, transition probabilities where each row sums to 1.\n        initial_state : int, starting state index.\n        num_steps : int, number of steps to simulate.\n\n        Returns:\n        numpy.ndarray, array of state indices over time, including the initial state.\n        \"\"\"\n        states = np.zeros(num_steps + 1, dtype=int)\n        states[0] = initial_state\n        current_state = initial_state\n        for t in range(num_steps):\n            probabilities = transition_matrix[current_state]\n            next_state = np.random.choice(transition_matrix.shape[1], p=probabilities)\n            states[t + 1] = next_state\n            current_state = next_state\n        return states",
    "test_cases": [
      {
        "test": [
          "import numpy as np; np.random.seed(42); transition_matrix = np.array([[0.8, 0.2], [0.3, 0.7]]); print(simulate_markov_chain(transition_matrix, 0, 3))"
        ],
        "expected_output": "[0 0 1 1]"
      },
      {
        "test": [
          "import numpy as np; np.random.seed(0); transition_matrix = np.array([[0.5, 0.5], [0.5, 0.5]]); print(simulate_markov_chain(transition_matrix, 1, 4))"
        ],
        "expected_output": "[1 1 1 1 1]"
      }
    ],
    "function_name": "simulate_markov_chain"
  },
  {
    "id": 133,
    "title": "Implement Q-Learning Algorithm for MDPs",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "V3JpdGUgYSBmdW5jdGlvbiB0aGF0IGltcGxlbWVudHMgdGhlIFEtTGVhcm5pbmcgYWxnb3JpdGhtIHRvIGxlYXJuIHRoZSBvcHRpbWFsIFEtdGFibGUgZm9yIGEgZ2l2ZW4gTWFya292IERlY2lzaW9uIFByb2Nlc3MgKE1EUCkuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSB0aGUgbnVtYmVyIG9mIHN0YXRlcywgbnVtYmVyIG9mIGFjdGlvbnMsIHRyYW5zaXRpb24gcHJvYmFiaWxpdGllcyBtYXRyaXgsIHJld2FyZHMgbWF0cml4LCBsaXN0IG9mIHRlcm1pbmFsIHN0YXRlcywgbGVhcm5pbmcgcmF0ZSwgZGlzY291bnQgZmFjdG9yLCBlcHNpbG9uIGZvciBleHBsb3JhdGlvbiwgYW5kIHRoZSBudW1iZXIgb2YgZXBpc29kZXMgYXMgaW5wdXRzLiBVc2UgdGhlc2UgcGFyYW1ldGVycyB0byBpdGVyYXRpdmVseSB1cGRhdGUgdGhlIFEtdGFibGUgYmFzZWQgb24gdGhlIFEtTGVhcm5pbmcgdXBkYXRlIHJ1bGUsIGVtcGxveWluZyBhbiBlcHNpbG9uLWdyZWVkeSBzdHJhdGVneSBmb3IgYWN0aW9uIHNlbGVjdGlvbi4gRW5zdXJlIHRoZSBmdW5jdGlvbiBoYW5kbGVzIHN0YXJ0aW5nIGZyb20gbm9uLXRlcm1pbmFsIHN0YXRlcyBhbmQgc3RvcHMgZXBpc29kZXMgdXBvbiByZWFjaGluZyBhIHRlcm1pbmFsIHN0YXRlLgoKQ29uc3RyYWludHM6Ci0gbnVtX3N0YXRlczogSW50ZWdlciBncmVhdGVyIHRoYW4gb3IgZXF1YWwgdG8gMS4KLSBudW1fYWN0aW9uczogSW50ZWdlciBncmVhdGVyIHRoYW4gb3IgZXF1YWwgdG8gMS4KLSBQOiBBIDNEIE51bVB5IGFycmF5IG9mIHNoYXBlIChudW1fc3RhdGVzLCBudW1fYWN0aW9ucywgbnVtX3N0YXRlcykgd2hlcmUgZWFjaCBlbGVtZW50IGlzIGEgcHJvYmFiaWxpdHkgYmV0d2VlbiAwIGFuZCAxLCBhbmQgZWFjaCBzdWItYXJyYXkgc3VtcyB0byAxLgotIFI6IEEgMkQgTnVtUHkgYXJyYXkgb2Ygc2hhcGUgKG51bV9zdGF0ZXMsIG51bV9hY3Rpb25zKSB3aXRoIGZsb2F0IG9yIGludGVnZXIgdmFsdWVzLgotIHRlcm1pbmFsX3N0YXRlczogQSBsaXN0IG9yIE51bVB5IGFycmF5IG9mIGludGVnZXJzLCBlYWNoIGJldHdlZW4gMCBhbmQgbnVtX3N0YXRlcyAtIDEsIHdpdGggbm8gZHVwbGljYXRlcy4KLSBhbHBoYTogQSBmbG9hdCBiZXR3ZWVuIDAgYW5kIDEuCi0gZ2FtbWE6IEEgZmxvYXQgYmV0d2VlbiAwIGFuZCAxLgotIGVwc2lsb246IEEgZmxvYXQgYmV0d2VlbiAwIGFuZCAxLgotIG51bV9lcGlzb2RlczogQW4gaW50ZWdlciBncmVhdGVyIHRoYW4gb3IgZXF1YWwgdG8gMS4KVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gYSAyRCBOdW1QeSBhcnJheSBvZiBzaGFwZSAobnVtX3N0YXRlcywgbnVtX2FjdGlvbnMpIHJlcHJlc2VudGluZyB0aGUgbGVhcm5lZCBRLXRhYmxlLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def q_learning(self, num_states, num_actions, P, R, terminal_states, alpha, gamma, epsilon, num_episodes):\n        # Your code here\n        pass",
    "example_input": "import numpy as np; np.random.seed(42); P = np.array([[[0, 1], [1, 0]], [[1, 0], [1, 0]]]); R = np.array([[1, 0], [0, 0]]); terminal_states = [1]; print(q_learning(2, 2, P, R, terminal_states, 0.1, 0.9, 0.1, 10))",
    "example_output": "[[0.65132156, 0.052902  ],[0., 0.]]",
    "example_reasoning": "The Q-Learning algorithm initializes a Q-table with zeros and iteratively updates it over 10 episodes by starting from random non-terminal states, selecting actions via an epsilon-greedy policy, sampling next states and rewards from the provided transition probabilities (P) and rewards (R), and applying the update rule: Q(s, a) += alpha * (reward + gamma * max(Q(next_state)) - Q(s, a)). This process results in the output Q-table [[0.65132156, 0.052902], [0., 0.]], where the values represent learned estimates of state-action values, with the second state's Q-values remaining zero because it is a terminal state and no further actions are taken from there.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def q_learning(self, num_states, num_actions, P, R, terminal_states, alpha, gamma, epsilon, num_episodes):\n        \"\"\"\n        Implements Q-Learning algorithm to learn the optimal Q-table for a given MDP.\n\n        Parameters:\n        - num_states: int, number of states in the MDP\n        - num_actions: int, number of actions in the MDP\n        - P: NumPy array of shape (num_states, num_actions, num_states), transition probabilities\n        - R: NumPy array of shape (num_states, num_actions), rewards for each state-action pair\n        - terminal_states: list or NumPy array of integers, indices of terminal states\n        - alpha: float, learning rate\n        - gamma: float, discount factor\n        - epsilon: float, probability of choosing a random action in epsilon-greedy policy\n        - num_episodes: int, number of episodes to train\n\n        Returns:\n        - Q: NumPy array of shape (num_states, num_actions), the learned Q-table\n        \"\"\"\n        Q = np.zeros((num_states, num_actions))\n\n        for episode in range(num_episodes):\n            # Start from a random non-terminal state\n            state = np.random.choice([s for s in range(num_states) if s not in set(terminal_states)])\n\n            while state not in terminal_states:\n                # Epsilon-greedy action selection\n                if np.random.rand() < epsilon:\n                    action = np.random.randint(num_actions)\n                else:\n                    action = np.argmax(Q[state])\n\n                # Sample next state based on transition probabilities\n                next_state = np.random.choice(num_states, p=P[state, action])\n\n                # Get reward\n                reward = R[state, action]\n\n                # Compute target Q-value\n                if next_state in terminal_states:\n                    target = reward\n                else:\n                    target = reward + gamma * np.max(Q[next_state])\n\n                # Update Q-table\n                Q[state, action] += alpha * (target - Q[state, action])\n\n                # Transition to next state\n                state = next_state\n\n        return Q",
    "test_cases": [
      {
        "test": [
          "import numpy as np; np.random.seed(42); P = np.array([[[0, 1], [1, 0]], [[1, 0], [1, 0]]]); R = np.array([[1, 0], [0, 0]]); terminal_states = [1]; print(q_learning(2, 2, P, R, terminal_states, 0.1, 0.9, 0.1, 10))"
        ],
        "expected_output": [
          [
            0.65132156,
            0.052902
          ],
          [
            0.0,
            0.0
          ]
        ]
      },
      {
        "test": [
          "import numpy as np; np.random.seed(42); P = np.array([[[0.5, 0.5], [0, 1]], [[0, 1], [1, 0]]]); R = np.array([[0.5, 1], [0, 0]]); terminal_states = [1]; print(q_learning(2, 2, P, R, terminal_states, 0.5, 0.8, 0.2, 5))"
        ],
        "expected_output": [
          [
            0.91785477,
            0.5
          ],
          [
            0.0,
            0.0
          ]
        ]
      }
    ],
    "function_name": "q_learning"
  },
  {
    "id": 134,
    "title": "Compute Multi-class Cross-Entropy Loss",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgYXZlcmFnZSBjcm9zcy1lbnRyb3B5IGxvc3MgZm9yIGEgYmF0Y2ggb2YgcHJlZGljdGlvbnMgaW4gYSBtdWx0aS1jbGFzcyBjbGFzc2lmaWNhdGlvbiB0YXNrLiBZb3VyIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGluIGEgYmF0Y2ggb2YgcHJlZGljdGVkIHByb2JhYmlsaXRpZXMgYW5kIG9uZS1ob3QgZW5jb2RlZCB0cnVlIGxhYmVscywgdGhlbiByZXR1cm4gdGhlIGF2ZXJhZ2UgY3Jvc3MtZW50cm9weSBsb3NzLiBFbnN1cmUgdGhhdCB5b3UgaGFuZGxlIG51bWVyaWNhbCBzdGFiaWxpdHkgYnkgY2xpcHBpbmcgcHJvYmFiaWxpdGllcyBieSBlcHNpbG9uLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def compute_cross_entropy_loss(self, predicted_probs: np.ndarray, true_labels: np.ndarray, epsilon = 1e-15) -> float:\n        # Your code here\n        pass",
    "example_input": "predicted_probs = [[0.7, 0.2, 0.1], [0.3, 0.6, 0.1]]\ntrue_labels = [[1, 0, 0], [0, 1, 0]]",
    "example_output": "0.4338",
    "example_reasoning": "The predicted probabilities for the correct classes are 0.7 and 0.6. The cross-entropy is computed as -mean(log(0.7), log(0.6)), resulting in approximately 0.4463.",
    "learn_content": "IyMgTXVsdGktY2xhc3MgQ3Jvc3MtRW50cm9weSBMb3NzIEltcGxlbWVudGF0aW9uCgpDcm9zcy1lbnRyb3B5IGxvc3MsIGFsc28ga25vd24gYXMgbG9nIGxvc3MsIG1lYXN1cmVzIHRoZSBwZXJmb3JtYW5jZSBvZiBhIGNsYXNzaWZpY2F0aW9uIG1vZGVsIHdob3NlIG91dHB1dCBpcyBhIHByb2JhYmlsaXR5IHZhbHVlIGJldHdlZW4gMCBhbmQgMS4gRm9yIG11bHRpLWNsYXNzIGNsYXNzaWZpY2F0aW9uIHRhc2tzLCB3ZSB1c2UgdGhlIGNhdGVnb3JpY2FsIGNyb3NzLWVudHJvcHkgbG9zcy4KCiMjIyBNYXRoZW1hdGljYWwgQmFja2dyb3VuZAoKRm9yIGEgc2luZ2xlIHNhbXBsZSB3aXRoIEMgY2xhc3NlcywgdGhlIGNhdGVnb3JpY2FsIGNyb3NzLWVudHJvcHkgbG9zcyBpcyBkZWZpbmVkIGFzOgoKJEwgPSAtXHN1bV97Yz0xfV57Q30geV9jIFxsb2cocF9jKSQKCndoZXJlOgoKLSAkeV9jJCBpcyBhIGJpbmFyeSBpbmRpY2F0b3IgKDAgb3IgMSkgaWYgY2xhc3MgbGFiZWwgYyBpcyB0aGUgY29ycmVjdCBjbGFzc2lmaWNhdGlvbiBmb3IgdGhlIHNhbXBsZQotICRwX2MkIGlzIHRoZSBwcmVkaWN0ZWQgcHJvYmFiaWxpdHkgdGhhdCB0aGUgc2FtcGxlIGJlbG9uZ3MgdG8gY2xhc3MgYwotICRDJCBpcyB0aGUgbnVtYmVyIG9mIGNsYXNzZXMKCiMjIyBJbXBsZW1lbnRhdGlvbiBSZXF1aXJlbWVudHMKCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIHRoZSBhdmVyYWdlIGNyb3NzLWVudHJvcHkgbG9zcyBhY3Jvc3MgbXVsdGlwbGUgc2FtcGxlczoKCiRMX3tiYXRjaH0gPSAtXGZyYWN7MX17Tn1cc3VtX3tuPTF9XntOfVxzdW1fe2M9MX1ee0N9IHlfe24sY30gXGxvZyhwX3tuLGN9KSQKCndoZXJlIE4gaXMgdGhlIG51bWJlciBvZiBzYW1wbGVzIGluIHRoZSBiYXRjaC4KCiMjIyBJbXBvcnRhbnQgQ29uc2lkZXJhdGlvbnMKCi0gSGFuZGxlIG51bWVyaWNhbCBzdGFiaWxpdHkgYnkgYWRkaW5nIGEgc21hbGwgZXBzaWxvbiB0byBhdm9pZCBsb2coMCkKLSBFbnN1cmUgcHJlZGljdGVkIHByb2JhYmlsaXRpZXMgc3VtIHRvIDEgZm9yIGVhY2ggc2FtcGxlCi0gUmV0dXJuIGF2ZXJhZ2UgbG9zcyBhY3Jvc3MgYWxsIHNhbXBsZXMKLSBIYW5kbGUgaW52YWxpZCBpbnB1dHMgYXBwcm9wcmlhdGVseQoKVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIHByZWRpY3RlZCBwcm9iYWJpbGl0aWVzIGFuZCB0cnVlIGxhYmVscyBhcyBpbnB1dCBhbmQgcmV0dXJuIHRoZSBhdmVyYWdlIGNyb3NzLWVudHJvcHkgbG9zcy4=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def compute_cross_entropy_loss(self, predicted_probs: np.ndarray, true_labels: np.ndarray,epsilon = 1e-15) -> float:\n\n        predicted_probs = np.clip(predicted_probs, epsilon, 1 - epsilon)\n\n        #Write your code here\n        log_probs = np.log(predicted_probs)\n        loss = -np.sum(true_labels * log_probs, axis=1)\n        return float(np.mean(loss))",
    "test_cases": [
      {
        "test": [
          "import numpy as np\npred = np.array([[1, 0, 0], [0, 1, 0]])\ntrue = np.array([[1, 0, 0], [0, 1, 0]])\nprint(round(compute_cross_entropy_loss(pred, true), 4))"
        ],
        "expected_output": 0.0
      },
      {
        "test": [
          "import numpy as np\npred = np.array([[0.1, 0.8, 0.1], [0.8, 0.1, 0.1]])\ntrue = np.array([[0, 0, 1], [0, 1, 0]])\nprint(round(compute_cross_entropy_loss(pred, true), 4))"
        ],
        "expected_output": 2.3026
      }
    ],
    "function_name": "compute_cross_entropy_loss"
  },
  {
    "id": 135,
    "title": "Implement Early Stopping Based on Validation Loss",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "Q3JlYXRlIGEgZnVuY3Rpb24gdG8gZGVjaWRlIHdoZW4gdG8gc3RvcCB0cmFpbmluZyBhIG1vZGVsIGVhcmx5IGJhc2VkIG9uIGEgbGlzdCBvZiB2YWxpZGF0aW9uIGxvc3Nlcy4gVGhlIGVhcmx5IHN0b3BwaW5nIGNyaXRlcmlvbiBzaG91bGQgc3RvcCB0cmFpbmluZyBpZiB0aGUgdmFsaWRhdGlvbiBsb3NzIGhhc24ndCBpbXByb3ZlZCBmb3IgYSBzcGVjaWZpZWQgbnVtYmVyIG9mIGVwb2NocyAocGF0aWVuY2UpLCBhbmQgb25seSBjb3VudCBhcyBpbXByb3ZlbWVudCBpZiB0aGUgbG9zcyBkZWNyZWFzZXMgYnkgbW9yZSB0aGFuIGEgY2VydGFpbiB0aHJlc2hvbGQgKG1pbl9kZWx0YSkuIFlvdXIgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgZXBvY2ggdG8gc3RvcCBhdCBhbmQgdGhlIGJlc3QgZXBvY2ggdGhhdCBhY2hpZXZlZCB0aGUgbG93ZXN0IHZhbGlkYXRpb24gbG9zcy4=",
    "starter_code": "from typing import Tuple\n\nclass Solution:\n    def early_stopping(self, val_losses: list[float], patience: int, min_delta: float) -> Tuple[int, int]:\n        # Your code here\n        pass",
    "example_input": "[0.9, 0.8, 0.75, 0.77, 0.76, 0.77, 0.78], patience=2, min_delta=0.01",
    "example_output": "(4, 2)",
    "example_reasoning": "The best validation loss is 0.75 at epoch 2. There is no improvement greater than 0.01 for the next 2 epochs. Therefore, training should stop at epoch 4.",
    "learn_content": "IyMgSW1wbGVtZW50aW5nIEVhcmx5IFN0b3BwaW5nIENyaXRlcmlvbgoKRWFybHkgc3RvcHBpbmcgaXMgYSByZWd1bGFyaXphdGlvbiB0ZWNobmlxdWUgdGhhdCBoZWxwcyBwcmV2ZW50IG92ZXJmaXR0aW5nIGluIG1hY2hpbmUgbGVhcm5pbmcgbW9kZWxzLiBZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IHRoZSBlYXJseSBzdG9wcGluZyBkZWNpc2lvbiBsb2dpYyBiYXNlZCBvbiB0aGUgdmFsaWRhdGlvbiBsb3NzIGhpc3RvcnkuCgojIyMgUHJvYmxlbSBEZXNjcmlwdGlvbgoKR2l2ZW4gYSBzZXF1ZW5jZSBvZiB2YWxpZGF0aW9uIGxvc3NlcyBmcm9tIG1vZGVsIHRyYWluaW5nLCBkZXRlcm1pbmUgaWYgdHJhaW5pbmcgc2hvdWxkIGJlIHN0b3BwZWQgYmFzZWQgb24gdGhlIGZvbGxvd2luZyBjcml0ZXJpYToKCi0gVHJhaW5pbmcgc2hvdWxkIHN0b3AgaWYgdGhlIHZhbGlkYXRpb24gbG9zcyBoYXNuJ3QgaW1wcm92ZWQgKGRlY3JlYXNlZCkgZm9yIGEgc3BlY2lmaWVkIG51bWJlciBvZiBlcG9jaHMgKHBhdGllbmNlKQotIEFuIGltcHJvdmVtZW50IGlzIG9ubHkgY291bnRlZCBpZiB0aGUgbG9zcyBkZWNyZWFzZXMgYnkgbW9yZSB0aGFuIGEgbWluaW11bSB0aHJlc2hvbGQgKG1pbl9kZWx0YSkKLSBUaGUgYmVzdCBtb2RlbCBpcyB0aGUgb25lIHdpdGggdGhlIGxvd2VzdCB2YWxpZGF0aW9uIGxvc3MKCiMjIyBFeGFtcGxlCgpDb25zaWRlciB0aGUgZm9sbG93aW5nIHZhbGlkYXRpb24gbG9zc2VzOiBbMC45LCAwLjgsIDAuNzUsIDAuNzcsIDAuNzYsIDAuNzcsIDAuNzhdCgotIFdpdGggcGF0aWVuY2U9MiBhbmQgbWluX2RlbHRhPTAuMDE6CiAgLSBCZXN0IGxvc3MgaXMgMC43NSBhdCBlcG9jaCAyCiAgLSBObyBpbXByb3ZlbWVudCA+IDAuMDEgZm9yIG5leHQgMiBlcG9jaHMKICAtIFNob3VsZCBzdG9wIGF0IGVwb2NoIDQKCiMjIyBGdW5jdGlvbiBSZXF1aXJlbWVudHMKCi0gUmV0dXJuIGJvdGggdGhlIGVwb2NoIHRvIHN0b3AgYXQgYW5kIHRoZSBiZXN0IGVwb2NoCi0gSWYgbm8gc3RvcHBpbmcgaXMgbmVlZGVkLCByZXR1cm4gdGhlIGxhc3QgZXBvY2gKLSBFcG9jaHMgYXJlIDAtaW5kZXhlZA==",
    "solution_code": "from typing import Tuple\n\nclass Solution:\n    def early_stopping(self, val_losses: list[float], patience: int, min_delta: float) -> Tuple[int, int]:\n        best_loss = float('inf')\n        best_epoch = 0\n        epochs_without_improvement = 0\n\n        for epoch, loss in enumerate(val_losses):\n            if loss < best_loss - min_delta:\n                best_loss = loss\n                best_epoch = epoch\n                epochs_without_improvement = 0\n            else:\n                epochs_without_improvement += 1\n\n            if epochs_without_improvement >= patience:\n                return epoch, best_epoch\n\n        return len(val_losses) - 1, best_epoch",
    "test_cases": [
      {
        "test": [
          [
            0.9,
            0.8,
            0.75,
            0.77,
            0.76,
            0.77,
            0.78
          ],
          2,
          0.01
        ],
        "expected_output": "(4, 2)"
      },
      {
        "test": [
          [
            0.9,
            0.8,
            0.7,
            0.6,
            0.5
          ],
          2,
          0.01
        ],
        "expected_output": "(4, 4)"
      }
    ],
    "function_name": "early_stopping"
  },
  {
    "id": 136,
    "title": "Calculate KL Divergence Between Two Multivariate Gaussian Distributions",
    "difficulty": "medium",
    "category": "probability",
    "problem_description": "S0wgZGl2ZXJnZW5jZSBtZWFzdXJlcyB0aGUgZGlzc2ltaWxhcml0eSBiZXR3ZWVuIHR3byBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb25zLiBJbiB0aGlzIHByb2JsZW0sIHlvdSdsbCBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0byBjb21wdXRlIHRoZSBLTCBkaXZlcmdlbmNlIGJldHdlZW4gdHdvIG11bHRpdmFyaWF0ZSBHYXVzc2lhbiBkaXN0cmlidXRpb25zIGdpdmVuIHRoZWlyIG1lYW5zIGFuZCBjb3ZhcmlhbmNlIG1hdHJpY2VzLiBVc2UgdGhlIHByb3ZpZGVkIG1hdGhlbWF0aWNhbCBmb3JtdWxhcyBhbmQgbnVtZXJpY2FsIGNvbnNpZGVyYXRpb25zIHRvIGVuc3VyZSBhY2N1cmFjeS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def multivariate_kl_divergence(self, mu_p: np.ndarray, Cov_p: np.ndarray, mu_q: np.ndarray, Cov_q: np.ndarray) -> float:\n        \"\"\"\n        Computes the KL divergence between two multivariate Gaussian distributions.\n\n        Parameters:\n        mu_p: mean vector of the first distribution\n        Cov_p: covariance matrix of the first distribution\n        mu_q: mean vector of the second distribution\n        Cov_q: covariance matrix of the second distribution\n\n        Returns:\n        KL divergence as a float\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "mu_p, Cov_p, mu_q, Cov_q for two random multivariate Gaussians",
    "example_output": "A float representing the KL divergence",
    "example_reasoning": "The KL divergence is calculated using the formula: 0.5 * (log det term, minus dimension p, Mahalanobis distance between means, and trace term). It measures how dissimilar the second Gaussian is from the first.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def multivariate_kl_divergence(self, mu_p:np.ndarray, Cov_p:np.ndarray, \n                                   mu_q:np.ndarray, Cov_q:np.ndarray) -> float:\n\n        def trace(self, x: np.ndarray) -> float:\n            return np.diag(x).sum()\n\n        p = Cov_p.shape[0]\n        return float(1/2 * (\n            np.log(np.linalg.det(Cov_q)/np.linalg.det(Cov_p))         - p + (mu_p-mu_q).T @ np.linalg.inv(Cov_q) @ (mu_p-mu_q)         + trace(np.linalg.inv(Cov_q) @ Cov_p)\n        ))",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nPx = np.random.randn(4, 10)\nQx = np.random.randn(4, 10)\nmu1, cov1, mu2, cov2 = np.mean(Px, axis=1), np.cov(Px), np.mean(Qx, axis=1), np.cov(Qx)\nprint(round(multivariate_kl_divergence(mu1, cov1, mu2, cov2),4))"
        ],
        "expected_output": 2.193
      },
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nPx = np.random.randn(3, 8)\nQx = np.random.randn(3, 8)\nmu1, cov1, mu2, cov2 = np.mean(Px, axis=1), np.cov(Px), np.mean(Qx, axis=1), np.cov(Qx)\nprint(round(multivariate_kl_divergence(mu1, cov1, mu2, cov2),4))"
        ],
        "expected_output": 1.7741
      }
    ],
    "function_name": "multivariate_kl_divergence"
  },
  {
    "id": 137,
    "title": "Implement a Dense Block with 2D Convolutions",
    "difficulty": "hard",
    "category": "deep learning",
    "problem_description": "Q3JlYXRlIGEgZnVuY3Rpb24gYGRlbnNlX25ldF9ibG9ja2AgdGhhdCBwZXJmb3JtcyB0aGUgZm9yd2FyZCBwYXNzIG9mIGEgKipEZW5zZU5ldCBkZW5zZSBibG9jayoqIG9uIGEgYmF0Y2ggb2YgaW1hZ2VzIHN0b3JlZCBpbiBhbiAqKk5IV0MqKiBOdW1QeSB0ZW5zb3IgYGlucHV0X2RhdGFgIChzaGFwZSBgKE4sIEgsIFcsIEMwKWApLiBUaGUgYmxvY2sgbXVzdCBydW4gYG51bV9sYXllcnNgIGl0ZXJhdGlvbnM7IGF0IGVhY2ggaXRlcmF0aW9uIGl0IHNob3VsZCAoaSkgYXBwbHkgKipSZUxVKiogdG8gdGhlIHJ1bm5pbmcgZmVhdHVyZSB0ZW5zb3IsIChpaSkgY29udm9sdmUgaXQgd2l0aCB0aGUgY29ycmVzcG9uZGluZyBrZXJuZWwgZnJvbSBga2VybmVsc2AgKHVzaW5nIHN0cmlkZSAxLCBubyBiaWFzLCBhbmQgc3ltbWV0cmljIHplcm8tcGFkZGluZyBzbyB0aGF0IGBIYCBhbmQgYFdgIGFyZSBwcmVzZXJ2ZWQpLCBhbmQgKGlpaSkgY29uY2F0ZW5hdGUgdGhlIGNvbnZvbHV0aW9uIG91dHB1dCAod2hvc2UgY2hhbm5lbCBjb3VudCBlcXVhbHMgYGdyb3d0aF9yYXRlYCkgdG8gdGhlIHJ1bm5pbmcgdGVuc29yIGFsb25nIHRoZSBjaGFubmVsIGF4aXMuIEV2ZXJ5IGtlcm5lbCBga2VybmVsc1tsXWAgdGhlcmVmb3JlIGhhcyBzaGFwZSBgKGtoLCBrdywgQzAgKyBsIHggZ3Jvd3RoX3JhdGUsIGdyb3d0aF9yYXRlKWAsIHdoZXJlIGAoa2gsIGt3KWAgZXF1YWxzIGBrZXJuZWxfc2l6ZWAgKGRlZmF1bHQgYCgzLCAzKWApLiBBZnRlciB0aGUgZmluYWwgbGF5ZXIgdGhlIGZ1bmN0aW9uIG11c3QgcmV0dXJuIGEgdGVuc29yIG9mIHNoYXBlIGAoTiwgSCwgVywgQzAgKyBudW1fbGF5ZXJzIHggZ3Jvd3RoX3JhdGUpYC4gSWYgYW55IGtlcm5lbCdzIGlucHV0LWNoYW5uZWwgZGltZW5zaW9uIGRvZXMgbm90IG1hdGNoIHRoZSBjdXJyZW50IGZlYXR1cmUtbWFwIGNoYW5uZWxzLCB0aGUgZnVuY3Rpb24gc2hvdWxkIHJhaXNlIGEgYFZhbHVlRXJyb3JgLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def dense_net_block(self, input_data, num_layers, growth_rate, kernels, kernel_size=(3, 3)):\n        # Your code here\n        pass",
    "example_input": "X = np.random.randn(1, 2, 2, 1); kernels = [np.random.randn(3, 3, 2 + i*1, 1) * 0.01 for i in range(2)]; print(dense_net_block(X, 2, 1, kernels))",
    "example_output": "[[[[ 4.96714153e-01, -1.38264301e-01, -2.30186127e-03, -6.70426255e-05]]]]",
    "example_reasoning": "Each dense block layer concatenates its output to the existing feature maps, expanding the number of output channels by 1 per layer (the growth rate). After 2 layers, the original 2 channels become 4.",
    "learn_content": "IyMgVW5kZXJzdGFuZGluZyBEZW5zZSBCbG9ja3MgYW5kIDJEIENvbnZvbHV0aW9ucwoKRGVuc2UgYmxvY2tzIGFyZSBhIGtleSBpbm5vdmF0aW9uIGluIHRoZSBEZW5zZU5ldCBhcmNoaXRlY3R1cmUuIEVhY2ggbGF5ZXIgcmVjZWl2ZXMgaW5wdXQgZnJvbSAqKmFsbCoqIHByZXZpb3VzIGxheWVycywgbGVhZGluZyB0byByaWNoIGZlYXR1cmUgcmV1c2UgYW5kIGVmZmljaWVudCBncmFkaWVudCBmbG93LgoKIyMjIERlbnNlIEJsb2NrIENvbmNlcHQKRm9yIGEgZGVuc2UgYmxvY2s6Ci0gKipFYWNoIGxheWVyKio6IEFwcGxpZXMgUmVMVSwgdGhlbiAyRCBjb252b2x1dGlvbiwgYW5kIHRoZW4gY29uY2F0ZW5hdGVzIHRoZSBvdXRwdXQgdG8gcHJldmlvdXMgZmVhdHVyZXMuCi0gTWF0aGVtYXRpY2FsbHk6CiQkCnhfbCA9IEhfbChbeF8wLCB4XzEsIFxsZG90cywgeF97bC0xfV0pCiQkCndoZXJlICRIX2woXGNkb3QpJCBpcyB0aGUgY29udm9sdXRpb24gYW5kIGFjdGl2YXRpb24gb3BlcmF0aW9ucy4KCiMjIyAyRCBDb252b2x1dGlvbiBCYXNpY3MKQSAyRCBjb252b2x1dGlvbiBhdCBhIHBvc2l0aW9uICQoaSwgaikkIGZvciBpbnB1dCAkWCQgYW5kIGtlcm5lbCAkSyQgaXM6CiQkCllbaSwgal0gPSBcc3VtX3ttPTB9XntrX2ggLSAxfSBcc3VtX3tuPTB9XntrX3cgLSAxfSBYW2kgKyBtLCBqICsgbl0gXGNkb3QgS1ttLCBuXQokJAoKIyMjIFBhZGRpbmcgdG8gUHJlc2VydmUgU3BhdGlhbCBEaW1lbnNpb25zClRvIHByZXNlcnZlIGhlaWdodCBhbmQgd2lkdGg6CiQkClx0ZXh0e3BhZGRpbmd9ID0gXGZyYWN7ayAtIDF9ezJ9CiQkCgojIyMgRGVuc2UgQmxvY2sgR3Jvd3RoCi0gRWFjaCBsYXllciBhZGRzICRcdGV4dHtncm93dGggcmF0ZX0kIGNoYW5uZWxzLgotIEFmdGVyICRMJCBsYXllcnMsIHRvdGFsIGNoYW5uZWxzID0gaW5wdXQgY2hhbm5lbHMgKyAkTCBcdGltZXMgXHRleHR7Z3Jvd3RoIHJhdGV9JC4KCiMjIyBQdXR0aW5nIEl0IEFsbCBUb2dldGhlcgox77iP4oOjIFN0YXJ0IHdpdGggYW4gaW5wdXQgdGVuc29yLiAgCjLvuI/ig6MgUmVwZWF0IGZvciAkXHRleHR7bnVtIGxheWVyc30kOgotIEFwcGx5IFJlTFUgYWN0aXZhdGlvbi4KLSBBcHBseSAyRCBjb252b2x1dGlvbiAod2l0aCBwYWRkaW5nKS4KLSBDb25jYXRlbmF0ZSB0aGUgb3V0cHV0IGFsb25nIHRoZSBjaGFubmVsIGRpbWVuc2lvbi4KCkJ5IHVuZGVyc3RhbmRpbmcgdGhlc2UgY29yZSBwcmluY2lwbGVzLCB5b3XigJlyZSByZWFkeSB0byBidWlsZCB0aGUgZGVuc2UgYmxvY2sgZnVuY3Rpb24h",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def conv2d(self, x, kernel, padding=0):\n        if padding > 0:\n            x_padded = np.pad(x, ((0, 0), (padding, padding), (padding, padding), (0, 0)), mode='constant')\n        else:\n            x_padded = x\n        batch_size, in_height, in_width, in_channels = x_padded.shape\n        kh, kw, _, out_channels = kernel.shape\n        out_height = in_height - kh + 1\n        out_width = in_width - kw + 1\n        output = np.zeros((batch_size, out_height, out_width, out_channels))\n        for b in range(batch_size):\n            for i in range(out_height):\n                for j in range(out_width):\n                    for c_out in range(out_channels):\n                        sum_val = 0.0\n                        for c_in in range(in_channels):\n                            sum_val += np.sum(x_padded[b, i:i+kh, j:j+kw, c_in] * kernel[:, :, c_in, c_out])\n                        output[b, i, j, c_out] = sum_val\n        return output\n\n    def dense_net_block(self, input_data, num_layers, growth_rate, kernels, kernel_size=(3, 3)):\n        kh, kw = kernel_size\n        padding = (kh - 1) // 2\n        concatenated_features = input_data.copy()\n        for l in range(num_layers):\n            activated = np.maximum(concatenated_features, 0.0)\n            conv_output = conv2d(activated, kernels[l], padding=padding)\n            concatenated_features = np.concatenate([concatenated_features, conv_output], axis=3)\n        return concatenated_features",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nX = np.random.randn(1, 1, 1, 2)\nkernels = [np.random.randn(3, 3, 2 + i*1, 1) * 0.01 for i in range(2)]\nprint(dense_net_block(X, 2, 1, kernels))"
        ],
        "expected_output": [
          [
            [
              [
                0.496714153,
                -0.138264301,
                -0.00230186127,
                -6.70426255e-05
              ]
            ]
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\n\nnp.random.seed(42)\nX = np.random.randn(1, 2, 3, 2)\nkernels = [np.random.randn(3, 3, 2 + i*1, 1) * 0.01 for i in range(2)]\nprint(dense_net_block(X, 2, 1, kernels))"
        ],
        "expected_output": [
          [
            [
              [
                0.49671415,
                -0.1382643,
                -0.0308579,
                -0.01845547
              ],
              [
                0.64768854,
                1.52302986,
                -0.0041634,
                -0.0161227
              ],
              [
                -0.23415337,
                -0.23413696,
                -0.02678915,
                0.00295656
              ]
            ],
            [
              [
                1.57921282,
                0.76743473,
                0.00334109,
                -0.04043312
              ],
              [
                -0.46947439,
                0.54256004,
                -0.04493715,
                0.00983633
              ],
              [
                -0.46341769,
                -0.46572975,
                -0.03523526,
                0.02832019
              ]
            ]
          ]
        ]
      }
    ],
    "function_name": "dense_net_block"
  },
  {
    "id": 138,
    "title": "Find the Best Gini-Based Split for a Binary Decision Tree",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBzY2FucyBldmVyeSBmZWF0dXJlIGFuZCB0aHJlc2hvbGQgaW4gYSBzbWFsbCBkYXRhIHNldCwgdGhlbiByZXR1cm5zIHRoZSBzcGxpdCB0aGF0IG1pbmltaXNlcyB0aGUgd2VpZ2h0ZWQgR2luaSBpbXB1cml0eS4gWW91ciBpbXBsZW1lbnRhdGlvbiBzaG91bGQgc3VwcG9ydCBiaW5hcnkgY2xhc3MgbGFiZWxzICgwIG9yIDEpIGFuZCBoYW5kbGUgdGllcyBncmFjZWZ1bGx5LiAgCgpZb3Ugd2lsbCB3cml0ZSAqKm9uZSoqIGZ1bmN0aW9uOgoKYGBgcHl0aG9uCmZpbmRfYmVzdF9zcGxpdChYOiBucC5uZGFycmF5LCB5OiBucC5uZGFycmF5KSAtPiB0dXBsZVtpbnQsIGZsb2F0XQpgYGAKCiogKipgWGAqKiBpcyBhbiAkblx0aW1lcyBkJCBOdW1QeSBhcnJheSBvZiBudW1lcmljIGZlYXR1cmVzLgoqICoqYHlgKiogaXMgYSBsZW5ndGgtJG4kIE51bVB5IGFycmF5IG9mIDAvMSBsYWJlbHMuCiogVGhlIGZ1bmN0aW9uIHJldHVybnMgYChiZXN0X2ZlYXR1cmVfaW5kZXgsIGJlc3RfdGhyZXNob2xkKWAgZm9yIHRoZSBzcGxpdCB3aXRoIHRoZSAqKmxvd2VzdCoqIHdlaWdodGVkIEdpbmkgaW1wdXJpdHkuCiogSWYgc2V2ZXJhbCBzcGxpdHMgc2hhcmUgdGhlIHNhbWUgaW1wdXJpdHksIHJldHVybiB0aGUgZmlyc3QgdGhhdCB5b3UgZW5jb3VudGVyIHdoaWxlIHNjYW5uaW5nIGZlYXR1cmVzIGFuZCB0aHJlc2hvbGRzLg==",
    "starter_code": "import numpy as np\nfrom typing import Tuple\n\nclass Solution:\n    def find_best_split(self, X: np.ndarray, y: np.ndarray) -> Tuple[int, float]:\n        \"\"\"Return the (feature_index, threshold) that minimises weighted Gini impurity.\"\"\"\n        # \u270f\ufe0f TODO: implement\n        pass",
    "example_input": "import numpy as np\nX = np.array([[2.5],[3.5],[1.0],[4.0]])\ny = np.array([0,1,0,1])\nprint(find_best_split(X, y))",
    "example_output": "(0, 2.5)",
    "example_reasoning": "Splitting on feature 0 at threshold 2.5 yields two perfectly pure leaves, producing the minimum possible weighted Gini impurity.",
    "learn_content": "IyBMZWFybjogR2luaSBJbXB1cml0eSBhbmQgQmVzdCBTcGxpdCBpbiBEZWNpc2lvbiBUcmVlcwoKIyMgT3ZlcnZpZXcKCkEgY29yZSBjb25jZXB0IGluIERlY2lzaW9uIFRyZWVzIChhbmQgYnkgZXh0ZW5zaW9uLCBSYW5kb20gRm9yZXN0cykgaXMgaG93IHRoZSBtb2RlbCBjaG9vc2VzIHdoZXJlIHRvIHNwbGl0IHRoZSBkYXRhIGF0IGVhY2ggbm9kZS4gT25lIHBvcHVsYXIgY3JpdGVyaW9uIHVzZWQgZm9yIHNwbGl0dGluZyBpcyAqKkdpbmkgSW1wdXJpdHkqKi4KCkluIHRoaXMgdGFzaywgeW91IHdpbGwgaW1wbGVtZW50OgotIEdpbmkgaW1wdXJpdHkgY29tcHV0YXRpb24KLSBGaW5kaW5nIHRoZSBiZXN0IGZlYXR1cmUgYW5kIHRocmVzaG9sZCB0byBzcGxpdCBvbiBiYXNlZCBvbiBpbXB1cml0eSByZWR1Y3Rpb24KClRoaXMgaGVscHMgYnVpbGQgdGhlIGZvdW5kYXRpb24gZm9yIGhvdyB0cmVlcyBncm93IGluIGEgUmFuZG9tIEZvcmVzdC4KCi0tLQoKIyMgR2luaSBJbXB1cml0eQoKRm9yIGEgc2V0IG9mIHNhbXBsZXMgd2l0aCBjbGFzcyBsYWJlbHMgXCggeSBcKSwgdGhlIEdpbmkgSW1wdXJpdHkgaXMgZGVmaW5lZCBhczoKCiQkCkcoeSkgPSAxIC0gXHN1bV97aT0xfV57a30gcF9pXjIKJCQKCldoZXJlIFwoIHBfaSBcKSBpcyB0aGUgcHJvcG9ydGlvbiBvZiBzYW1wbGVzIGJlbG9uZ2luZyB0byBjbGFzcyBcKCBpIFwpLgoKQSBwdXJlIG5vZGUgKGFsbCBvbmUgY2xhc3MpIGhhcyBcKCBHID0gMCBcKSwgYW5kIGhpZ2hlciB2YWx1ZXMgaW5kaWNhdGUgbW9yZSBjbGFzcyBkaXZlcnNpdHkuCgotLS0KCiMjIFdlaWdodGVkIEdpbmkgSW1wdXJpdHkKCkdpdmVuIGEgZmVhdHVyZSBhbmQgYSB0aHJlc2hvbGQgdG8gc3BsaXQgdGhlIGRhdGFzZXQgaW50byBsZWZ0IGFuZCByaWdodCBzdWJzZXRzOgoKJCQKR197XHRleHR7c3BsaXR9fSA9IFxmcmFje25fe1x0ZXh0e2xlZnR9fX17bn0gRyh5X3tcdGV4dHtsZWZ0fX0pICsgXGZyYWN7bl97XHRleHR7cmlnaHR9fX17bn0gRyh5X3tcdGV4dHtyaWdodH19KQokJAoKV2UgY2hvb3NlIHRoZSBzcGxpdCB0aGF0ICoqbWluaW1pemVzKiogJCggR197XHRleHR7c3BsaXR9fSApJC4KCi0tLQoKIyMgUHJvYmxlbSBTdGF0ZW1lbnQKCllvdSBhcmUgZ2l2ZW4gYSBkYXRhc2V0ICQoIFggXGluIFxtYXRoYmJ7Un1ee24gXHRpbWVzIGR9ICkkIGFuZCBsYWJlbHMgJCggeSBcaW4gXHswLCAxXH1ebiAkKS4gSW1wbGVtZW50IHRoZSBmb2xsb3dpbmcgZnVuY3Rpb25zOgoKIyMjIEZ1bmN0aW9ucyB0byBJbXBsZW1lbnQKCmBgYHB5dGhvbgpkZWYgZmluZF9iZXN0X3NwbGl0KFg6IG5wLm5kYXJyYXksIHk6IG5wLm5kYXJyYXkpIC0+IFR1cGxlW2ludCwgZmxvYXRdOgogICAgLi4uCmBgYA==",
    "solution_code": "import numpy as np\nfrom typing import Tuple\n\nclass Solution:\n    def find_best_split(self, X: np.ndarray, y: np.ndarray) -> Tuple[int, float]:\n        def gini(self, y_subset: np.ndarray) -> float:\n            if y_subset.size == 0:\n                return 0.0\n            p = y_subset.mean()\n            return 1.0 - (p**2 + (1 - p)**2)\n\n        n_samples, n_features = X.shape\n        best_feature, best_threshold = -1, float('inf')\n        best_gini = float('inf')\n\n        for f in range(n_features):\n            for threshold in np.unique(X[:, f]):\n                left = y[X[:, f] <= threshold]\n                right = y[X[:, f] > threshold]\n                g_left, g_right = gini(left), gini(right)\n                weighted = (len(left) * g_left + len(right) * g_right) / n_samples\n                if weighted < best_gini:\n                    best_gini, best_feature, best_threshold = weighted, f, threshold\n\n        return best_feature, best_threshold",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nX1 = np.array([[2.5], [3.5], [1.0], [4.0]])\ny1 = np.array([0, 1, 0, 1])\nf1, t1 = find_best_split(X1, y1)\nprint(f1, round(t1, 4))"
        ],
        "expected_output": "0, 2.5"
      },
      {
        "test": [
          "import numpy as np\nX2 = np.array([[1], [2], [3]])\ny2 = np.array([1, 1, 1])\nf2, t2 = find_best_split(X2, y2)\nprint(f2, t2)"
        ],
        "expected_output": "0, 1"
      }
    ],
    "function_name": "find_best_split"
  },
  {
    "id": 139,
    "title": "Elastic Net Regression via Gradient Descent",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IEVsYXN0aWMgTmV0IFJlZ3Jlc3Npb24gdXNpbmcgZ3JhZGllbnQgZGVzY2VudCwgY29tYmluaW5nIEwxIGFuZCBMMiBwZW5hbHRpZXMgdG8gaGFuZGxlIG11bHRpY29sbGluZWFyaXR5IGFuZCBlbmNvdXJhZ2Ugc3BhcnNpdHkgaW4gdGhlIGZlYXR1cmUgd2VpZ2h0cy4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def elastic_net_gradient_descent(self, \n        X: np.ndarray,\n        y: np.ndarray,\n        alpha1: float = 0.1,\n        alpha2: float = 0.1,\n        learning_rate: float = 0.01,\n        max_iter: int = 1000,\n        tol: float = 1e-4,\n    ) -> tuple:\n        # Implement Elastic Net regression here\n        pass",
    "example_input": "X = np.array([[0, 0], [1, 1], [2, 2]]); y = np.array([0, 1, 2])",
    "example_output": "(array([0.37, 0.37]), 0.25)",
    "example_reasoning": "The model learns a nearly perfect linear relationship with regularization controlling weight magnitude. The weights converge around 0.37 with a bias around 0.25.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def elastic_net_gradient_descent(self, \n        X: np.ndarray,\n        y: np.ndarray,\n        alpha1: float = 0.1,\n        alpha2: float = 0.1,\n        learning_rate: float = 0.01,\n        max_iter: int = 1000,\n        tol: float = 1e-4,\n    ) -> tuple:\n        n_samples, n_features = X.shape\n        weights = np.zeros(n_features)\n        bias = 0\n\n        for _ in range(max_iter):\n            y_pred = np.dot(X, weights) + bias\n            error = y_pred - y\n            grad_w = (1 / n_samples) * np.dot(X.T, error) + alpha1 * np.sign(weights) + 2 * alpha2 * weights\n            grad_b = (1 / n_samples) * np.sum(error)\n            weights -= learning_rate * grad_w\n            bias -= learning_rate * grad_b\n            if np.linalg.norm(grad_w, ord=1) < tol:\n                break\n\n        return weights, bias",
    "test_cases": [
      {
        "test": [
          "X = np.array([[0, 0], [1, 1], [2, 2]])\ny = np.array([0, 1, 2])\nw, b = elastic_net_gradient_descent(X, y, alpha1=0.1, alpha2=0.1, learning_rate=0.01, max_iter=1000)\nprint(np.round(w,2), round(b,2))"
        ],
        "expected_output": [
          [
            0.37,
            0.37
          ],
          0.25
        ]
      },
      {
        "test": [
          "X = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 2, 3, 4, 5])\nw, b = elastic_net_gradient_descent(X, y, alpha1=0.1, alpha2=0.1, learning_rate=0.01, max_iter=2000)\nprint(np.round(w,2),round(b,2))"
        ],
        "expected_output": [
          [
            0.43,
            0.48
          ],
          0.69
        ]
      }
    ],
    "function_name": "elastic_net_gradient_descent"
  },
  {
    "id": 140,
    "title": "Bernoulli Naive Bayes Classifier",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gY2xhc3MgdG8gaW1wbGVtZW50IHRoZSBCZXJub3VsbGkgTmFpdmUgQmF5ZXMgY2xhc3NpZmllciBmb3IgYmluYXJ5ICgwLzEpIGZlYXR1cmUgZGF0YS4gWW91ciBjbGFzcyBzaG91bGQgaGF2ZSB0d28gbWV0aG9kczogYGZvcndhcmQoc2VsZiwgWCwgeSlgIHRvIHRyYWluIG9uIHRoZSBpbnB1dCBkYXRhIChYOiAyRCBOdW1QeSBhcnJheSBvZiBiaW5hcnkgZmVhdHVyZXMsIHk6IDFEIE51bVB5IGFycmF5IG9mIGNsYXNzIGxhYmVscykgYW5kIGBwcmVkaWN0KHNlbGYsIFgpYCB0byBvdXRwdXQgcHJlZGljdGVkIGxhYmVscyBmb3IgYSAyRCB0ZXN0IG1hdHJpeCBYLiBVc2UgTGFwbGFjZSBzbW9vdGhpbmcgKHBhcmFtZXRlcjogc21vb3RoaW5nPTEuMCkuIFJldHVybiBwcmVkaWN0aW9ucyBhcyBhIE51bVB5IGFycmF5LiBPbmx5IHVzZSBOdW1QeS4gUHJlZGljdGlvbnMgbXVzdCBiZSBiaW5hcnkgKDAgb3IgMSkgYW5kIHlvdSBtdXN0IGhhbmRsZSBjYXNlcyB3aGVyZSB0aGUgdHJhaW5pbmcgZGF0YSBjb250YWlucyBvbmx5IG9uZSBjbGFzcy4gQWxsIGxvZy9saWtlbGlob29kIGNhbGN1bGF0aW9ucyBzaG91bGQgdXNlIGxvZyBwcm9iYWJpbGl0aWVzIGZvciBudW1lcmljYWwgc3RhYmlsaXR5Lg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    class NaiveBayes():\n    def __init__(self, self, smoothing=1.0):\n        # Initialize smoothing\n        pass\n\n    def forward(self, self, X, y):\n        # Fit model to binary features X and labels y\n        pass\n\n    def predict(self, self, X):\n        # Predict class labels for test set X\n        pass",
    "example_input": "X = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 0], [1, 1, 1]]); y = np.array([1, 1, 0, 0, 1])\nmodel = NaiveBayes(smoothing=1.0)\nmodel.forward(X, y)\nprint(model.predict(np.array([[1, 0, 1]])))",
    "example_output": "[1]",
    "example_reasoning": "The model learns class priors and feature probabilities with Laplace smoothing. For [1, 0, 1], the posterior for class 1 is higher, so the model predicts 1.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    class NaiveBayes():\n    def __init__(self, self, smoothing=1.0):\n        self.smoothing = smoothing\n        self.classes = None\n        self.priors = None\n        self.likelihoods = None\n\n    def forward(self, self, X, y):\n        self.classes, class_counts = np.unique(y, return_counts=True)\n        self.priors = {cls: np.log(class_counts[i] / len(y)) for i, cls in enumerate(self.classes)}\n        self.likelihoods = {}\n        for cls in self.classes:\n            X_cls = X[y == cls]\n            prob = (np.sum(X_cls, axis=0) + self.smoothing) / (X_cls.shape[0] + 2 * self.smoothing)\n            self.likelihoods[cls] = (np.log(prob), np.log(1 - prob))\n\n    def _compute_posterior(self, self, sample):\n        posteriors = {}\n        for cls in self.classes:\n            posterior = self.priors[cls]\n            prob_1, prob_0 = self.likelihoods[cls]\n            likelihood = np.sum(sample * prob_1 + (1 - sample) * prob_0)\n            posterior += likelihood\n            posteriors[cls] = posterior\n        return max(posteriors, key=posteriors.get)\n\n    def predict(self, self, X):\n        return np.array([self._compute_posterior(sample) for sample in X])",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nmodel = NaiveBayes(smoothing=1.0)\nX = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 0], [1, 1, 1]])\ny = np.array([1, 1, 0, 0, 1])\nmodel.forward(X, y)\nprint(model.predict(np.array([[1, 0, 1]])))"
        ],
        "expected_output": [
          1
        ]
      },
      {
        "test": [
          "import numpy as np\nmodel = NaiveBayes(smoothing=1.0)\nX = np.array([[0], [1], [0], [1]])\ny = np.array([0, 1, 0, 1])\nmodel.forward(X, y)\nprint(model.predict(np.array([[0], [1]])))"
        ],
        "expected_output": "[0 1]"
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 141,
    "title": "Shift and Scale Array to Target Range",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYGNvbnZlcnRfcmFuZ2VgIHRoYXQgc2hpZnRzIGFuZCBzY2FsZXMgdGhlIHZhbHVlcyBvZiBhIE51bVB5IGFycmF5IGZyb20gdGhlaXIgb3JpZ2luYWwgcmFuZ2UgJFthLCBiXSQgKHdoZXJlICRhPVxtaW4oeCkkIGFuZCAkYj1cbWF4KHgpJCkgdG8gYSBuZXcgdGFyZ2V0IHJhbmdlICRbYywgZF0kLiBZb3VyIGZ1bmN0aW9uIHNob3VsZCB3b3JrIGZvciBib3RoIDFEIGFuZCAyRCBhcnJheXMsIHJldHVybmluZyBhbiBhcnJheSBvZiB0aGUgc2FtZSBzaGFwZSwgYW5kIG9ubHkgdXNlIE51bVB5LiBSZXR1cm4gZmxvYXRpbmctcG9pbnQgcmVzdWx0cywgYW5kIGVuc3VyZSB5b3UgdXNlIHRoZSBjb3JyZWN0IGZvcm11bGEgdG8gbWFwIHRoZSBpbnB1dCBpbnRlcnZhbCB0byB0aGUgb3V0cHV0IGludGVydmFsLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def convert_range(self, values: np.ndarray, c: float, d: float) -> np.ndarray:\n        \"\"\"\n        Shift and scale values from their original range [min, max] to a target [c, d] range.\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "import numpy as np\nx = np.array([0, 5, 10])\nc, d = 2, 4\nprint(convert_range(x, c, d))",
    "example_output": "[2. 3. 4.]",
    "example_reasoning": "The minimum value (a) is 0 and the maximum value (b) is 10. The formula maps 0 to 2, 5 to 3, and 10 to 4 using: f(x) = c + (d-c)/(b-a)*(x-a).",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def convert_range(self, values: np.ndarray, c: float, d: float) -> np.ndarray:\n        \"\"\"\n        Shift and scale values from their original range [min, max] to a target [c, d] range.\n\n        Parameters\n        ----------\n        values : np.ndarray\n            Input array (1D or 2D) to be rescaled.\n        c : float\n            New range lower bound.\n        d : float\n            New range upper bound.\n\n        Returns\n        -------\n        np.ndarray\n            Scaled array with the same shape as the input.\n        \"\"\"\n        a, b = values.min(), values.max()\n        return c + (d - c) / (b - a) * (values - a)",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nseq = np.array([388, 242, 124, 384, 313, 277, 339, 302, 268, 392])\nc, d = 0, 1\nout = convert_range(seq, c, d)\nprint(np.round(out, 6))"
        ],
        "expected_output": [
          0.985075,
          0.440299,
          0.0,
          0.970149,
          0.705224,
          0.570896,
          0.802239,
          0.664179,
          0.537313,
          1.0
        ]
      },
      {
        "test": [
          "import numpy as np\nseq = np.array([[2028, 4522], [1412, 2502], [3414, 3694], [1747, 1233], [1862, 4868]])\nc, d = 4, 8\nout = convert_range(seq, c, d)\nprint(np.round(out, 6))"
        ],
        "expected_output": "[[4.874828 7.619257]\n [4.196974 5.396424]\n [6.4      6.708116]\n [4.565612 4.      ]\n [4.69216  8.      ]]"
      }
    ],
    "function_name": "convert_range"
  },
  {
    "id": 142,
    "title": "Gridworld Policy Evaluation",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "SW1wbGVtZW50IHBvbGljeSBldmFsdWF0aW9uIGZvciBhIDV4NSBncmlkd29ybGQuIEdpdmVuIGEgcG9saWN5IChtYXBwaW5nIGVhY2ggc3RhdGUgdG8gYWN0aW9uIHByb2JhYmlsaXRpZXMpLCBjb21wdXRlIHRoZSBzdGF0ZS12YWx1ZSBmdW5jdGlvbiAkVihzKSQgZm9yIGVhY2ggY2VsbCB1c2luZyB0aGUgQmVsbG1hbiBleHBlY3RhdGlvbiBlcXVhdGlvbi4gVGhlIGFnZW50IGNhbiBtb3ZlIHVwLCBkb3duLCBsZWZ0LCBvciByaWdodCwgcmVjZWl2aW5nIGEgY29uc3RhbnQgcmV3YXJkIG9mIC0xIGZvciBlYWNoIG1vdmUuIFRlcm1pbmFsIHN0YXRlcyAodGhlIGZvdXIgY29ybmVycykgYXJlIGZpeGVkIGF0IDAuIEl0ZXJhdGUgdW50aWwgdGhlIGxhcmdlc3QgY2hhbmdlIGluICRWJCBpcyBsZXNzIHRoYW4gYSBnaXZlbiB0aHJlc2hvbGQuIE9ubHkgdXNlIFB5dGhvbiBidWlsdC1pbnMgYW5kIG5vIGV4dGVybmFsIFJMIGxpYnJhcmllcy4=",
    "starter_code": "\n\nclass Solution:\n    def gridworld_policy_evaluation(self, policy: dict, gamma: float, threshold: float) -> list[list[float]]:\n        \"\"\"\n        Evaluate state-value function for a policy on a 5x5 gridworld.\n\n        Args:\n            policy: dict mapping (row, col) to action probability dicts\n            gamma: discount factor\n            threshold: convergence threshold\n        Returns:\n            5x5 list of floats\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "policy = {(i, j): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25} for i in range(5) for j in range(5)}\ngamma = 0.9\nthreshold = 0.001\nV = gridworld_policy_evaluation(policy, gamma, threshold)\nprint(round(V[2][2], 4))",
    "example_output": "-7.0902",
    "example_reasoning": "The policy is uniform (equal chance of each move). The agent receives -1 per step. After iterative updates, the center state value converges to about -7.09, and corners remain at 0.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def gridworld_policy_evaluation(self, policy: dict, gamma: float, threshold: float) -> list[list[float]]:\n        grid_size = 5\n        V = [[0.0 for _ in range(grid_size)] for _ in range(grid_size)]\n        actions = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n        reward = -1\n        while True:\n            delta = 0.0\n            new_V = [row[:] for row in V]\n            for i in range(grid_size):\n                for j in range(grid_size):\n                    if (i, j) in [(0, 0), (0, grid_size-1), (grid_size-1, 0), (grid_size-1, grid_size-1)]:\n                        continue\n                    v = 0.0\n                    for action, prob in policy[(i, j)].items():\n                        di, dj = actions[action]\n                        ni = i + di if 0 <= i + di < grid_size else i\n                        nj = j + dj if 0 <= j + dj < grid_size else j\n                        v += prob * (reward + gamma * V[ni][nj])\n                    new_V[i][j] = v\n                    delta = max(delta, abs(V[i][j] - new_V[i][j]))\n            V = new_V\n            if delta < threshold:\n                break\n        return V",
    "test_cases": [
      {
        "test": [
          "grid_size = 5\ngamma = 0.9\nthreshold = 0.001\npolicy = {(i, j): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25} for i in range(grid_size) for j in range(grid_size)}\nV = gridworld_policy_evaluation(policy, gamma, threshold)\nprint([round(V[2][2], 4), V[0][0], V[0][4], V[4][0], V[4][4]])"
        ],
        "expected_output": [
          -7.0902,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      {
        "test": [
          "grid_size = 5\ngamma = 0.9\nthreshold = 0.001\npolicy = {(i, j): {'up': 0.1, 'down': 0.4, 'left': 0.1, 'right': 0.4} for i in range(grid_size) for j in range(grid_size)}\nV = gridworld_policy_evaluation(policy, gamma, threshold)\nprint(round(V[1][3], 4) < 0)"
        ],
        "expected_output": "True"
      }
    ],
    "function_name": "gridworld_policy_evaluation"
  },
  {
    "id": 143,
    "title": "Instance Normalization (IN) Implementation",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBJbnN0YW5jZSBOb3JtYWxpemF0aW9uIG9wZXJhdGlvbiBmb3IgNEQgdGVuc29ycyAoQiwgQywgSCwgVykgdXNpbmcgTnVtUHkuIEZvciBlYWNoIGluc3RhbmNlIGluIHRoZSBiYXRjaCBhbmQgZWFjaCBjaGFubmVsLCBub3JtYWxpemUgdGhlIHNwYXRpYWwgZGltZW5zaW9ucyAoaGVpZ2h0IGFuZCB3aWR0aCkgYnkgc3VidHJhY3RpbmcgdGhlIG1lYW4gYW5kIGRpdmlkaW5nIGJ5IHRoZSBzdGFuZGFyZCBkZXZpYXRpb24sIHRoZW4gYXBwbHkgYSBsZWFybmVkIHNjYWxlIChnYW1tYSkgYW5kIHNoaWZ0IChiZXRhKS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def instance_normalization(self, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n        \"\"\"\n        Perform Instance Normalization over a 4D tensor X of shape (B, C, H, W).\n        gamma: scale parameter of shape (C,)\n        beta: shift parameter of shape (C,)\n        epsilon: small value for numerical stability\n        Returns: normalized array of same shape as X\n        \"\"\"\n        # TODO: Implement Instance Normalization\n        pass",
    "example_input": "import numpy as np\nB, C, H, W = 2, 2, 2, 2\nnp.random.seed(42)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C)\nbeta = np.zeros(C)\nout = instance_normalization(X, gamma, beta)\nprint(np.round(out, 8))",
    "example_output": "[[[[-0.08841405, -0.50250083]\n   [ 0.01004046, 0.58087442]]\n\n  [[-0.43833369, -0.43832346]\n   [ 0.69114093, 0.18551622]]]\n\n [[[-0.17259136, 0.51115219]\n   [-0.16849938, -0.17006144]]\n\n  [[ 0.73955155, -0.55463639]\n   [-0.44152783, 0.25661268]]]]",
    "example_reasoning": "The function normalizes each instance and channel across (H, W), then applies the gamma and beta scaling/shifting parameters. This matches standard InstanceNorm behavior.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def instance_normalization(self, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n        # Reshape gamma, beta for broadcasting: (1, C, 1, 1)\n        gamma = gamma.reshape(1, -1, 1, 1)\n        beta = beta.reshape(1, -1, 1, 1)\n        mean = np.mean(X, axis=(2, 3), keepdims=True)\n        var = np.var(X, axis=(2, 3), keepdims=True)\n        X_norm = (X - mean) / np.sqrt(var + epsilon)\n        return gamma * X_norm + beta",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nB, C, H, W = 2, 2, 2, 2\nnp.random.seed(42)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C)\nbeta = np.zeros(C)\nout = instance_normalization(X, gamma, beta)\nprint(np.round(out[1,1], 4))"
        ],
        "expected_output": [
          [
            1.4005,
            -1.0503
          ],
          [
            -0.8361,
            0.486
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\nB, C, H, W = 2, 2, 2, 2\nnp.random.seed(101)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C)\nbeta = np.zeros(C)\nout = instance_normalization(X, gamma, beta)\nprint(np.round(out[1,0], 4))"
        ],
        "expected_output": [
          [
            -1.537,
            0.9811
          ],
          [
            0.7882,
            -0.2323
          ]
        ]
      }
    ],
    "function_name": "instance_normalization"
  },
  {
    "id": 144,
    "title": "Apriori Frequent Itemset Mining",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBBcHJpb3JpIGFsZ29yaXRobSB0byBkaXNjb3ZlciBhbGwgZnJlcXVlbnQgaXRlbXNldHMgaW4gYSBzZXQgb2YgdHJhbnNhY3Rpb25zLCBnaXZlbiBhIG1pbmltdW0gc3VwcG9ydCB0aHJlc2hvbGQuIFlvdXIgZnVuY3Rpb24gc2hvdWxkIHJldHVybiBhbGwgZnJlcXVlbnQgaXRlbXNldHMgKG9mIGFueSBzaXplKSB3aG9zZSBzdXBwb3J0IGlzIGF0IGxlYXN0IHRoZSBnaXZlbiBtaW5pbXVtLiBSZXR1cm4gdGhlIGZyZXF1ZW50IGl0ZW1zZXRzIGFzIGEgZGljdGlvbmFyeSBtYXBwaW5nIGZyb3plbnNldCBvZiBpdGVtcyB0byB0aGVpciBzdXBwb3J0IChmcmFjdGlvbmFsKS4gT25seSB1c2UgYnVpbHQtaW4gUHl0aG9uIGFuZCBzdGFuZGFyZCBsaWJyYXJpZXMgKGNvbGxlY3Rpb25zLCBpdGVydG9vbHMpLg==",
    "starter_code": "import itertools\nfrom collections import defaultdict\n\nclass Solution:\n    def apriori(self, transactions, min_support=0.5, max_length=None):\n        \"\"\"\n        Returns: dict mapping frozenset(itemset) -> support (float)\n        \"\"\"\n        # TODO: Implement the Apriori algorithm\n        pass",
    "example_input": "transactions = [\n    {'bread', 'milk'},\n    {'bread', 'diaper', 'beer', 'eggs'},\n    {'milk', 'diaper', 'beer', 'cola'},\n    {'bread', 'milk', 'diaper', 'beer'},\n    {'bread', 'milk', 'diaper', 'cola'}\n]\nresult = apriori(transactions, min_support=0.6)\nfor k in sorted(result, key=lambda x: (len(x), sorted(x))):\n    print(sorted(list(k)), round(result[k], 2))",
    "example_output": "['bread'] 0.8\n['diaper'] 0.8\n['milk'] 0.8\n['bread', 'diaper'] 0.6\n['bread', 'milk'] 0.6\n['milk', 'diaper'] 0.6",
    "example_reasoning": "Bread, Milk, and Diaper each appear in 4 out of 5 transactions (support=0.8), and the 2-itemsets {'bread','milk'}, {'bread','diaper'}, and {'milk','diaper'} each appear in 3 out of 5 (support=0.6). Only these satisfy the min_support threshold.",
    "learn_content": "IyAqKkFwcmlvcmkgQWxnb3JpdGhtIGZvciBGcmVxdWVudCBJdGVtc2V0IE1pbmluZyoqCgpUaGUgQXByaW9yaSBhbGdvcml0aG0gaXMgYSBjbGFzc2ljIGFwcHJvYWNoIHRvIGRpc2NvdmVyaW5nICoqZnJlcXVlbnQgaXRlbXNldHMqKiBmb3IgYXNzb2NpYXRpb24gcnVsZSBtaW5pbmcgaW4gdHJhbnNhY3Rpb25hbCBkYXRhLgoKLS0tCgojIyAqKkhvdyBBcHJpb3JpIFdvcmtzKioKCioqR29hbDoqKgotIEZpbmQgYWxsIGl0ZW1zZXRzIHRoYXQgYXBwZWFyIGluIGF0IGxlYXN0IGEgZ2l2ZW4gZnJhY3Rpb24gKHRoZSAqbWluaW11bSBzdXBwb3J0Kikgb2YgdHJhbnNhY3Rpb25zLgoKKipBbGdvcml0aG0gc3RlcHM6KioKMS4gU3RhcnQgYnkgY291bnRpbmcgc3VwcG9ydCBmb3IgYWxsIHNpbmdsZSBpdGVtcyAoMS1pdGVtc2V0cykuCjIuIFJlbW92ZSBhbnkgdGhhdCBkbyBub3QgbWVldCB0aGUgbWluX3N1cHBvcnQgdGhyZXNob2xkLgozLiBJdGVyYXRpdmVseToKICAgLSBVc2UgZnJlcXVlbnQgay1pdGVtc2V0cyB0byBnZW5lcmF0ZSBjYW5kaWRhdGUgKGsrMSktaXRlbXNldHMuCiAgIC0gQ291bnQgc3VwcG9ydHMgZm9yIHRoZXNlIGNhbmRpZGF0ZXMuCiAgIC0gUmVtb3ZlIHRob3NlIG5vdCBtZWV0aW5nIG1pbl9zdXBwb3J0LgogICAtIFJlcGVhdCB1bnRpbCBubyBuZXcgZnJlcXVlbnQgaXRlbXNldHMgYXJlIGZvdW5kLCBvciB5b3UgcmVhY2ggbWF4X2xlbmd0aCAoaWYgZ2l2ZW4pLgoKKipLZXkgUHJpbmNpcGxlOioqCi0gKipBbnRpLW1vbm90b25pY2l0eToqKiBJZiBhbiBpdGVtc2V0IGlzIGluZnJlcXVlbnQsIGFsbCBzdXBlcnNldHMgYXJlIGFsc28gaW5mcmVxdWVudC4gVGhpcyBhbGxvd3MgQXByaW9yaSB0byBwcnVuZSBjYW5kaWRhdGVzIGVmZmljaWVudGx5LgoKKipFeGFtcGxlOioqCi0gRm9yIHRoZSBkYXRhc2V0OgogIC0gVDE6IHtCcmVhZCwgTWlsa30KICAtIFQyOiB7QnJlYWQsIERpYXBlciwgQmVlcn0KICAtIFQzOiB7TWlsaywgRGlhcGVyLCBCZWVyfQogIC0gVDQ6IHtCcmVhZCwgTWlsaywgRGlhcGVyfQogIC0gVDU6IHtCcmVhZCwgTWlsaywgQ29sYX0KLSBXaXRoIG1pbl9zdXBwb3J0ID0gMC42IChtdXN0IGFwcGVhciBpbiDiiaUzIG9mIDUgdHJhbnNhY3Rpb25zKToKICAtIDEtaXRlbXNldHM6IEJyZWFkICg0LzUpLCBNaWxrICg0LzUpLCBEaWFwZXIgKDMvNSkgYXJlIGZyZXF1ZW50CiAgLSAyLWl0ZW1zZXRzOiB7QnJlYWQsIE1pbGt9ICgzLzUpIGlzIGZyZXF1ZW50CgoqKk5vIGNoYXJ0cyByZXF1aXJlZDogdGhpcyB0YXNrIGlzIGFib3V0IGltcGxlbWVudGF0aW9uIGFuZCBzZXQgbG9naWMuKioKCi0tLQoKKipBcHBsaWNhdGlvbnM6KioKLSBNYXJrZXQgYmFza2V0IGFuYWx5c2lzCi0gRGlzY292ZXJpbmcgYXNzb2NpYXRpb25zIGluIG1lZGljYWwsIHdlYiwgb3IgdHJhbnNhY3Rpb25hbCBkYXRh",
    "solution_code": "import itertools\nfrom collections import defaultdict\n\nclass Solution:\n    def apriori(self, transactions, min_support=0.5, max_length=None):\n        if not transactions:\n            raise ValueError('Transaction list cannot be empty')\n        if not 0 < min_support <= 1:\n            raise ValueError('Minimum support must be between 0 and 1')\n\n        num_transactions = len(transactions)\n        min_support_count = min_support * num_transactions\n        item_counts = defaultdict(int)\n        for transaction in transactions:\n            for item in transaction:\n                item_counts[frozenset([item])] += 1\n        frequent_itemsets = {itemset: count for itemset, count in item_counts.items() if count >= min_support_count}\n        k = 1\n        all_frequent_itemsets = dict(frequent_itemsets)\n        while frequent_itemsets and (max_length is None or k < max_length):\n            k += 1\n            candidates = generate_candidates(frequent_itemsets.keys(), k)\n            candidate_counts = defaultdict(int)\n            for transaction in transactions:\n                transaction_set = frozenset(transaction)\n                for candidate in candidates:\n                    if candidate.issubset(transaction_set):\n                        candidate_counts[candidate] += 1\n            frequent_itemsets = {itemset: count for itemset, count in candidate_counts.items() if count >= min_support_count}\n            all_frequent_itemsets.update(frequent_itemsets)\n        return {itemset: count / num_transactions for itemset, count in all_frequent_itemsets.items()}\n\n    def generate_candidates(self, prev_frequent_itemsets, k):\n        candidates = set()\n        prev_frequent_list = sorted(list(prev_frequent_itemsets), key=lambda x: sorted(x))\n        for i in range(len(prev_frequent_list)):\n            for j in range(i + 1, len(prev_frequent_list)):\n                itemset1 = prev_frequent_list[i]\n                itemset2 = prev_frequent_list[j]\n                if k > 2:\n                    if sorted(itemset1)[:-1] != sorted(itemset2)[:-1]:\n                        continue\n                new_candidate = itemset1 | itemset2\n                if len(new_candidate) == k:\n                    candidates.add(new_candidate)\n        return candidates",
    "test_cases": [
      {
        "test": [
          "transactions = [\n    {'bread', 'milk'},\n    {'bread', 'diaper', 'beer', 'eggs'},\n    {'milk', 'diaper', 'beer', 'cola'},\n    {'bread', 'milk', 'diaper', 'beer'},\n    {'bread', 'milk', 'diaper', 'cola'}\n]\nresult = apriori(transactions, min_support=0.6)\nl=[]\nfor k in sorted(result, key=lambda x: (len(x), sorted(x))):\n    l.append(result[k])\nprint(l)"
        ],
        "expected_output": "0.6 0.8 0.8 0.8 0.6 0.6 0.6 0.6"
      },
      {
        "test": [
          "transactions = [\n    {'a', 'b'}, {'c', 'd'}, {'e', 'f'}, {'g', 'h'}\n]\nresult = apriori(transactions, min_support=0.5)\nprint(result)"
        ],
        "expected_output": {}
      }
    ],
    "function_name": "apriori"
  },
  {
    "id": 145,
    "title": "Adagrad Optimizer",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBBZGFncmFkIG9wdGltaXplciB1cGRhdGUgc3RlcCBmdW5jdGlvbi4gWW91ciBmdW5jdGlvbiBzaG91bGQgdGFrZSB0aGUgY3VycmVudCBwYXJhbWV0ZXIgdmFsdWUsIGdyYWRpZW50LCBhbmQgYWNjdW11bGF0ZWQgc3F1YXJlZCBncmFkaWVudHMgYXMgaW5wdXRzLCBhbmQgcmV0dXJuIHRoZSB1cGRhdGVkIHBhcmFtZXRlciB2YWx1ZSBhbmQgbmV3IGFjY3VtdWxhdGVkIHNxdWFyZWQgZ3JhZGllbnRzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIGFsc28gaGFuZGxlIHNjYWxhciBhbmQgYXJyYXkgaW5wdXRzLCBhbmQgaW5jbHVkZSBwcm9wZXIgaW5wdXQgdmFsaWRhdGlvbi4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def adagrad_optimizer(self, parameter, grad, G, learning_rate=0.01, epsilon=1e-8):\n        \"\"\"\n        Update parameters using the Adagrad optimizer.\n        Adapts the learning rate for each parameter based on the historical gradients.\n        Args:\n            parameter: Current parameter value\n            grad: Current gradient\n            G: Accumulated squared gradients\n            learning_rate: Learning rate (default=0.01)\n            epsilon: Small constant for numerical stability (default=1e-8)\n        Returns:\n            tuple: (updated_parameter, updated_G)\n        \"\"\"\n        # Your code here\n        return np.round(parameter, 5), np.round(G, 5)",
    "example_input": "parameter = 1.0, grad = 0.1, G = 1.0",
    "example_output": "(0.999, 1.01)",
    "example_reasoning": "The Adagrad optimizer computes updated values for the parameter and the accumulated squared gradients. With input values parameter=1.0, grad=0.1, and G=1.0, the updated parameter becomes 0.999 and the updated G becomes 1.01.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def adagrad_optimizer(self, parameter, grad, G, learning_rate=0.01, epsilon=1e-8):\n        \"\"\"\n        Update parameters using the Adagrad optimizer.\n        Adapts the learning rate for each parameter based on the historical gradients.\n        Args:\n            parameter: Current parameter value\n            grad: Current gradient\n            G: Accumulated squared gradients\n            learning_rate: Learning rate (default=0.01)\n            epsilon: Small constant for numerical stability (default=1e-8)\n        Returns:\n            tuple: (updated_parameter, updated_G)\n        \"\"\"\n        assert learning_rate > 0, \"Learning rate must be positive\"\n        assert epsilon > 0, \"Epsilon must be positive\"\n        assert all(G >= 0) if isinstance(G, np.ndarray) else G >= 0, \"G must be non-negative\"\n\n        # Update accumulated squared gradients\n        G = G + grad**2\n\n        # Update parameters using adaptive learning rate\n        update = learning_rate * grad / (np.sqrt(G) + epsilon)\n        parameter = parameter - update\n\n        return np.round(parameter, 5), np.round(G, 5)",
    "test_cases": [
      {
        "test": [
          1.0,
          0.5,
          1.0,
          0.01,
          1e-08
        ],
        "expected_output": "(0.99553, 1.25)"
      },
      {
        "test": [
          [
            1.0,
            2.0
          ],
          [
            0.1,
            0.2
          ],
          [
            1.0,
            1.0
          ],
          0.01,
          1e-08
        ],
        "expected_output": "array([0.999, 1.99804]), array([1.01, 1.04])"
      }
    ],
    "function_name": "adagrad_optimizer"
  },
  {
    "id": 146,
    "title": "Momentum Optimizer",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBtb21lbnR1bSBvcHRpbWl6ZXIgdXBkYXRlIHN0ZXAgZnVuY3Rpb24uIFlvdXIgZnVuY3Rpb24gc2hvdWxkIHRha2UgdGhlIGN1cnJlbnQgcGFyYW1ldGVyIHZhbHVlLCBncmFkaWVudCwgYW5kIHZlbG9jaXR5IGFzIGlucHV0cywgYW5kIHJldHVybiB0aGUgdXBkYXRlZCBwYXJhbWV0ZXIgdmFsdWUgYW5kIG5ldyB2ZWxvY2l0eS4gVGhlIGZ1bmN0aW9uIHNob3VsZCBhbHNvIGhhbmRsZSBzY2FsYXIgYW5kIGFycmF5IGlucHV0cy4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def momentum_optimizer(self, parameter, grad, velocity, learning_rate=0.01, momentum=0.9):\n        \"\"\"\n        Update parameters using the momentum optimizer.\n        Uses momentum to accelerate learning in relevant directions and dampen oscillations.\n        Args:\n            parameter: Current parameter value\n            grad: Current gradient\n            velocity: Current velocity/momentum term\n            learning_rate: Learning rate (default=0.01)\n            momentum: Momentum coefficient (default=0.9)\n        Returns:\n            tuple: (updated_parameter, updated_velocity)\n        \"\"\"\n        # Your code here\n        return np.round(parameter, 5), np.round(velocity, 5)",
    "example_input": "parameter = 1.0, grad = 0.1, velocity = 0.1",
    "example_output": "(0.909, 0.091)",
    "example_reasoning": "The momentum optimizer computes updated values for the parameter and the velocity. With input values parameter=1.0, grad=0.1, and velocity=0.1, the updated parameter becomes 0.909 and the updated velocity becomes 0.091.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def momentum_optimizer(self, parameter, grad, velocity, learning_rate=0.01, momentum=0.9):\n        \"\"\"\n        Update parameters using the momentum optimizer.\n        Uses momentum to accelerate learning in relevant directions and dampen oscillations.\n        Args:\n            parameter: Current parameter value\n            grad: Current gradient\n            velocity: Current velocity/momentum term\n            learning_rate: Learning rate (default=0.01)\n            momentum: Momentum coefficient (default=0.9)\n        Returns:\n            tuple: (updated_parameter, updated_velocity)\n        \"\"\"\n        assert learning_rate > 0, \"Learning rate must be positive\"\n        assert 0 <= momentum < 1, \"Momentum must be between 0 and 1\"\n\n        # Update velocity\n        velocity = momentum * velocity + learning_rate * grad\n\n        # Update parameters\n        parameter = parameter - velocity\n\n        return np.round(parameter, 5), np.round(velocity, 5)",
    "test_cases": [
      {
        "test": [
          1.0,
          0.1,
          0.5,
          0.01,
          0.9
        ],
        "expected_output": "(0.549, 0.451)"
      },
      {
        "test": [
          [
            1.0,
            2.0
          ],
          [
            0.1,
            0.2
          ],
          [
            0.5,
            1.0
          ],
          0.01,
          0.9
        ],
        "expected_output": "(array([0.549, 1.098]), array([0.451, 0.902]))"
      }
    ],
    "function_name": "momentum_optimizer"
  },
  {
    "id": 147,
    "title": "GeLU Activation Function ",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgUHl0aG9uIGZ1bmN0aW9uIHRoYXQgYXBwbGllcyB0aGUgR0VMVSAoR2F1c3NpYW4gRXJyb3IgTGluZWFyIFVuaXQpIGFjdGl2YXRpb24gZnVuY3Rpb24gdG8gYSBOdW1QeSBhcnJheSBvZiBsb2dpdHMuIFJvdW5kIGVhY2ggb3V0cHV0IHRvIGZvdXIgZGVjaW1hbCBwbGFjZXMgYW5kIHJldHVybiB0aGUgcmVzdWx0IGFzIGEgTnVtUHkgYXJyYXkgb2YgdGhlIHNhbWUgc2hhcGUu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def GeLU(self, x: np.ndarray) -> np.ndarray:\n    \t# Your code here\n    \treturn scores",
    "example_input": "np.array([-2.0, -1.0, 0.0, 1.0, 2.0])",
    "example_output": "[-0.0454 -0.1588 0. 0.8412 1.9546]",
    "example_reasoning": "Each value in the input array is passed through the GELU activation function using the approximation formula. For example, GELU(1.0) \u2248 0.841192, which rounds to 0.8412. Similarly, GELU(-2.0) \u2248 -0.045402, which rounds to -0.0454. Since we're using numpy, it uses vectorized operations and applies the formula to each element in the array. Then, we rounded it to four decimal places to produce the output array.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def GeLU(self, x: np.ndarray) -> np.ndarray:\n        return np.round(0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3)))),4)",
    "test_cases": [
      {
        "test": [
          "print(np.round(GeLU(np.array([0.1, 0.0, -0.1])),4))"
        ],
        "expected_output": [
          0.054,
          0.0,
          -0.046
        ]
      },
      {
        "test": [
          "print(np.round(GeLU(np.array([5.5, -4.2, 0.75])),4))"
        ],
        "expected_output": [
          5.5,
          -0.0,
          0.58
        ]
      }
    ],
    "function_name": "GeLU"
  },
  {
    "id": 148,
    "title": "Adamax Optimizer",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBBZGFtYXggb3B0aW1pemVyIHVwZGF0ZSBzdGVwIGZ1bmN0aW9uLiBZb3VyIGZ1bmN0aW9uIHNob3VsZCB0YWtlIHRoZSBjdXJyZW50IHBhcmFtZXRlciB2YWx1ZSwgZ3JhZGllbnQsIGFuZCBtb3ZpbmcgYXZlcmFnZXMgYXMgaW5wdXRzLCBhbmQgcmV0dXJuIHRoZSB1cGRhdGVkIHBhcmFtZXRlciB2YWx1ZSBhbmQgbmV3IG1vdmluZyBhdmVyYWdlcy4gVGhlIGZ1bmN0aW9uIHNob3VsZCBhbHNvIGhhbmRsZSBzY2FsYXIgYW5kIGFycmF5IGlucHV0cyBhbmQgaW5jbHVkZSBiaWFzIGNvcnJlY3Rpb24gZm9yIHRoZSBtb3ZpbmcgYXZlcmFnZXMu",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def adamax_optimizer(self, parameter, grad, m, u, t, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        \"\"\"\n        Update parameters using the Adamax optimizer.\n        Adamax is a variant of Adam based on the infinity norm.\n        It uses the maximum of past squared gradients instead of the exponential moving average.\n        Args:\n            parameter: Current parameter value\n            grad: Current gradient\n            m: First moment estimate\n            u: Infinity norm estimate\n            t: Current timestep\n            learning_rate: Learning rate (default=0.002)\n            beta1: First moment decay rate (default=0.9)\n            beta2: Infinity norm decay rate (default=0.999)\n            epsilon: Small constant for numerical stability (default=1e-8)\n        Returns:\n            tuple: (updated_parameter, updated_m, updated_u)\n        \"\"\"\n    \t# Your code here\n        return np.round(parameter, 5), np.round(m, 5), np.round(u, 5)",
    "example_input": "parameter = 1.0, grad = 0.1, m = 0.0, u = 0.0, t = 1",
    "example_output": "(0.998, 0.01, 0.1)",
    "example_reasoning": "The Adamax optimizer computes updated values for the parameter, first moment (m), and infinity norm (u) using bias-corrected estimates of gradients. With input values parameter=1.0, grad=0.1, m=0.0, u=0.0, and t=1, the updated parameter becomes 0.998, the updated m becomes 0.01, and the updated u becomes 0.1.",
    "learn_content": "IyBJbXBsZW1lbnRpbmcgQWRhbWF4IE9wdGltaXplcgoKIyMgSW50cm9kdWN0aW9uCkFkYW1heCBpcyBhIHZhcmlhbnQgb2YgQWRhbSBvcHRpbWl6ZXIgdGhhdCB1c2VzIHRoZSBpbmZpbml0eSBub3JtIChtYXgpIGluc3RlYWQgb2YgdGhlIEwyIG5vcm0gZm9yIHRoZSBzZWNvbmQgbW9tZW50IGVzdGltYXRlLiBUaGlzIG1ha2VzIGl0IG1vcmUgcm9idXN0IGluIHNvbWUgY2FzZXMgYW5kIGNhbiBsZWFkIHRvIGJldHRlciBjb252ZXJnZW5jZSBpbiBjZXJ0YWluIHNjZW5hcmlvcywgcGFydGljdWxhcmx5IHdoZW4gZGVhbGluZyB3aXRoIHNwYXJzZSBncmFkaWVudHMuCgojIyBMZWFybmluZyBPYmplY3RpdmVzCi0gVW5kZXJzdGFuZCBob3cgQWRhbWF4IG9wdGltaXphdGlvbiB3b3JrcwotIExlYXJuIHRvIGltcGxlbWVudCBBZGFtYXgtYmFzZWQgZ3JhZGllbnQgdXBkYXRlcwotIFVuZGVyc3RhbmQgdGhlIGVmZmVjdCBvZiBpbmZpbml0eSBub3JtIG9uIG9wdGltaXphdGlvbgoKIyMgVGhlb3J5CkFkYW1heCBtYWludGFpbnMgYSBtb3ZpbmcgYXZlcmFnZSBvZiBncmFkaWVudHMgKGZpcnN0IG1vbWVudCkgYW5kIHVzZXMgdGhlIGluZmluaXR5IG5vcm0gZm9yIHRoZSBzZWNvbmQgbW9tZW50IGVzdGltYXRlLiBUaGUga2V5IGVxdWF0aW9ucyBhcmU6CgpGaXJzdCBtb21lbnQgZXN0aW1hdGUgKHNhbWUgYXMgQWRhbSk6CgokbV90ID0gXGJldGFfMSBtX3t0LTF9ICsgKDEtXGJldGFfMSlnX3QkCgpUaGUgc2Vjb25kIG1vbWVudCBlc3RpbWF0ZSBpbiBBZGFtIHVzZXMgdGhlICRsXzIkIG5vcm06Cgokdl90ID0gXGJldGFfMiB2X3t0LTF9ICsgKDEtXGJldGFfMil8Z190fF4yJAoKVGhpcyBjYW4gYmUgZ2VuZXJhbGl6ZWQgdG8gdGhlICRsX3AkIG5vcm0sIGJ1dCBub3JtcyBmb3IgbGFyZ2UgcCB2YWx1ZXMgYXJlIG51bWVyaWNhbGx5IHVuc3RhYmxlLiBIb3dldmVyLCBBZGFtYXggdXNlcyB0aGUgJGxfXGluZmluJCBub3JtIChpbmZpbml0eSBub3JtKSwgd2hpY2ggY29udmVyZ2VzIHRvOgoKJHVfdCA9IFxtYXgoXGJldGFfMiBcY2RvdCB1X3t0LTF9LCB8Z190fCkkCgpVbmxpa2UgQWRhbSwgQWRhbWF4IGRvZXNuJ3QgcmVxdWlyZSBiaWFzIGNvcnJlY3Rpb24gZm9yICR1X3QkIGJlY2F1c2UgdGhlIG1heCBvcGVyYXRpb24gbWFrZXMgaXQgbGVzcyBzdXNjZXB0aWJsZSB0byBiaWFzIHRvd2FyZHMgemVyby4KCkJpYXMgY29ycmVjdGlvbjoKJFxoYXR7bX1fdCA9IFxkZnJhY3ttX3R9ezEtXGJldGFfMV50fSQKClBhcmFtZXRlciB1cGRhdGU6CiRcdGhldGFfdCA9IFx0aGV0YV97dC0xfSAtIFxkZnJhY3tcZXRhfXt1X3R9IFxoYXR7bX1fdCQKCldoZXJlOgotICRtX3QkIGlzIHRoZSBmaXJzdCBtb21lbnQgZXN0aW1hdGUgYXQgdGltZSB0Ci0gJHVfdCQgaXMgdGhlIGluZmluaXR5IG5vcm0gZXN0aW1hdGUgYXQgdGltZSB0Ci0gJFxiZXRhXzEkIGlzIHRoZSBmaXJzdCBtb21lbnQgY29lZmZpY2llbnQgKHR5cGljYWxseSAwLjkpCi0gJFxiZXRhXzIkIGlzIHRoZSBzZWNvbmQgbW9tZW50IGNvZWZmaWNpZW50ICh0eXBpY2FsbHkgMC45OTkpCi0gJFxldGEkIGlzIHRoZSBsZWFybmluZyByYXRlCi0gJGdfdCQgaXMgdGhlIGdyYWRpZW50IGF0IHRpbWUgdAoKTm90ZTogVW5saWtlIEFkYW0sIEFkYW1heCBkb2Vzbid0IHJlcXVpcmUgYmlhcyBjb3JyZWN0aW9uIGZvciAkdV90JCBiZWNhdXNlIHRoZSBtYXggb3BlcmF0aW9uIG1ha2VzIGl0IGxlc3Mgc3VzY2VwdGlibGUgdG8gYmlhcyB0b3dhcmRzIHplcm8uCgpSZWFkIG1vcmUgYXQ6CgoxLiBLaW5nbWEsIEQuIGFuZCBCYSwgSi4gKDIwMTUpLiBBZGFtOiBBIE1ldGhvZCBmb3IgU3RvY2hhc3RpYyBPcHRpbWl6YXRpb24uIFthclhpdjoxNDEyLjY5ODBdKGh0dHBzOi8vYXJ4aXYub3JnL2Ficy8xNDEyLjY5ODApCjIuIFJ1ZGVyLCBTLiAoMjAxNykuIEFuIG92ZXJ2aWV3IG9mIGdyYWRpZW50IGRlc2NlbnQgb3B0aW1pemF0aW9uIGFsZ29yaXRobXMuIFthclhpdjoxNjA5LjA0NzQ3XShodHRwczovL2FyeGl2Lm9yZy9wZGYvMTYwOS4wNDc0NykKCgojIyBQcm9ibGVtIFN0YXRlbWVudApJbXBsZW1lbnQgdGhlIEFkYW1heCBvcHRpbWl6ZXIgdXBkYXRlIHN0ZXAgZnVuY3Rpb24uIFlvdXIgZnVuY3Rpb24gc2hvdWxkIHRha2UgdGhlIGN1cnJlbnQgcGFyYW1ldGVyIHZhbHVlLCBncmFkaWVudCwgYW5kIG1vbWVudCBlc3RpbWF0ZXMgYXMgaW5wdXRzLCBhbmQgcmV0dXJuIHRoZSB1cGRhdGVkIHBhcmFtZXRlciB2YWx1ZSBhbmQgbmV3IG1vbWVudCBlc3RpbWF0ZXMuCgojIyMgSW5wdXQgRm9ybWF0ClRoZSBmdW5jdGlvbiBzaG91bGQgYWNjZXB0OgotIHBhcmFtZXRlcjogQ3VycmVudCBwYXJhbWV0ZXIgdmFsdWUKLSBncmFkOiBDdXJyZW50IGdyYWRpZW50Ci0gbTogRmlyc3QgbW9tZW50IGVzdGltYXRlCi0gdTogSW5maW5pdHkgbm9ybSBlc3RpbWF0ZQotIHQ6IEN1cnJlbnQgdGltZXN0ZXAKLSBsZWFybmluZ19yYXRlOiBMZWFybmluZyByYXRlIChkZWZhdWx0PTAuMDAyKQotIGJldGExOiBGaXJzdCBtb21lbnQgZGVjYXkgcmF0ZSAoZGVmYXVsdD0wLjkpCi0gYmV0YTI6IFNlY29uZCBtb21lbnQgZGVjYXkgcmF0ZSAoZGVmYXVsdD0wLjk5OSkKLSBlcHNpbG9uOiBTbWFsbCBjb25zdGFudCBmb3IgbnVtZXJpY2FsIHN0YWJpbGl0eSAoZGVmYXVsdD0xZS04KQoKIyMjIE91dHB1dCBGb3JtYXQKUmV0dXJuIHR1cGxlOiAodXBkYXRlZF9wYXJhbWV0ZXIsIHVwZGF0ZWRfbSwgdXBkYXRlZF91KQoKIyMgRXhhbXBsZQpgYGBweXRob24KIyBFeGFtcGxlIHVzYWdlOgpwYXJhbWV0ZXIgPSAxLjAKZ3JhZCA9IDAuMQptID0gMC4wCnUgPSAwLjAKdCA9IDEKCm5ld19wYXJhbSwgbmV3X20sIG5ld191ID0gYWRhbWF4X29wdGltaXplcihwYXJhbWV0ZXIsIGdyYWQsIG0sIHUsIHQpCmBgYAoKIyMgVGlwcwotIEluaXRpYWxpemUgbSBhbmQgdSBhcyB6ZXJvcwotIEtlZXAgdHJhY2sgb2YgdGltZXN0ZXAgdCBmb3IgYmlhcyBjb3JyZWN0aW9uCi0gVXNlIG51bXB5IGZvciBudW1lcmljYWwgb3BlcmF0aW9ucwotIFRlc3Qgd2l0aCBib3RoIHNjYWxhciBhbmQgYXJyYXkgaW5wdXRzCi0gUmVtZW1iZXIgdG8gYXBwbHkgYmlhcyBjb3JyZWN0aW9uIHRvIHRoZSBmaXJzdCBtb21lbnQgZXN0aW1hdGUKCi0tLQ==",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def adamax_optimizer(self, parameter, grad, m, u, t, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        \"\"\"\n        Update parameters using the Adamax optimizer.\n        Adamax is a variant of Adam based on the infinity norm.\n        It uses the maximum of past squared gradients instead of the exponential moving average.\n        Args:\n            parameter: Current parameter value\n            grad: Current gradient\n            m: First moment estimate\n            u: Infinity norm estimate\n            t: Current timestep\n            learning_rate: Learning rate (default=0.002)\n            beta1: First moment decay rate (default=0.9)\n            beta2: Infinity norm decay rate (default=0.999)\n            epsilon: Small constant for numerical stability (default=1e-8)\n        Returns:\n            tuple: (updated_parameter, updated_m, updated_u)\n        \"\"\"\n        assert learning_rate > 0, \"Learning rate must be positive\"\n        assert 0 <= beta1 < 1, \"Beta1 must be between 0 and 1\"\n        assert 0 <= beta2 < 1, \"Beta2 must be between 0 and 1\"\n        assert epsilon > 0, \"Epsilon must be positive\"\n        assert all(u >= 0) if isinstance(u, np.ndarray) else u >= 0, \"u must be non-negative\"\n\n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * grad\n\n        # Update infinity norm estimate\n        u = np.maximum(beta2 * u, np.abs(grad))\n\n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1**t)\n\n        # Update parameters\n        update = learning_rate * m_hat / (u + epsilon)\n        parameter = parameter - update\n\n        return np.round(parameter, 5), np.round(m, 5), np.round(u, 5)",
    "test_cases": [
      {
        "test": [
          1.0,
          0.1,
          1.0,
          1.0,
          1,
          0.002,
          0.9,
          0.999,
          1e-08
        ],
        "expected_output": "(0.98178, 0.91, 0.999)"
      },
      {
        "test": [
          [
            1.0,
            2.0
          ],
          [
            0.1,
            0.2
          ],
          [
            1.0,
            1.0
          ],
          [
            1.0,
            1.0
          ],
          1,
          0.002,
          0.9,
          0.999,
          1e-08
        ],
        "expected_output": "(array([0.98178, 1.98158]), array([0.91, 0.92]), array([0.999, 0.999]))"
      }
    ],
    "function_name": "adamax_optimizer"
  },
  {
    "id": 149,
    "title": "Adadelta Optimizer",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBBZGFkZWx0YSBvcHRpbWl6ZXIgdXBkYXRlIHN0ZXAgZnVuY3Rpb24uIFlvdXIgZnVuY3Rpb24gc2hvdWxkIHRha2UgdGhlIGN1cnJlbnQgcGFyYW1ldGVyIHZhbHVlLCBncmFkaWVudCwgYW5kIG1vdmluZyBhdmVyYWdlcyBhcyBpbnB1dHMsIGFuZCByZXR1cm4gdGhlIHVwZGF0ZWQgcGFyYW1ldGVyIHZhbHVlIGFuZCBuZXcgbW92aW5nIGF2ZXJhZ2VzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIGhhbmRsZSBib3RoIHNjYWxhciBhbmQgYXJyYXkgaW5wdXRzLCBhbmQgaW5jbHVkZSBwcm9wZXIgaW5wdXQgdmFsaWRhdGlvbi4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def adadelta_optimizer(self, parameter, grad, u, v, rho=0.95, epsilon=1e-6):\n        \"\"\"\n        Update parameters using the AdaDelta optimizer.\n        AdaDelta is an extension of AdaGrad that seeks to reduce its aggressive,\n        monotonically decreasing learning rate.\n        Args:\n            parameter: Current parameter value\n            grad: Current gradient\n            u: Running average of squared gradients\n            v: Running average of squared parameter updates\n            rho: Decay rate for the moving average (default=0.95)\n            epsilon: Small constant for numerical stability (default=1e-6)\n        Returns:\n            tuple: (updated_parameter, updated_u, updated_v)\n        \"\"\"\n        # Your code here\n        return np.round(parameter, 5), np.round(u, 5), np.round(v, 5)",
    "example_input": "parameter = 1.0, grad = 0.1, u = 1.0, v = 1.0, rho = 0.95, epsilon = 1e-6",
    "example_output": "(0.89743, 0.9505, 0.95053)",
    "example_reasoning": null,
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def adadelta_optimizer(self, parameter, grad, u, v, rho=0.95, epsilon=1e-6):\n        \"\"\"\n        Update parameters using the AdaDelta optimizer.\n        AdaDelta is an extension of AdaGrad that seeks to reduce its aggressive,\n        monotonically decreasing learning rate.\n        Args:\n            parameter: Current parameter value\n            grad: Current gradient\n            u: Running average of squared gradients\n            v: Running average of squared parameter updates\n            rho: Decay rate for the moving average (default=0.95)\n            epsilon: Small constant for numerical stability (default=1e-6)\n        Returns:\n            tuple: (updated_parameter, updated_u, updated_v)\n        \"\"\"\n        assert 0 <= rho < 1, \"Rho must be between 0 and 1\"\n        assert epsilon > 0, \"Epsilon must be positive\"\n        assert all(u >= 0) if isinstance(u, np.ndarray) else u >= 0, \"u must be non-negative\"\n        assert all(v >= 0) if isinstance(v, np.ndarray) else v >= 0, \"v must be non-negative\"\n\n        # Update running average of squared gradients\n        u = rho * u + (1 - rho) * grad**2\n\n        # Compute RMS of gradient\n        RMS_g = np.sqrt(u + epsilon)\n\n        # Compute RMS of parameter updates\n        RMS_dx = np.sqrt(v + epsilon)\n\n        # Compute parameter update\n        dx = -RMS_dx / RMS_g * grad\n\n        # Update running average of squared parameter updates\n        v = rho * v + (1 - rho) * dx**2\n\n        # Update parameters\n        parameter = parameter + dx\n\n        return np.round(parameter, 5), np.round(u, 5), np.round(v, 5)",
    "test_cases": [
      {
        "test": [
          1.0,
          0.5,
          1.0,
          1.0,
          0.95,
          1e-06
        ],
        "expected_output": "(0.49035, 0.9625, 0.96299)"
      },
      {
        "test": [
          [
            1.0,
            2.0
          ],
          [
            0.1,
            0.2
          ],
          [
            1.0,
            1.0
          ],
          [
            1.0,
            1.0
          ],
          0.95,
          1e-06
        ],
        "expected_output": "(array([0.89743, 1.79502]), array([0.9505, 0.952]), array([0.95053, 0.9521]))"
      }
    ],
    "function_name": "adadelta_optimizer"
  },
  {
    "id": 150,
    "title": "Nesterov Accelerated Gradient Optimizer",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBOZXN0ZXJvdiBBY2NlbGVyYXRlZCBHcmFkaWVudCAoTkFHKSBvcHRpbWl6ZXIgdXBkYXRlIHN0ZXAgZnVuY3Rpb24uIFlvdXIgZnVuY3Rpb24gc2hvdWxkIHRha2UgdGhlIGN1cnJlbnQgcGFyYW1ldGVyIHZhbHVlLCBncmFkaWVudCBmdW5jdGlvbiwgYW5kIHZlbG9jaXR5IGFzIGlucHV0cywgYW5kIHJldHVybiB0aGUgdXBkYXRlZCBwYXJhbWV0ZXIgdmFsdWUgYW5kIG5ldyB2ZWxvY2l0eS4gVGhlIGZ1bmN0aW9uIHNob3VsZCB1c2UgdGhlICJsb29rLWFoZWFkIiBhcHByb2FjaCB3aGVyZSBtb21lbnR1bSBpcyBhcHBsaWVkIGJlZm9yZSBjb21wdXRpbmcgdGhlIGdyYWRpZW50LCBhbmQgc2hvdWxkIGhhbmRsZSBib3RoIHNjYWxhciBhbmQgYXJyYXkgaW5wdXRzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def nag_optimizer(self, parameter, grad_fn, velocity, learning_rate=0.01, momentum=0.9):\n        \"\"\"\n        Update parameters using the Nesterov Accelerated Gradient optimizer.\n        Uses a \"look-ahead\" approach to improve convergence by applying momentum before computing the gradient.\n        Args:\n            parameter: Current parameter value\n            grad_fn: Function that computes the gradient at a given position\n            velocity: Current velocity (momentum term)\n            learning_rate: Learning rate (default=0.01)\n            momentum: Momentum coefficient (default=0.9)\n        Returns:\n            tuple: (updated_parameter, updated_velocity)\n        \"\"\"\n        # Your code here\n        return np.round(parameter, 5), np.round(velocity, 5)",
    "example_input": "parameter = 1.0, grad_fn = lambda x: x, velocity = 0.1",
    "example_output": "(0.9009, 0.0991)",
    "example_reasoning": "The Nesterov Accelerated Gradient optimizer computes updated values for the parameter and velocity using a look-ahead approach. With input values parameter=1.0, grad_fn=lambda x: x, and velocity=0.1, the updated parameter becomes 0.9009 and the updated velocity becomes 0.0991.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def nag_optimizer(self, parameter, grad_fn, velocity, learning_rate=0.01, momentum=0.9):\n        \"\"\"\n        Update parameters using the Nesterov Accelerated Gradient optimizer.\n        Uses a \"look-ahead\" approach to improve convergence by applying momentum before computing the gradient.\n        Args:\n            parameter: Current parameter value\n            grad_fn: Function that computes the gradient at a given position\n            velocity: Current velocity (momentum term)\n            learning_rate: Learning rate (default=0.01)\n            momentum: Momentum coefficient (default=0.9)\n        Returns:\n            tuple: (updated_parameter, updated_velocity)\n        \"\"\"\n        assert 0 <= momentum < 1, \"Momentum must be between 0 and 1\"\n        assert learning_rate > 0, \"Learning rate must be positive\"\n\n        # Compute look-ahead position\n        look_ahead = parameter - momentum * velocity\n\n        # Compute gradient at look-ahead position\n        grad = grad_fn(look_ahead)\n\n        # Update velocity using momentum and gradient\n        velocity = momentum * velocity + learning_rate * grad\n\n        # Update parameters using the new velocity\n        parameter = parameter - velocity\n\n        return np.round(parameter, 5), np.round(velocity, 5)",
    "test_cases": [
      {
        "test": [
          "import numpy as np\ndef gradient_function(x):\n    if isinstance(x, np.ndarray):\n        n = len(x)\n        return x - np.arange(n)\n    else:\n        return x - 0\nprint(nag_optimizer(1., gradient_function, 0.5, 0.01, 0.9))"
        ],
        "expected_output": "(0.5445, 0.4555)"
      },
      {
        "test": [
          "import numpy as np\ndef gradient_function(x):\n    if isinstance(x, np.ndarray):\n        n = len(x)\n        return x - np.arange(n)\n    else:\n        return x - 0\nprint(nag_optimizer(np.array([1.0, 2.0]), gradient_function, np.array([0.5, 1.0]), 0.01, 0.9))"
        ],
        "expected_output": "(array([0.5445, 1.099]), array([0.4555, 0.901]))"
      }
    ],
    "function_name": "nag_optimizer"
  },
  {
    "id": 151,
    "title": "Dropout Layer",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgZHJvcG91dCBsYXllciB0aGF0IGFwcGxpZXMgcmFuZG9tIG5ldXJvbiBkZWFjdGl2YXRpb24gZHVyaW5nIHRyYWluaW5nIHRvIHByZXZlbnQgb3ZlcmZpdHRpbmcgaW4gbmV1cmFsIG5ldHdvcmtzLiBUaGUgbGF5ZXIgc2hvdWxkIHJhbmRvbWx5IHplcm8gb3V0IGEgcHJvcG9ydGlvbiBvZiBpbnB1dCBlbGVtZW50cyBiYXNlZCBvbiBhIGRyb3BvdXQgcmF0ZSBwLCBzY2FsZSB0aGUgcmVtYWluaW5nIHZhbHVlcyBieSAxLygxLXApIHRvIG1haW50YWluIGV4cGVjdGVkIHZhbHVlcywgYW5kIHBhc3MgaW5wdXRzIHVuY2hhbmdlZCBkdXJpbmcgaW5mZXJlbmNlLiBEdXJpbmcgYmFja3Byb3BhZ2F0aW9uLCBncmFkaWVudHMgbXVzdCBiZSBtYXNrZWQgd2l0aCB0aGUgc2FtZSBkcm9wb3V0IHBhdHRlcm4gYW5kIHNjYWxlZCBieSB0aGUgc2FtZSBmYWN0b3IgdG8gZW5zdXJlIHByb3BlciBncmFkaWVudCBmbG93Lg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    class DropoutLayer:\n    def __init__(self, self, p: float):\n        \"\"\"Initialize the dropout layer.\"\"\"\n        # Your code here\n\n    def forward(self, self, x: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Forward pass of the dropout layer.\"\"\"\n        # Your code here\n\n    def backward(self, self, grad: np.ndarray) -> np.ndarray:\n        \"\"\"Backward pass of the dropout layer.\"\"\"\n        # Your code here",
    "example_input": "x = np.array([1.0, 2.0, 3.0, 4.0]), grad = np.array([0.1, 0.2, 0.3, 0.4]), p = 0.5",
    "example_output": "output = array([[2., 0. , 6. , 0. ]]), grad = array([[0.2, 0. , 0.6, 0. ]])",
    "example_reasoning": "The Dropout layer randomly zeroes out elements of the input tensor with probability p during training. To maintain the expected value of the activations, the remaining elements are scaled by a factor of 1 / (1 - p). During inference, Dropout is disabled and the input is passed through unchanged. During backpropagation, the same dropout mask and scaling are applied to the gradients, ensuring the expected gradient magnitude is preserved.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    class DropoutLayer:\n    def __init__(self, self, p: float):\n        \"\"\"Initialize the dropout layer.\"\"\"\n        if p < 0 or p >= 1:\n            raise ValueError(\"Dropout rate must be between 0 and 1 (1-exclusive)\")\n\n        self.p = p\n        self.mask = None\n\n    def forward(self, self, x: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Forward pass of the dropout layer.\"\"\"\n        if not training:\n            return x\n\n        self.mask = np.random.binomial(1, 1 - self.p, x.shape)\n\n        return x * self.mask / (1 - self.p)\n\n    def backward(self, self, grad: np.ndarray) -> np.ndarray:\n        \"\"\"Backward pass of the dropout layer.\"\"\"\n        if self.mask is None:\n            raise ValueError(\"Forward pass must be called before backward pass\")\n\n        return grad * self.mask / (1 - self.p)",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nx = np.array([[1.0, 2.0], [3.0, 4.0]])\ngrad = np.array([[0.5, 0.2], [1.0, 2.0]])\n\ndropout = DropoutLayer(0.2)\n\nprint(dropout.forward(x, training=True), dropout.forward(x, training=False), dropout.backward(grad))"
        ],
        "expected_output": "(array([[1.25, 0.], [3.75, 5.]]), array([[1.0, 2.0], [3.0, 4.0]]), array([[0.625, 0.], [1.25, 2.5]]))"
      },
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nx = np.ones((1000, 1000))\ndropout = DropoutLayer(0.2)\n\n_ = dropout.forward(x, training=True)\nmask1 = dropout.mask.copy()\n_ = dropout.forward(x, training=True)\nmask2 = dropout.mask.copy()\nprint(np.array_equal(mask1, mask2))"
        ],
        "expected_output": "False"
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 152,
    "title": "Implementing ROUGE Score",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBST1VHRS0xIChSZWNhbGwtT3JpZW50ZWQgVW5kZXJzdHVkeSBmb3IgR2lzdGluZyBFdmFsdWF0aW9uKSBzY29yZSB0byBldmFsdWF0ZSB0aGUgcXVhbGl0eSBvZiBhIGdlbmVyYXRlZCBzdW1tYXJ5IGJ5IGNvbXBhcmluZyBpdCB0byBhIHJlZmVyZW5jZSBzdW1tYXJ5LiBST1VHRS0xIGZvY3VzZXMgb24gdW5pZ3JhbSAoc2luZ2xlIHdvcmQpIG92ZXJsYXBzIGJldHdlZW4gdGhlIGNhbmRpZGF0ZSBhbmQgcmVmZXJlbmNlIHRleHRzLiBZb3VyIHRhc2sgaXMgdG8gd3JpdGUgYSBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIHRoZSBST1VHRS0xIHJlY2FsbCwgcHJlY2lzaW9uLCBhbmQgRjEgc2NvcmUgYmFzZWQgb24gdGhlIG51bWJlciBvZiBvdmVybGFwcGluZyB1bmlncmFtcy4=",
    "starter_code": "\n\nclass Solution:\n    # Implement your function below.\n\n    def rouge_1_score(self, reference: str, candidate: str) -> dict:\n        \"\"\"\n        Compute ROUGE-1 score between reference and candidate texts.\n\n        Returns a dictionary with precision, recall, and f1.\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "rouge_1_score('the cat sat on the mat', 'the cat is on the mat')",
    "example_output": "{'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1': 0.8333333333333334}",
    "example_reasoning": "The reference text 'the cat sat on the mat' has 6 tokens, and the candidate text 'the cat is on the mat' has 6 tokens. The overlapping words are: 'the' (appears 2 times in reference, 2 times in candidate, so min(2,2)=2 overlap), 'cat' (1,1 \u2192 1 overlap), 'on' (1,1 \u2192 1 overlap), and 'mat' (1,1 \u2192 1 overlap). Total overlap = 2+1+1+1 = 5. Precision = 5/6 \u2248 0.833 (5 overlapping words out of 6 candidate words). Recall = 5/6 \u2248 0.833 (5 overlapping words out of 6 reference words). F1 = 2\u00d7(0.833\u00d70.833)/(0.833+0.833) = 0.833 since precision equals recall.",
    "learn_content": "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",
    "solution_code": "from collections import Counter\n\nclass Solution:\n    def rouge_1_score(self, reference: str, candidate: str) -> dict:\n        \"\"\"\n        Compute ROUGE-1 score between reference and candidate texts.\n\n        Returns a dictionary with precision, recall, and f1.\n        \"\"\"\n        ref_tokens = reference.lower().split()\n        cand_tokens = candidate.lower().split()\n\n        ref_counter = Counter(ref_tokens)\n        cand_counter = Counter(cand_tokens)\n\n        # Count overlapping unigrams\n        overlap = sum(min(ref_counter[w], cand_counter[w]) for w in cand_counter)\n\n        precision = overlap / len(cand_tokens) if cand_tokens else 0.0\n        recall = overlap / len(ref_tokens) if ref_tokens else 0.0\n        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0.0\n\n        return {\"precision\": precision, \"recall\": recall, \"f1\": f1}",
    "test_cases": [
      {
        "test": [
          "the cat sat on the mat",
          "the cat is on the mat"
        ],
        "expected_output": {
          "precision": 0.8333333333333334,
          "recall": 0.8333333333333334,
          "f1": 0.8333333333333334
        }
      },
      {
        "test": [
          "hello there",
          "hello there"
        ],
        "expected_output": {
          "precision": 1.0,
          "recall": 1.0,
          "f1": 1.0
        }
      }
    ],
    "function_name": "rouge_1_score"
  },
  {
    "id": 153,
    "title": "StepLR Learning Rate Scheduler",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gY2xhc3MgU3RlcExSU2NoZWR1bGVyIHRvIGltcGxlbWVudCBhIGxlYXJuaW5nIHJhdGUgc2NoZWR1bGVyIGJhc2VkIG9uIHRoZSBTdGVwTFIgc3RyYXRlZ3kuIFlvdXIgY2xhc3Mgc2hvdWxkIGhhdmUgYW4gX19pbml0X18gbWV0aG9kIGltcGxlbWVudGVkIHRvIGluaXRpYWxpemUgd2l0aCBhbiBpbml0aWFsX2xyIChmbG9hdCksIHN0ZXBfc2l6ZSAoaW50KSwgYW5kIGdhbW1hIChmbG9hdCkgcGFyYW1ldGVyLiBJdCBzaG91bGQgYWxzbyBoYXZlIGEgKipnZXRfbHIoc2VsZiwgZXBvY2gpKiogbWV0aG9kIGltcGxlbWVudGVkIHRoYXQgcmV0dXJucyB0aGUgY3VycmVudCBsZWFybmluZyByYXRlIGZvciBhIGdpdmVuIGVwb2NoIChpbnQpLiBUaGUgbGVhcm5pbmcgcmF0ZSBzaG91bGQgYmUgZGVjcmVhc2VkIGJ5IGdhbW1hIGV2ZXJ5IHN0ZXBfc2l6ZSBlcG9jaHMuIFRoZSBhbnN3ZXIgc2hvdWxkIGJlIHJvdW5kZWQgdG8gNCBkZWNpbWFsIHBsYWNlcy4gT25seSB1c2Ugc3RhbmRhcmQgUHl0aG9uLg==",
    "starter_code": "\n\nclass Solution:\n    class StepLRScheduler:\n    def __init__(self, self, initial_lr, step_size, gamma):\n        # Initialize initial_lr, step_size, and gamma\n        pass\n\n    def get_lr(self, self, epoch):\n        # Calculate and return the learning rate for the given epoch\n        pass",
    "example_input": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=0))\nprint(scheduler.get_lr(epoch=4))\nprint(scheduler.get_lr(epoch=5))\nprint(scheduler.get_lr(epoch=9))\nprint(scheduler.get_lr(epoch=10))",
    "example_output": "0.1\n0.1\n0.05\n0.05\n0.025",
    "example_reasoning": "The initial learning rate is 0.1. It stays 0.1 for epochs 0-4. At epoch 5, it decays by 0.5 to 0.05. It stays 0.05 for epochs 5-9. At epoch 10, it decays again to 0.025.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    class StepLRScheduler:\n    def __init__(self, self, initial_lr, step_size, gamma):\n        \"\"\"\n        Initializes the StepLR scheduler.\n        Args:\n            initial_lr (float): The initial learning rate.\n            step_size (int): The period of learning rate decay.\n                             Learning rate is decayed every `step_size` epochs.\n            gamma (float): The multiplicative factor of learning rate decay.\n                           (e.g., 0.1 for reducing LR by 10x).\n        \"\"\"\n        self.initial_lr = initial_lr\n        self.step_size = step_size\n        self.gamma = gamma\n\n    def get_lr(self, self, epoch):\n        \"\"\"\n        Calculates and returns the current learning rate for a given epoch.\n        Args:\n            epoch (int): The current epoch number (0-indexed).\n        Returns:\n            float: The calculated learning rate for the current epoch.\n        \"\"\"\n        # Calculate the number of decays that have occurred.\n        # Integer division (//) in Python automatically performs the floor operation.\n        num_decays = epoch // self.step_size\n\n        # Apply the decay factor 'gamma' raised to the power of 'num_decays'\n        # to the initial learning rate.\n        current_lr = self.initial_lr * (self.gamma ** num_decays)\n\n        return round(current_lr, 4)",
    "test_cases": [
      {
        "test": [
          "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=0))"
        ],
        "expected_output": 0.1
      },
      {
        "test": [
          "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=4))"
        ],
        "expected_output": 0.1
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 154,
    "title": "ExponentialLR Learning Rate Scheduler",
    "difficulty": "easy",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gY2xhc3MgRXhwb25lbnRpYWxMUlNjaGVkdWxlciB0byBpbXBsZW1lbnQgYSBsZWFybmluZyByYXRlIHNjaGVkdWxlciBiYXNlZCBvbiB0aGUgRXhwb25lbnRpYWxMUiBzdHJhdGVneS4gWW91ciBjbGFzcyBzaG91bGQgaGF2ZSBhbiBfX2luaXRfXyBtZXRob2QgdG8gaW5pdGlhbGl6ZSB3aXRoIGFuIGluaXRpYWxfbHIgKGZsb2F0KSBhbmQgZ2FtbWEgKGZsb2F0KSBwYXJhbWV0ZXIuIEl0IHNob3VsZCBhbHNvIGhhdmUgYSBnZXRfbHIoc2VsZiwgZXBvY2gpIG1ldGhvZCB0aGF0IHJldHVybnMgdGhlIGN1cnJlbnQgbGVhcm5pbmcgcmF0ZSBmb3IgYSBnaXZlbiBlcG9jaCAoaW50KS4gVGhlIGxlYXJuaW5nIHJhdGUgc2hvdWxkIGJlIGRlY3JlYXNlZCBieSBnYW1tYSBldmVyeSBlcG9jaC4gVGhlIHJldHVybmVkIGxlYXJuaW5nIHJhdGUgc2hvdWxkIGJlIHJvdW5kZWQgdG8gNCBkZWNpbWFsIHBsYWNlcy4gT25seSB1c2Ugc3RhbmRhcmQgUHl0aG9uLg==",
    "starter_code": "\n\nclass Solution:\n    class ExponentialLRScheduler:\n    def __init__(self, self, initial_lr, gamma):\n        # Initialize initial_lr and gamma\n        pass\n\n    def get_lr(self, self, epoch):\n        # Calculate and return the learning rate for the given epoch\n        pass",
    "example_input": "scheduler = ExponentialLRScheduler(initial_lr=0.1, gamma=0.9)\nprint(f\"{scheduler.get_lr(epoch=0):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=1):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=2):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=3):.4f}\")",
    "example_output": "0.1000\n0.0900\n0.0810\n0.0729",
    "example_reasoning": "The initial learning rate is 0.1. At epoch 1, it decays by 0.9 to 0.09. At epoch 2, it decays again to 0.081, and so on, decaying by gamma every single epoch. All results are rounded to 4 decimal places.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    class ExponentialLRScheduler:\n    def __init__(self, self, initial_lr, gamma):\n        \"\"\"\n        Initializes the ExponentialLR scheduler.\n        Args:\n            initial_lr (float): The initial learning rate.\n            gamma (float): The multiplicative factor of learning rate decay per epoch.\n                           (e.g., 0.9 for reducing LR by 10% each epoch).\n        \"\"\"\n        self.initial_lr = initial_lr\n        self.gamma = gamma\n\n    def get_lr(self, self, epoch):\n        \"\"\"\n        Calculates and returns the current learning rate for a given epoch,\n        rounded to 4 decimal places.\n        Args:\n            epoch (int): The current epoch number (0-indexed).\n        Returns:\n            float: The calculated learning rate for the current epoch, rounded to 4 decimal places.\n        \"\"\"\n        # Apply the decay factor 'gamma' raised to the power of the current 'epoch'\n        # to the initial learning rate.\n        current_lr = self.initial_lr * (self.gamma ** epoch)\n\n        # Round the learning rate to 4 decimal places\n        return round(current_lr, 4)",
    "test_cases": [
      {
        "test": [
          "scheduler = ExponentialLRScheduler(initial_lr=0.1, gamma=0.9)\nprint(f\"{scheduler.get_lr(epoch=0):.4f}\")"
        ],
        "expected_output": 0.1
      },
      {
        "test": [
          "scheduler = ExponentialLRScheduler(initial_lr=0.1, gamma=0.9)\nprint(f\"{scheduler.get_lr(epoch=1):.4f}\")"
        ],
        "expected_output": 0.09
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 155,
    "title": "CosineAnnealingLR Learning Rate Scheduler",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gY2xhc3MgQ29zaW5lQW5uZWFsaW5nTFJTY2hlZHVsZXIgdG8gaW1wbGVtZW50IGEgbGVhcm5pbmcgcmF0ZSBzY2hlZHVsZXIgYmFzZWQgb24gdGhlIENvc2luZSBBbm5lYWxpbmcgTFIgc3RyYXRlZ3kuIFlvdXIgY2xhc3Mgc2hvdWxkIGhhdmUgYW4gX19pbml0X18gbWV0aG9kIHRvIGluaXRpYWxpemUgd2l0aCBhbiBpbml0aWFsX2xyIChmbG9hdCksIFRfbWF4IChpbnQsIHRoZSBtYXhpbXVtIG51bWJlciBvZiBpdGVyYXRpb25zL2Vwb2NocyksIGFuZCBtaW5fbHIgKGZsb2F0LCB0aGUgbWluaW11bSBsZWFybmluZyByYXRlKSBwYXJhbWV0ZXJzLiBJdCBzaG91bGQgYWxzbyBoYXZlIGEgKipnZXRfbHIoc2VsZiwgZXBvY2gpKiogbWV0aG9kIHRoYXQgcmV0dXJucyB0aGUgY3VycmVudCBsZWFybmluZyByYXRlIGZvciBhIGdpdmVuIGVwb2NoIChpbnQpLiBUaGUgbGVhcm5pbmcgcmF0ZSBzaG91bGQgZm9sbG93IGEgY29zaW5lIGFubmVhbGluZyBzY2hlZHVsZS4gVGhlIHJldHVybmVkIGxlYXJuaW5nIHJhdGUgc2hvdWxkIGJlIHJvdW5kZWQgdG8gNCBkZWNpbWFsIHBsYWNlcy4gT25seSB1c2Ugc3RhbmRhcmQgUHl0aG9uIGFuZCB0aGUgbWF0aCBtb2R1bGUgZm9yIHRyaWdvbm9tZXRyaWMgZnVuY3Rpb25zLg==",
    "starter_code": "import math\n\nclass Solution:\n    class CosineAnnealingLRScheduler:\n    def __init__(self, self, initial_lr, T_max, min_lr):\n        # Initialize initial_lr, T_max, and min_lr\n        pass\n\n    def get_lr(self, self, epoch):\n        # Calculate and return the learning rate for the given epoch, rounded to 4 decimal places\n        pass",
    "example_input": "import math\nscheduler = CosineAnnealingLRScheduler(initial_lr=0.1, T_max=10, min_lr=0.001)\nprint(f\"{scheduler.get_lr(epoch=0):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=2):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=5):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=7):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=10):.4f}\")",
    "example_output": "0.1000\n0.0905\n0.0505\n0.0214\n0.0010",
    "example_reasoning": "The learning rate starts at initial_lr (0.1), follows a cosine curve, reaches min_lr (0.001) at T_max (epoch 10), and then cycles back up. Each value is rounded to 4 decimal places.",
    "learn_content": "IyAqKkxlYXJuaW5nIFJhdGUgU2NoZWR1bGVyczogQ29zaW5lQW5uZWFsaW5nTFIqKgoKIyMgKioxLiBEZWZpbml0aW9uKioKQSAqKmxlYXJuaW5nIHJhdGUgc2NoZWR1bGVyKiogaXMgYSB0ZWNobmlxdWUgdXNlZCBpbiBtYWNoaW5lIGxlYXJuaW5nIHRvIGFkanVzdCB0aGUgbGVhcm5pbmcgcmF0ZSBkdXJpbmcgdGhlIHRyYWluaW5nIG9mIGEgbW9kZWwuIFRoZSAqKmxlYXJuaW5nIHJhdGUqKiBkaWN0YXRlcyB0aGUgc3RlcCBzaXplIHRha2VuIGluIHRoZSBkaXJlY3Rpb24gb2YgdGhlIG5lZ2F0aXZlIGdyYWRpZW50IG9mIHRoZSBsb3NzIGZ1bmN0aW9uLgoKKipDb3NpbmVBbm5lYWxpbmdMUiAoQ29zaW5lIEFubmVhbGluZyBMZWFybmluZyBSYXRlKSoqIGlzIGEgc2NoZWR1bGVyIHRoYXQgYWltcyB0byBkZWNyZWFzZSB0aGUgbGVhcm5pbmcgcmF0ZSBmcm9tIGEgbWF4aW11bSB2YWx1ZSB0byBhIG1pbmltdW0gdmFsdWUgZm9sbG93aW5nIHRoZSBzaGFwZSBvZiBhIGNvc2luZSBjdXJ2ZS4gVGhpcyBhcHByb2FjaCBoZWxwcyBpbiBhY2hpZXZpbmcgZmFzdGVyIGNvbnZlcmdlbmNlIHdoaWxlIGFsc28gYWxsb3dpbmcgdGhlIG1vZGVsIHRvIGV4cGxvcmUgZmxhdHRlciByZWdpb25zIG9mIHRoZSBsb3NzIGxhbmRzY2FwZSB0b3dhcmRzIHRoZSBlbmQgb2YgdHJhaW5pbmcuIEl0IGlzIHBhcnRpY3VsYXJseSBlZmZlY3RpdmUgZm9yIGRlZXAgbmV1cmFsIG5ldHdvcmtzLgoKIyMgKioyLiBXaHkgVXNlIExlYXJuaW5nIFJhdGUgU2NoZWR1bGVycz8qKgoqICoqRmFzdGVyIENvbnZlcmdlbmNlOioqIEEgaGlnaGVyIGluaXRpYWwgbGVhcm5pbmcgcmF0ZSBhbGxvd3MgZm9yIHF1aWNrZXIgbW92ZW1lbnQgdGhyb3VnaCB0aGUgbG9zcyBsYW5kc2NhcGUuCiogKipJbXByb3ZlZCBQZXJmb3JtYW5jZToqKiBBIHNtYWxsZXIgbGVhcm5pbmcgcmF0ZSB0b3dhcmRzIHRoZSBlbmQgb2YgdHJhaW5pbmcgZW5hYmxlcyBmaW5lciBhZGp1c3RtZW50cywgaGVscGluZyB0aGUgbW9kZWwgY29udmVyZ2UgdG8gYSBiZXR0ZXIgbG9jYWwgbWluaW11bSBhbmQgcHJldmVudGluZyBvc2NpbGxhdGlvbnMuCiogKipBdm9pZGluZyBMb2NhbCBNaW5pbWE6KiogVGhlIGN5Y2xpY2FsIG5hdHVyZSAob3IgYSBwYXJ0IG9mIGl0LCBhcyBvZnRlbiBzZWVuIGluIHJlc3RhcnRzKSBvZiBjb3NpbmUgYW5uZWFsaW5nIGNhbiBoZWxwIHRoZSBvcHRpbWl6ZXIgZXNjYXBlIHNoYWxsb3cgbG9jYWwgbWluaW1hLgoqICoqU3RhYmlsaXR5OioqIEdyYWR1YWwgcmVkdWN0aW9uIGluIGxlYXJuaW5nIHJhdGUgcHJvbW90ZXMgdHJhaW5pbmcgc3RhYmlsaXR5LgoKIyMgKiozLiBDb3NpbmVBbm5lYWxpbmdMUiBNZWNoYW5pc20qKgpUaGUgbGVhcm5pbmcgcmF0ZSBpcyBzY2hlZHVsZWQgYWNjb3JkaW5nIHRvIGEgY29zaW5lIGZ1bmN0aW9uLiBPdmVyIGEgY3ljbGUgb2YgJFRfe1x0ZXh0e21heH19JCBlcG9jaHMsIHRoZSBsZWFybmluZyByYXRlIGRlY3JlYXNlcyBmcm9tIGFuIGluaXRpYWwgbGVhcm5pbmcgcmF0ZSAob2Z0ZW4gY29uc2lkZXJlZCB0aGUgbWF4aW11bSAkTFJfe1x0ZXh0e21heH19JCkgdG8gYSBtaW5pbXVtIGxlYXJuaW5nIHJhdGUgKCRMUl97XHRleHR7bWlufX0kKS4KClRoZSBmb3JtdWxhIGZvciB0aGUgbGVhcm5pbmcgcmF0ZSBhdCBhIGdpdmVuIGVwb2NoIGUgaXM6CgokJExSX2UgPSBMUl97XHRleHR7bWlufX0gKyAwLjUgXHRpbWVzIChMUl97XHRleHR7aW5pdGlhbH19IC0gTFJfe1x0ZXh0e21pbn19KSBcdGltZXMgXGxlZnQoMSArIFxjb3NcbGVmdChcZnJhY3tlfXtUX3tcdGV4dHttYXh9fX0gXHRpbWVzIFxwaVxyaWdodClccmlnaHQpJCQKCldoZXJlOgoqICRMUl9lJDogVGhlIGxlYXJuaW5nIHJhdGUgYXQgZXBvY2ggZS4KKiAkTFJfe1x0ZXh0e2luaXRpYWx9fSQ6IFRoZSBpbml0aWFsIChtYXhpbXVtKSBsZWFybmluZyByYXRlLgoqICRMUl97XHRleHR7bWlufX0kOiBUaGUgbWluaW11bSBsZWFybmluZyByYXRlIHRoYXQgdGhlIHNjaGVkdWxlIHdpbGwgcmVhY2guCiogJFRfe1x0ZXh0e21heH19JDogVGhlIG1heGltdW0gbnVtYmVyIG9mIGVwb2NocyBpbiB0aGUgY29zaW5lIGFubmVhbGluZyBjeWNsZS4gVGhlIGxlYXJuaW5nIHJhdGUgd2lsbCByZWFjaCAkTFJfe1x0ZXh0e21pbn19JCBhdCBlcG9jaCAkVF97XHRleHR7bWF4fX0kLgoqIGU6IFRoZSBjdXJyZW50IGVwb2NoIG51bWJlciAoMC1pbmRleGVkKSwgY2xhbXBlZCBiZXR3ZWVuIDAgYW5kICRUX3tcdGV4dHttYXh9fSQuCiogz4A6IFRoZSBtYXRoZW1hdGljYWwgY29uc3RhbnQgcGkgKGFwcHJveGltYXRlbHkgMy4xNDE1OSkuCiogJFxjb3MoXGNkb3QpJDogVGhlIGNvc2luZSBmdW5jdGlvbi4KCioqRXhhbXBsZToqKgpJZiAkTFJfe1x0ZXh0e2luaXRpYWx9fSA9IDAuMSQsICRUX3tcdGV4dHttYXh9fSA9IDEwJCwgYW5kICRMUl97XHRleHR7bWlufX0gPSAwLjAwMSQ6CgoqICoqRXBvY2ggMDoqKiAKICAkTFJfMCA9IDAuMDAxICsgMC41IFx0aW1lcyAoMC4xIC0gMC4wMDEpIFx0aW1lcyAoMSArIFxjb3MoMCkpID0gMC4wMDEgKyAwLjA0OTUgXHRpbWVzIDIgPSAwLjEkCgoqICoqRXBvY2ggNSAobWlkLXBvaW50KToqKiAKICAkTFJfNSA9IDAuMDAxICsgMC41IFx0aW1lcyAoMC4xIC0gMC4wMDEpIFx0aW1lcyAoMSArIFxjb3MoXHBpLzIpKSA9IDAuMDAxICsgMC4wNDk1IFx0aW1lcyAxID0gMC4wNTA1JAoKKiAqKkVwb2NoIDEwIChlbmQgb2YgY3ljbGUpOioqIAogICRMUl97MTB9ID0gMC4wMDEgKyAwLjUgXHRpbWVzICgwLjEgLSAwLjAwMSkgXHRpbWVzICgxICsgXGNvcyhccGkpKSA9IDAuMDAxICsgMC4wNDk1IFx0aW1lcyAwID0gMC4wMDEkCgojIyAqKjQuIEFwcGxpY2F0aW9ucyBvZiBMZWFybmluZyBSYXRlIFNjaGVkdWxlcnMqKgpMZWFybmluZyByYXRlIHNjaGVkdWxlcnMsIGluY2x1ZGluZyBDb3NpbmVBbm5lYWxpbmdMUiwgYXJlIHdpZGVseSB1c2VkIGluIHRyYWluaW5nIHZhcmlvdXMgbWFjaGluZSBsZWFybmluZyBtb2RlbHMsIGVzcGVjaWFsbHkgZGVlcCBuZXVyYWwgbmV0d29ya3MsIGFjcm9zcyBkaXZlcnNlIGFwcGxpY2F0aW9ucyBzdWNoIGFzOgoqICoqSW1hZ2UgQ2xhc3NpZmljYXRpb246KiogVHJhaW5pbmcgQ29udm9sdXRpb25hbCBOZXVyYWwgTmV0d29ya3MgKENOTnMpIGZvciB0YXNrcyBsaWtlIG9iamVjdCByZWNvZ25pdGlvbi4KKiAqKk5hdHVyYWwgTGFuZ3VhZ2UgUHJvY2Vzc2luZyAoTkxQKToqKiBUcmFpbmluZyBSZWN1cnJlbnQgTmV1cmFsIE5ldHdvcmtzIChSTk5zKSBhbmQgVHJhbnNmb3JtZXJzIGZvciB0YXNrcyBsaWtlIG1hY2hpbmUgdHJhbnNsYXRpb24sIHRleHQgZ2VuZXJhdGlvbiwgYW5kIHNlbnRpbWVudCBhbmFseXNpcy4KKiAqKlNwZWVjaCBSZWNvZ25pdGlvbjoqKiBUcmFpbmluZyBtb2RlbHMgZm9yIGNvbnZlcnRpbmcgc3Bva2VuIGxhbmd1YWdlIHRvIHRleHQuCiogKipSZWluZm9yY2VtZW50IExlYXJuaW5nOioqIE9wdGltaXppbmcgcG9saWNpZXMgaW4gcmVpbmZvcmNlbWVudCBsZWFybmluZyBhZ2VudHMuCiogKipBbnkgb3B0aW1pemF0aW9uIHByb2JsZW0qKiB3aGVyZSBncmFkaWVudCBkZXNjZW50IG9yIGl0cyB2YXJpYW50cyBhcmUgdXNlZC4=",
    "solution_code": "import math\n\nclass Solution:\n    class CosineAnnealingLRScheduler:\n    def __init__(self, self, initial_lr, T_max, min_lr):\n        \"\"\"\n        Initializes the CosineAnnealingLR scheduler.\n        Args:\n            initial_lr (float): The initial (maximum) learning rate.\n            T_max (int): The maximum number of epochs in the cosine annealing cycle.\n                         The learning rate will reach min_lr at this epoch.\n            min_lr (float): The minimum learning rate.\n        \"\"\"\n        self.initial_lr = initial_lr\n        self.T_max = T_max\n        self.min_lr = min_lr\n\n    def get_lr(self, self, epoch):\n        \"\"\"\n        Calculates and returns the current learning rate for a given epoch,\n        following a cosine annealing schedule and rounded to 4 decimal places.\n        Args:\n            epoch (int): The current epoch number (0-indexed).\n        Returns:\n            float: The calculated learning rate for the current epoch, rounded to 4 decimal places.\n        \"\"\"\n        # Ensure epoch does not exceed T_max for the calculation cycle,\n        # as the cosine formula is typically defined for e from 0 to T_max.\n        # Although in practice, schedulers might restart or hold LR after T_max.\n        # For this problem, we'll clamp it to T_max if it goes over.\n        current_epoch = min(epoch, self.T_max)\n\n        # Calculate the learning rate using the Cosine Annealing formula\n        # LR_e = LR_min + 0.5 * (LR_initial - LR_min) * (1 + cos(e / T_max * pi))\n        lr = self.min_lr + 0.5 * (self.initial_lr - self.min_lr) *              (1 + math.cos(current_epoch / self.T_max * math.pi))\n\n        # Round the learning rate to 4 decimal places\n        return round(lr, 4)",
    "test_cases": [
      {
        "test": [
          "import math\nscheduler = CosineAnnealingLRScheduler(initial_lr=0.1, T_max=10, min_lr=0.001)\nprint(f\"{scheduler.get_lr(epoch=0):.4f}\")"
        ],
        "expected_output": 0.1
      },
      {
        "test": [
          "import math\nscheduler = CosineAnnealingLRScheduler(initial_lr=0.1, T_max=10, min_lr=0.001)\nprint(f\"{scheduler.get_lr(epoch=2):.4f}\")"
        ],
        "expected_output": 0.0905
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 156,
    "title": "Implement SwiGLU activation function",
    "difficulty": "easy",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgUHl0aG9uIGZ1bmN0aW9uIHRoYXQgYXBwbGllcyB0aGUgKipTd2lHTFUgYWN0aXZhdGlvbiBmdW5jdGlvbioqIHRvIGEgTnVtUHkgYXJyYXkuIEFzc3VtZSB0aGUgaW5wdXQgYXJyYXkgaGFzIGFscmVhZHkgYmVlbiBwYXNzZWQgdGhyb3VnaCBhIGxpbmVhciBwcm9qZWN0aW9uIGFuZCBoYXMgc2hhcGUgYChiYXRjaF9zaXplLCAyZClgLiBSb3VuZCBlYWNoIG91dHB1dCB0byBmb3VyIGRlY2ltYWwgcGxhY2VzIGFuZCByZXR1cm4gdGhlIHJlc3VsdCBhcyBhIE51bVB5IGFycmF5IG9mIHRoZSBzaGFwZSBgKGJhdGNoX3NpemUsIGQpYC4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def SwiGLU(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Args:\n            x: np.ndarray of shape (batch_size, 2d)\n        Returns:\n            np.ndarray of shape (batch_size, d)\n        \"\"\"\n        # Your code here\n        return scores",
    "example_input": "np.array([[1, -1, 1000, -1000]])",
    "example_output": "[[1000., 0.]]",
    "example_reasoning": "The input is of shape (1, 4), so it is split into x1 = [1, -1] and x2 = [1000, -1000]. The sigmoid of 1000 is approximately 1, and the sigmoid of -1000 is approximately 0 due to saturation. Thus, Swish(1000) \u2248 1000 x 1 = 1000 and Swish(-1000) \u2248 -1000 x 0 = 0. Then, SwiGLU = x1 * Swish(x2) = [1 x 1000, -1 x 0] = [1000, 0].",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def SwiGLU(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Args:\n            x: np.ndarray of shape (batch_size, 2d)\n        Returns:\n            np.ndarray of shape (batch_size, d)\n        \"\"\"\n        def sigmoid(self, x):\n            return 1 / (1 + np.exp(-x))\n\n        d = x.shape[1] // 2\n        x1 = x[:, :d]\n        x2 = x[:, d:]\n        return x1 * (x2 * sigmoid(x2))",
    "test_cases": [
      {
        "test": [
          "print(np.round(SwiGLU(np.zeros((1, 4))), 4))"
        ],
        "expected_output": [
          [
            0.0,
            0.0
          ]
        ]
      },
      {
        "test": [
          "print(np.round(SwiGLU(np.array([[1.0, -1.0, 2.0, -2.0]])), 4))"
        ],
        "expected_output": [
          [
            1.7616,
            0.2384
          ]
        ]
      }
    ],
    "function_name": "SwiGLU"
  },
  {
    "id": 157,
    "title": "Implement the Bellman Equation for Value Iteration",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "V3JpdGUgYSBmdW5jdGlvbiB0aGF0IHBlcmZvcm1zIG9uZSBzdGVwIG9mIHZhbHVlIGl0ZXJhdGlvbiBmb3IgYSBnaXZlbiBNYXJrb3YgRGVjaXNpb24gUHJvY2VzcyAoTURQKSB1c2luZyB0aGUgQmVsbG1hbiBlcXVhdGlvbi4gVGhlIGZ1bmN0aW9uIHNob3VsZCB1cGRhdGUgdGhlIHN0YXRlLXZhbHVlIGZ1bmN0aW9uIFYocykgZm9yIGVhY2ggc3RhdGUgYmFzZWQgb24gcG9zc2libGUgYWN0aW9ucywgdHJhbnNpdGlvbiBwcm9iYWJpbGl0aWVzLCByZXdhcmRzLCBhbmQgdGhlIGRpc2NvdW50IGZhY3RvciBnYW1tYS4gT25seSB1c2UgTnVtUHku",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def bellman_update(self, V, transitions, gamma):\n        \"\"\"\n        Perform one step of value iteration using the Bellman equation.\n        Args:\n          V: np.ndarray, state values, shape (n_states,)\n          transitions: list of dicts. transitions[s][a] is a list of (prob, next_state, reward, done)\n          gamma: float, discount factor\n        Returns:\n          np.ndarray, updated state values\n        \"\"\"\n        # TODO: Implement Bellman update\n        pass",
    "example_input": "import numpy as np\ntransitions = [\n  {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 1, 1.0, False)]},\n  {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 1, 1.0, True)]}\n]\nV = np.array([0.0, 0.0])\ngamma = 0.9\nnew_V = bellman_update(V, transitions, gamma)\nprint(np.round(new_V, 2))",
    "example_output": "[1. 1.]",
    "example_reasoning": "For state 0, the best action is to go to state 1 and get a reward of 1. For state 1, taking action 1 gives a reward of 1 and ends the episode, so its value is 1.",
    "learn_content": "IyAqKlRoZSBCZWxsbWFuIEVxdWF0aW9uKioKClRoZSAqKkJlbGxtYW4gZXF1YXRpb24qKiBpcyBhIGZ1bmRhbWVudGFsIHJlY3Vyc2l2ZSBlcXVhdGlvbiBpbiByZWluZm9yY2VtZW50IGxlYXJuaW5nIHRoYXQgcmVsYXRlcyB0aGUgdmFsdWUgb2YgYSBzdGF0ZSB0byB0aGUgdmFsdWVzIG9mIHBvc3NpYmxlIG5leHQgc3RhdGVzLiBJdCBwcm92aWRlcyB0aGUgbWF0aGVtYXRpY2FsIGZvdW5kYXRpb24gZm9yIGtleSBSTCBhbGdvcml0aG1zIHN1Y2ggYXMgdmFsdWUgaXRlcmF0aW9uIGFuZCBRLWxlYXJuaW5nLgoKLS0tCgojIyAqKktleSBJZGVhKioKRm9yIGVhY2ggc3RhdGUgJHMkLCB0aGUgdmFsdWUgJFYocykkIGlzIHRoZSBtYXhpbXVtIGV4cGVjdGVkIHJldHVybiBvYnRhaW5hYmxlIGJ5IGNob29zaW5nIHRoZSBiZXN0IGFjdGlvbiAkYSQgYW5kIHRoZW4gZm9sbG93aW5nIHRoZSBvcHRpbWFsIHBvbGljeToKCiQkClYocykgPSBcbWF4X3thfSBcc3VtX3tzJ30gUChzJ3xzLCBhKSBcbGVmdFsgUihzLCBhLCBzJykgKyBcZ2FtbWEgVihzJykgXHJpZ2h0XQokJAoKV2hlcmU6Ci0gJFYocykkOiB2YWx1ZSBvZiBzdGF0ZSAkcyQKLSAkYSQ6IHBvc3NpYmxlIGFjdGlvbnMKLSAkUChzJ3xzLCBhKSQ6IHByb2JhYmlsaXR5IG9mIG1vdmluZyB0byBzdGF0ZSAkcyckIGZyb20gJHMkIHZpYSAkYSQKLSAkUihzLCBhLCBzJykkOiByZXdhcmQgZm9yIHRoaXMgdHJhbnNpdGlvbgotICRcZ2FtbWEkOiBkaXNjb3VudCBmYWN0b3IgKCQwIFxsZXEgXGdhbW1hIFxsZXEgMSQpCi0gJFYocycpJDogdmFsdWUgb2YgbmV4dCBzdGF0ZQoKLS0tCgojIyAqKkhvdyB0byBVc2UqKgoxLiAqKkZvciBlYWNoIHN0YXRlOioqCiAgIC0gRm9yIGVhY2ggcG9zc2libGUgYWN0aW9uLCBzdW0gb3ZlciBwb3NzaWJsZSBuZXh0IHN0YXRlcywgd2VpZ2h0aW5nIGJ5IHRyYW5zaXRpb24gcHJvYmFiaWxpdHkuCiAgIC0gQWRkIHRoZSBpbW1lZGlhdGUgcmV3YXJkIGFuZCB0aGUgZGlzY291bnRlZCB2YWx1ZSBvZiB0aGUgbmV4dCBzdGF0ZS4KICAgLSBDaG9vc2UgdGhlIGFjdGlvbiB3aXRoIHRoZSBoaWdoZXN0IGV4cGVjdGVkIHZhbHVlIChmb3IgY29udHJvbCkuCjIuICoqUmVwZWF0IHVudGlsIHZhbHVlcyBjb252ZXJnZSoqICh2YWx1ZSBpdGVyYXRpb24pIG9yIGFzIHBhcnQgb2Ygb3RoZXIgUkwgdXBkYXRlcy4KCi0tLQoKIyMgKipBcHBsaWNhdGlvbnMqKgotICoqVmFsdWUgSXRlcmF0aW9uKiogYW5kICoqUG9saWN5IEl0ZXJhdGlvbioqIGluIE1hcmtvdiBEZWNpc2lvbiBQcm9jZXNzZXMgKE1EUCkKLSAqKlEtbGVhcm5pbmcqKiBhbmQgb3RoZXIgUkwgYWxnb3JpdGhtcwotIENhbGN1bGF0aW5nIHRoZSBvcHRpbWFsIHZhbHVlIGZ1bmN0aW9uIGFuZCBwb2xpY3kgaW4gZ3JpZHdvcmxkcywgZ2FtZXMsIGFuZCBnZW5lcmFsIE1EUHMKCi0tLQoKIyMgKipXaHkgSXQgTWF0dGVycyoqCi0gVGhlIEJlbGxtYW4gZXF1YXRpb24gZm9ybWFsaXplcyB0aGUgbm90aW9uIG9mICoqb3B0aW1hbGl0eSoqIGluIHNlcXVlbnRpYWwgZGVjaXNpb24tbWFraW5nLgotIEl0IGlzIGEgYmFja2JvbmUgZm9yIHRlYWNoaW5nIGFnZW50cyB0byBzb2x2ZSBlbnZpcm9ubWVudHMgd2l0aCByZXdhcmRzLCB1bmNlcnRhaW50eSwgYW5kIGxvbmctdGVybSBwbGFubmluZy4=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def bellman_update(self, V, transitions, gamma):\n        n_states = len(V)\n        new_V = np.zeros_like(V)\n        for s in range(n_states):\n            action_values = []\n            for a in transitions[s]:\n                total = 0\n                for prob, next_s, reward, done in transitions[s][a]:\n                    total += prob * (reward + gamma * (0 if done else V[next_s]))\n                action_values.append(total)\n            new_V[s] = max(action_values)\n        return new_V",
    "test_cases": [
      {
        "test": [
          "import numpy as np\ntransitions = [\n  # For state 0\n  {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 1, 1.0, False)]},\n  # For state 1\n  {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 1, 1.0, True)]}\n]\nV = np.array([0.0, 0.0])\ngamma = 0.9\nnew_V = bellman_update(V, transitions, gamma)\nprint(np.round(new_V, 2))"
        ],
        "expected_output": [
          1.0,
          1.0
        ]
      },
      {
        "test": [
          "import numpy as np\ntransitions = [\n  {0: [(0.8, 0, 5, False), (0.2, 1, 10, False)], 1: [(1.0, 1, 2, False)]},\n  {0: [(1.0, 0, 0, False)], 1: [(1.0, 1, 0, True)]}\n]\nV = np.array([0.0, 0.0])\ngamma = 0.5\nnew_V = bellman_update(V, transitions, gamma)\nprint(np.round(new_V, 2))"
        ],
        "expected_output": [
          6.0,
          0.0
        ]
      }
    ],
    "function_name": "bellman_update"
  },
  {
    "id": 158,
    "title": "Epsilon-Greedy Action Selection for n-Armed Bandit",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBlcHNpbG9uLWdyZWVkeSBtZXRob2QgZm9yIGFjdGlvbiBzZWxlY3Rpb24gaW4gYW4gbi1hcm1lZCBiYW5kaXQgcHJvYmxlbS4gR2l2ZW4gYSBzZXQgb2YgZXN0aW1hdGVkIGFjdGlvbiB2YWx1ZXMgKFEtdmFsdWVzKSwgc2VsZWN0IGFuIGFjdGlvbiB1c2luZyB0aGUgZXBzaWxvbi1ncmVlZHkgcG9saWN5OiB3aXRoIHByb2JhYmlsaXR5IGVwc2lsb24sIGNob29zZSBhIHJhbmRvbSBhY3Rpb247IHdpdGggcHJvYmFiaWxpdHkgMSAtIGVwc2lsb24sIGNob29zZSB0aGUgYWN0aW9uIHdpdGggdGhlIGhpZ2hlc3QgZXN0aW1hdGVkIHZhbHVlLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def epsilon_greedy(self, Q, epsilon=0.1):\n        \"\"\"\n        Selects an action using epsilon-greedy policy.\n        Q: np.ndarray of shape (n,) -- estimated action values\n        epsilon: float in [0, 1]\n        Returns: int, selected action index\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "Q = np.array([0.5, 2.3, 1.7])\nepsilon = 0.0\naction = epsilon_greedy(Q, epsilon)\nprint(action)",
    "example_output": "1",
    "example_reasoning": "With epsilon=0.0 (always greedy), the highest Q-value is 2.3 at index 1, so the function always returns 1.",
    "learn_content": "IyMjIEVwc2lsb24tR3JlZWR5IFBvbGljeQoKVGhlIGVwc2lsb24tZ3JlZWR5IG1ldGhvZCBpcyBhIGZ1bmRhbWVudGFsIGFjdGlvbiBzZWxlY3Rpb24gc3RyYXRlZ3kgdXNlZCBpbiByZWluZm9yY2VtZW50IGxlYXJuaW5nLCBlc3BlY2lhbGx5IGZvciBzb2x2aW5nIHRoZSBuLWFybWVkIGJhbmRpdCBwcm9ibGVtLiBUaGUga2V5IGlkZWEgaXMgdG8gYmFsYW5jZSAqKmV4cGxvcmF0aW9uKiogKHRyeWluZyBuZXcgYWN0aW9ucykgYW5kICoqZXhwbG9pdGF0aW9uKiogKGNob29zaW5nIHRoZSBiZXN0LWtub3duIGFjdGlvbik6CgotIFdpdGggcHJvYmFiaWxpdHkgJFx2YXJlcHNpbG9uJCAoZXBzaWxvbiksIHRoZSBhZ2VudCBleHBsb3JlcyBieSBzZWxlY3RpbmcgYW4gYWN0aW9uIGF0IHJhbmRvbS4KLSBXaXRoIHByb2JhYmlsaXR5ICQxLVx2YXJlcHNpbG9uJCwgaXQgZXhwbG9pdHMgYnkgY2hvb3NpbmcgdGhlIGFjdGlvbiB3aXRoIHRoZSBoaWdoZXN0IGVzdGltYXRlZCB2YWx1ZSAoZ3JlZWR5IGNob2ljZSkuCgpUaGUgZXBzaWxvbi1ncmVlZHkgcG9saWN5IGlzIHNpbXBsZSB0byBpbXBsZW1lbnQgYW5kIHByb3ZpZGVzIGEgd2F5IHRvIGF2b2lkIGdldHRpbmcgc3R1Y2sgd2l0aCBzdWJvcHRpbWFsIGFjdGlvbnMgZHVlIHRvIGluc3VmZmljaWVudCBleHBsb3JhdGlvbi4=",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def epsilon_greedy(self, Q, epsilon=0.1):\n        if np.random.rand() < epsilon:\n            return np.random.randint(len(Q))\n        else:\n            return int(np.argmax(Q))",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(0)\nprint([epsilon_greedy(np.array([1, 2, 3]), epsilon=0.0) for _ in range(5)])"
        ],
        "expected_output": [
          2,
          2,
          2,
          2,
          2
        ]
      },
      {
        "test": [
          "import numpy as np\nnp.random.seed(1)\nprint([epsilon_greedy(np.array([5, 2, 1]), epsilon=1.0) for _ in range(5)])"
        ],
        "expected_output": [
          0,
          1,
          0,
          0,
          0
        ]
      }
    ],
    "function_name": "epsilon_greedy"
  },
  {
    "id": 159,
    "title": "Incremental Mean for Online Reward Estimation",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "SW1wbGVtZW50IGFuIGVmZmljaWVudCBtZXRob2QgdG8gdXBkYXRlIHRoZSBtZWFuIHJld2FyZCBmb3IgYSBrLWFybWVkIGJhbmRpdCBhY3Rpb24gYWZ0ZXIgcmVjZWl2aW5nIGVhY2ggbmV3IHJld2FyZCwgKip3aXRob3V0IHN0b3JpbmcgdGhlIGZ1bGwgaGlzdG9yeSBvZiByZXdhcmRzKiouIEdpdmVuIHRoZSBwcmV2aW91cyBtZWFuIGVzdGltYXRlIChRX3ByZXYpLCB0aGUgbnVtYmVyIG9mIHRpbWVzIHRoZSBhY3Rpb24gaGFzIGJlZW4gc2VsZWN0ZWQgKGspLCBhbmQgYSBuZXcgcmV3YXJkIChSKSwgY29tcHV0ZSB0aGUgdXBkYXRlZCBtZWFuIHVzaW5nIHRoZSBpbmNyZW1lbnRhbCBmb3JtdWxhLgoKKipOb3RlOioqIFVzaW5nIGEgcmVndWxhciBtZWFuIHRoYXQgc3RvcmVzIGFsbCBwYXN0IHJld2FyZHMgd2lsbCBldmVudHVhbGx5IHJ1biBvdXQgb2YgbWVtb3J5LiBZb3VyIHNvbHV0aW9uIHNob3VsZCB1c2Ugb25seSB0aGUgcHJldmlvdXMgbWVhbiwgdGhlIGNvdW50LCBhbmQgdGhlIG5ldyByZXdhcmQu",
    "starter_code": "\n\nclass Solution:\n    def incremental_mean(self, Q_prev, k, R):\n        \"\"\"\n        Q_prev: previous mean estimate (float)\n        k: number of times the action has been selected (int)\n        R: new observed reward (float)\n        Returns: new mean estimate (float)\n        \"\"\"\n        # Your code here\n        pass\n",
    "example_input": "Q_prev = 2.0\nk = 2\nR = 6.0\nnew_Q = incremental_mean(Q_prev, k, R)\nprint(round(new_Q, 2))",
    "example_output": "4.0",
    "example_reasoning": "The updated mean is Q_prev + (1/k) * (R - Q_prev) = 2.0 + (1/2)*(6.0 - 2.0) = 2.0 + 2.0 = 4.0",
    "learn_content": "IyMjIEluY3JlbWVudGFsIE1lYW4gVXBkYXRlIFJ1bGUKClRoZSBpbmNyZW1lbnRhbCBtZWFuIGZvcm11bGEgbGV0cyB5b3UgdXBkYXRlIHlvdXIgZXN0aW1hdGUgb2YgdGhlIG1lYW4gYWZ0ZXIgZWFjaCBuZXcgb2JzZXJ2YXRpb24sICoqd2l0aG91dCBrZWVwaW5nIGFsbCBwcmV2aW91cyByZXdhcmRzIGluIG1lbW9yeSoqLiBGb3IgdGhlIGstdGggcmV3YXJkICRSX2skIGFuZCBwcmV2aW91cyBlc3RpbWF0ZSAkUV97a30kOgoKJCQKUV97aysxfSA9IFFfayArIFxmcmFjezF9e2t9IChSX2sgLSBRX2spCiQkCgpUaGlzIHNhdmVzIG1lbW9yeSBjb21wYXJlZCB0byB0aGUgcmVndWxhciBtZWFuLCB3aGljaCByZXF1aXJlcyBzdG9yaW5nIGFsbCBwYXN0IHJld2FyZHMgYW5kIHJlY2FsY3VsYXRpbmcgZWFjaCB0aW1lLiBUaGUgaW5jcmVtZW50YWwgcnVsZSBpcyBjcnVjaWFsIGZvciBvbmxpbmUgbGVhcm5pbmcgYW5kIGxhcmdlLXNjYWxlIHByb2JsZW1zIHdoZXJlIHN0b3JpbmcgYWxsIGRhdGEgaXMgaW1wcmFjdGljYWwu",
    "solution_code": "\n\nclass Solution:\n    def incremental_mean(self, Q_prev, k, R):\n        return Q_prev + (1 / k) * (R - Q_prev)",
    "test_cases": [
      {
        "test": [
          "Q = 0.0\nk = 1\nR = 5.0\nprint(round(incremental_mean(Q, k, R), 4))"
        ],
        "expected_output": 5.0
      },
      {
        "test": [
          "Q = 5.0\nk = 2\nR = 7.0\nprint(round(incremental_mean(Q, k, R), 4))"
        ],
        "expected_output": 6.0
      }
    ],
    "function_name": "incremental_mean"
  },
  {
    "id": 160,
    "title": "Mixed Precision Training",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "V3JpdGUgYSBQeXRob24gY2xhc3MgdG8gaW1wbGVtZW50IE1peGVkIFByZWNpc2lvbiBUcmFpbmluZyB0aGF0IHVzZXMgYm90aCBmbG9hdDMyIGFuZCBmbG9hdDE2IGRhdGEgdHlwZXMgdG8gb3B0aW1pemUgbWVtb3J5IHVzYWdlIGFuZCBzcGVlZC4gWW91ciBjbGFzcyBzaG91bGQgaGF2ZSBhbiBgX19pbml0X18oc2VsZiwgbG9zc19zY2FsZT0xMDI0LjApYCBtZXRob2QgdG8gaW5pdGlhbGl6ZSB3aXRoIGxvc3Mgc2NhbGluZyBmYWN0b3IuIEltcGxlbWVudCBgZm9yd2FyZChzZWxmLCB3ZWlnaHRzLCBpbnB1dHMsIHRhcmdldHMpYCB0byBwZXJmb3JtIGZvcndhcmQgcGFzcyB3aXRoIGZsb2F0MTYgY29tcHV0YXRpb24gYW5kIHJldHVybiBNZWFuIFNxdWFyZWQgRXJyb3IgKE1TRSkgbG9zcyAoc2NhbGVkKSBpbiBmbG9hdDMyLCBhbmQgYGJhY2t3YXJkKHNlbGYsIGdyYWRpZW50cylgIHRvIHVuc2NhbGUgZ3JhZGllbnRzIGFuZCBjaGVjayBmb3Igb3ZlcmZsb3cuIFVzZSBmbG9hdDE2IGZvciBjb21wdXRhdGlvbnMgYnV0IGZsb2F0MzIgZm9yIGdyYWRpZW50IGFjY3VtdWxhdGlvbi4gUmV0dXJuIGdyYWRpZW50cyBhcyBmbG9hdDMyIGFuZCBzZXQgdGhlbSB0byB6ZXJvIGlmIG92ZXJmbG93IGlzIGRldGVjdGVkLiBPbmx5IHVzZSBOdW1QeS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    class MixedPrecision:\n    def __init__(self, self, loss_scale=1024.0):\n        # Initialize loss scaling factor\n        pass\n\n    def forward(self, self, weights, inputs, targets):\n        # Perform forward pass with float16, return scaled loss as float32\n        pass\n\n    def backward(self, self, gradients):\n        # Unscale gradients and check for overflow, return as float32\n        pass",
    "example_input": "import numpy as np\nmp = MixedPrecision(loss_scale=1024.0)\nweights = np.array([0.5, -0.3], dtype=np.float32)\ninputs = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\ntargets = np.array([1.0, 0.0], dtype=np.float32)\nloss = mp.forward(weights, inputs, targets)\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Loss dtype: {type(loss).__name__}\")\ngrads = np.array([512.0, -256.0], dtype=np.float32)\nresult = mp.backward(grads)\nprint(f\"Gradients: {result}\")\nprint(f\"Grad dtype: {result.dtype}\")",
    "example_output": "Loss: 665.0000\nLoss dtype: float\nGradients: [0.5 -0.25]\nGrad dtype: float32",
    "example_reasoning": "Forward pass converts inputs to float16, computes loss, then scales and returns as Python float (float32). Backward converts gradients to float32 and unscales. Final gradients must be float32 type.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    class MixedPrecision:\n    def __init__(self, self, loss_scale=1024.0):\n        self.loss_scale = loss_scale\n\n    def forward(self, self, weights, inputs, targets):\n        # Convert ALL inputs to float16 for computation (regardless of input dtype)\n        weights_fp16 = weights.astype(np.float16)\n        inputs_fp16 = inputs.astype(np.float16)\n        targets_fp16 = targets.astype(np.float16)\n\n        # Simple forward pass: linear model + MSE loss\n        predictions = np.dot(inputs_fp16, weights_fp16)\n        loss = np.mean((targets_fp16 - predictions) ** 2)\n\n        # Scale loss and convert back to float32 (Python float)\n        scaled_loss = float(loss) * self.loss_scale\n        return scaled_loss\n\n    def backward(self, self, gradients):\n        # Convert gradients to float32 for precision (regardless of input dtype)\n        gradients_fp32 = gradients.astype(np.float32)\n\n        # Check for overflow (NaN or Inf)\n        overflow = np.any(np.isnan(gradients_fp32)) or np.any(np.isinf(gradients_fp32))\n\n        if overflow:\n            # Return zero gradients if overflow detected (must be float32)\n            return np.zeros_like(gradients_fp32, dtype=np.float32)\n\n        # Unscale gradients (ensure result is float32)\n        unscaled_gradients = gradients_fp32 / self.loss_scale\n        return unscaled_gradients.astype(np.float32)\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nmp = MixedPrecision(loss_scale=1024.0)\nweights = np.array([0.5, -0.3], dtype=np.float32)\ninputs = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\ntargets = np.array([1.0, 0.0], dtype=np.float32)\nloss = mp.forward(weights, inputs, targets)\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Loss dtype: {type(loss).__name__}\")"
        ],
        "expected_output": "Loss: 665.0000\nLoss dtype: float"
      },
      {
        "test": [
          "import numpy as np\nmp = MixedPrecision(loss_scale=1024.0)\ngrads = np.array([512.0, -256.0], dtype=np.float32)\nresult = mp.backward(grads)\nprint(f\"Gradients: {result}\")\nprint(f\"Grad dtype: {result.dtype}\")"
        ],
        "expected_output": "Gradients: [ 0.5  -0.25]\nGrad dtype: float32"
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 161,
    "title": "Exponential Weighted Average of Rewards",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "R2l2ZW4gYW4gaW5pdGlhbCB2YWx1ZSAkUV8xJCwgYSBsaXN0IG9mICRrJCBvYnNlcnZlZCByZXdhcmRzICRSXzEsIFJfMiwgXGxkb3RzLCBSX2skLCBhbmQgYSBzdGVwIHNpemUgJFxhbHBoYSQsIGltcGxlbWVudCBhIGZ1bmN0aW9uIHRvIGNvbXB1dGUgdGhlIGV4cG9uZW50aWFsbHkgd2VpZ2h0ZWQgYXZlcmFnZSBhczoKCiQkKDEtXGFscGhhKV5rIFFfMSArIFxzdW1fe2k9MX1eayBcYWxwaGEgKDEtXGFscGhhKV57ay1pfSBSX2kkJAoKVGhpcyB3ZWlnaHRpbmcgZ2l2ZXMgbW9yZSBpbXBvcnRhbmNlIHRvIHJlY2VudCByZXdhcmRzLCB3aGlsZSB0aGUgaW5mbHVlbmNlIG9mIHRoZSBpbml0aWFsIGVzdGltYXRlICRRXzEkIGRlY2F5cyBvdmVyIHRpbWUuIERvICoqbm90KiogdXNlIHJ1bm5pbmcvaW5jcmVtZW50YWwgdXBkYXRlczsgaW5zdGVhZCwgY29tcHV0ZSBkaXJlY3RseSBmcm9tIHRoZSBmb3JtdWxhLiAoVGhpcyBpcyBjYWxsZWQgdGhlICpleHBvbmVudGlhbCByZWNlbmN5LXdlaWdodGVkIGF2ZXJhZ2UqLik=",
    "starter_code": "\n\nclass Solution:\n    def exp_weighted_average(self, Q1, rewards, alpha):\n        \"\"\"\n        Q1: float, initial estimate\n        rewards: list or array of rewards, R_1 to R_k\n        alpha: float, step size (0 < alpha <= 1)\n        Returns: float, exponentially weighted average after k rewards\n        \"\"\"\n        # Your code here\n        pass\n",
    "example_input": "Q1 = 2.0\nrewards = [5.0, 9.0]\nalpha = 0.3\nresult = exp_weighted_average(Q1, rewards, alpha)\nprint(round(result, 4))",
    "example_output": "5.003",
    "example_reasoning": "Here, k=2, so the result is: (1-0.3)^2*2.0 + 0.3*(1-0.3)^1*5.0 + 0.3*(1-0.3)^0*9.0 = 0.49*2.0 + 0.21*5.0 + 0.3*9.0 = 0.98 + 1.05 + 2.7 = 4.73 (actually, should be 0.49*2+0.3*0.7*5+0.3*9 = 0.98+1.05+2.7=4.73)",
    "learn_content": "IyMjIEV4cG9uZW50aWFsIFJlY2VuY3ktV2VpZ2h0ZWQgQXZlcmFnZQoKV2hlbiB0aGUgZW52aXJvbm1lbnQgaXMgbm9uc3RhdGlvbmFyeSwgaXQgaXMgYmV0dGVyIHRvIGdpdmUgbW9yZSB3ZWlnaHQgdG8gcmVjZW50IHJld2FyZHMuIFRoZSBmb3JtdWxhICQkKDEtXGFscGhhKV5rIFFfMSArIFxzdW1fe2k9MX1eayBcYWxwaGEgKDEtXGFscGhhKV57ay1pfSBSX2kkJCBjb21wdXRlcyB0aGUgZXhwZWN0ZWQgdmFsdWUgYnkgZXhwb25lbnRpYWxseSBkZWNheWluZyB0aGUgaW5mbHVlbmNlIG9mIG9sZCByZXdhcmRzIGFuZCB0aGUgaW5pdGlhbCBlc3RpbWF0ZS4gVGhlIHBhcmFtZXRlciAkXGFscGhhJCBjb250cm9scyBob3cgcXVpY2tseSBvbGQgaW5mb3JtYXRpb24gaXMgZm9yZ290dGVuOiBoaWdoZXIgJFxhbHBoYSQgZ2l2ZXMgbW9yZSB3ZWlnaHQgdG8gbmV3IHJld2FyZHMu",
    "solution_code": "\n\nclass Solution:\n    def exp_weighted_average(self, Q1, rewards, alpha):\n        k = len(rewards)\n        value = (1 - alpha) ** k * Q1\n        for i, Ri in enumerate(rewards):\n            value += alpha * (1 - alpha) ** (k - i - 1) * Ri\n        return value",
    "test_cases": [
      {
        "test": [
          "Q1 = 10.0\nrewards = [4.0, 7.0, 13.0]\nalpha = 0.5\nprint(round(exp_weighted_average(Q1, rewards, alpha), 4))"
        ],
        "expected_output": 10.0
      },
      {
        "test": [
          "Q1 = 0.0\nrewards = [1.0, 1.0, 1.0, 1.0]\nalpha = 0.1\nprint(round(exp_weighted_average(Q1, rewards, alpha), 4))"
        ],
        "expected_output": 0.3439
      }
    ],
    "function_name": "exp_weighted_average"
  },
  {
    "id": 162,
    "title": "Upper Confidence Bound (UCB) Action Selection",
    "difficulty": "easy",
    "category": "reinforcement learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBVcHBlciBDb25maWRlbmNlIEJvdW5kIChVQ0IpIGFjdGlvbiBzZWxlY3Rpb24gc3RyYXRlZ3kgZm9yIHRoZSBtdWx0aS1hcm1lZCBiYW5kaXQgcHJvYmxlbS4gV3JpdGUgYSBmdW5jdGlvbiB0aGF0LCBnaXZlbiB0aGUgY3VycmVudCBudW1iZXIgb2YgdGltZXMgZWFjaCBhY3Rpb24gaGFzIGJlZW4gc2VsZWN0ZWQsIHRoZSBhdmVyYWdlIHJld2FyZHMgZm9yIGVhY2ggYWN0aW9uLCBhbmQgdGhlIGN1cnJlbnQgdGltZXN0ZXAgdCwgcmV0dXJucyB0aGUgYWN0aW9uIHRvIHNlbGVjdCBhY2NvcmRpbmcgdG8gdGhlIFVDQjEgZm9ybXVsYS4gVXNlIG9ubHkgTnVtUHku",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def ucb_action(self, counts, values, t, c):\n        \"\"\"\n        Choose an action using the UCB1 formula.\n        Args:\n          counts (np.ndarray): Number of times each action has been chosen\n          values (np.ndarray): Average reward of each action\n          t (int): Current timestep (starts from 1)\n          c (float): Exploration coefficient\n        Returns:\n          int: Index of action to select\n        \"\"\"\n        # TODO: Implement the UCB action selection\n        pass",
    "example_input": "import numpy as np\ncounts = np.array([1, 1, 1, 1])\nvalues = np.array([1.0, 2.0, 1.5, 0.5])\nt = 4\nc = 2.0\nprint(ucb_action(counts, values, t, c))",
    "example_output": "1",
    "example_reasoning": "At t=4, each action has been tried once, but action 1 has the highest average reward (2.0) and the same confidence bound as the others, so it is chosen.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def ucb_action(self, counts, values, t, c):\n        counts = np.array(counts)\n        values = np.array(values)\n        ucb = values + c * np.sqrt(np.log(t) / (counts + 1e-8))\n        return int(np.argmax(ucb))",
    "test_cases": [
      {
        "test": [
          "import numpy as np\ncounts = np.array([1, 1, 1, 1])  # Each action tried once\nvalues = np.array([1.0, 2.0, 1.5, 0.5])\nt = 4\nc = 2.0\nprint(ucb_action(counts, values, t, c))"
        ],
        "expected_output": 1
      },
      {
        "test": [
          "import numpy as np\ncounts = np.array([10, 10, 1, 10])\nvalues = np.array([0.7, 0.6, 2.0, 0.8])\nt = 31\nc = 1.0\nprint(ucb_action(counts, values, t, c))"
        ],
        "expected_output": 2
      }
    ],
    "function_name": "ucb_action"
  },
  {
    "id": 163,
    "title": "Gradient Bandit Action Selection",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "SW1wbGVtZW50IHRoZSBncmFkaWVudCBiYW5kaXQgYWxnb3JpdGhtIGZvciBhY3Rpb24gc2VsZWN0aW9uIGluIGEgbXVsdGktYXJtZWQgYmFuZGl0IHNldHRpbmcuIFdyaXRlIGEgY2xhc3MgR3JhZGllbnRCYW5kaXQgdGhhdCBtYWludGFpbnMgYSBzZXQgb2YgYWN0aW9uIHByZWZlcmVuY2VzIGFuZCB1cGRhdGVzIHRoZW0gYWZ0ZXIgZWFjaCByZXdhcmQuIFRoZSBjbGFzcyBzaG91bGQgcHJvdmlkZSBhIG1ldGhvZCBgc2VsZWN0X2FjdGlvbigpYCB0byBzYW1wbGUgYW4gYWN0aW9uIHVzaW5nIHRoZSBzb2Z0bWF4IG9mIHByZWZlcmVuY2VzLCBhbmQgYSBtZXRob2QgYHVwZGF0ZShhY3Rpb24sIHJld2FyZClgIHRvIHVwZGF0ZSBwcmVmZXJlbmNlcyB1c2luZyB0aGUgZ3JhZGllbnQgYXNjZW50IHVwZGF0ZSBydWxlLiBVc2Ugb25seSBOdW1QeS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    class GradientBandit:\n    def __init__(self, self, num_actions, alpha=0.1):\n        \"\"\"\n        num_actions (int): Number of possible actions\n        alpha (float): Step size for preference updates\n        \"\"\"\n        self.num_actions = num_actions\n        self.alpha = alpha\n        self.preferences = np.zeros(num_actions)\n        self.avg_reward = 0.0\n        self.time = 0\n    def softmax(self):\n        # Compute softmax probabilities from preferences\n        pass\n    def select_action(self):\n        # Sample an action according to the softmax distribution\n        pass\n    def update(self, self, action, reward):\n        # Update action preferences using the gradient ascent update\n        pass",
    "example_input": "import numpy as np\ngb = GradientBandit(num_actions=3, alpha=0.1)\na = gb.select_action()\ngb.update(a, reward=1.0)\nprobs = gb.softmax()\nprint(np.round(probs, 2).tolist())",
    "example_output": "[0.32, 0.34, 0.34]",
    "example_reasoning": "After a positive reward, the selected action's preference is increased, boosting its softmax probability.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    class GradientBandit:\n    def __init__(self, self, num_actions, alpha=0.1):\n        self.num_actions = num_actions\n        self.alpha = alpha\n        self.preferences = np.zeros(num_actions)\n        self.avg_reward = 0.0\n        self.time = 0\n    def softmax(self):\n        exp_prefs = np.exp(self.preferences - np.max(self.preferences))\n        return exp_prefs / np.sum(exp_prefs)\n    def select_action(self):\n        probs = self.softmax()\n        return int(np.random.choice(self.num_actions, p=probs))\n    def update(self, self, action, reward):\n        self.time += 1\n        self.avg_reward += (reward - self.avg_reward) / self.time\n        probs = self.softmax()\n        for a in range(self.num_actions):\n            if a == action:\n                self.preferences[a] += self.alpha * (reward - self.avg_reward) * (1 - probs[a])\n            else:\n                self.preferences[a] -= self.alpha * (reward - self.avg_reward) * probs[a]",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(0)\ngb = GradientBandit(num_actions=3, alpha=0.1)\na1 = gb.select_action()\ngb.update(a1, reward=1.0)\na2 = gb.select_action()\ngb.update(a2, reward=0.0)\nprint(a1, a2)"
        ],
        "expected_output": "1 2"
      },
      {
        "test": [
          "import numpy as np\ngb = GradientBandit(num_actions=2, alpha=0.2)\nacts = [gb.select_action() for _ in range(10)]\nprint(len(acts), set(acts).issubset({0,1}))"
        ],
        "expected_output": "10 True"
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 164,
    "title": "Gambler's Problem: Value Iteration",
    "difficulty": "hard",
    "category": "reinforcement learning",
    "problem_description": "QSBnYW1ibGVyIGhhcyB0aGUgY2hhbmNlIHRvIGJldCBvbiBhIHNlcXVlbmNlIG9mIGNvaW4gZmxpcHMuIElmIHRoZSBjb2luIGxhbmRzIGhlYWRzLCB0aGUgZ2FtYmxlciB3aW5zIHRoZSBhbW91bnQgc3Rha2VkOyBpZiB0YWlscywgdGhlIGdhbWJsZXIgbG9zZXMgdGhlIHN0YWtlLiBUaGUgZ29hbCBpcyB0byByZWFjaCAxMDAsIHN0YXJ0aW5nIGZyb20gYSBnaXZlbiBjYXBpdGFsICRzJCAod2l0aCAkMCA8IHMgPCAxMDAkKS4gVGhlIGdhbWUgZW5kcyB3aGVuIHRoZSBnYW1ibGVyIHJlYWNoZXMgJDAkIChiYW5rcnVwdGN5KSBvciAkMTAwJCAoZ29hbCkuIE9uIGVhY2ggZmxpcCwgdGhlIGdhbWJsZXIgY2FuIGJldCBhbnkgaW50ZWdlciBhbW91bnQgZnJvbSAkMSQgdXAgdG8gJFxtaW4ocywgMTAwLXMpJC4KClRoZSBwcm9iYWJpbGl0eSBvZiBoZWFkcyBpcyAkcF9oJCAoa25vd24pLiBSZXdhcmQgaXMgJCsxJCBpZiB0aGUgZ2FtYmxlciByZWFjaGVzICQxMDAkIGluIGEgdHJhbnNpdGlvbiwgJDAkIG90aGVyd2lzZS4KCioqWW91ciBUYXNrOioqCldyaXRlIGEgZnVuY3Rpb24gYGdhbWJsZXJfdmFsdWVfaXRlcmF0aW9uKHBoLCB0aGV0YT0xZS05KWAgdGhhdDoKLSBDb21wdXRlcyB0aGUgb3B0aW1hbCBzdGF0ZS12YWx1ZSBmdW5jdGlvbiAkVihzKSQgZm9yIGFsbCAkcyA9IDEsIC4uLiwgOTkkIHVzaW5nIHZhbHVlIGl0ZXJhdGlvbi4KLSBSZXR1cm5zIHRoZSBvcHRpbWFsIHBvbGljeSBhcyBhIG1hcHBpbmcgZnJvbSBzdGF0ZSAkcyQgdG8gdGhlIG9wdGltYWwgc3Rha2UgJGFeKiQgKGNhbiByZXR1cm4gYW55IG9wdGltYWwgc3Rha2UgaWYgdGhlcmUgYXJlIHRpZXMpLgoKKipJbnB1dHM6KioKLSBgcGhgOiBwcm9iYWJpbGl0eSBvZiBoZWFkcyAoZmxvYXQgYmV0d2VlbiAwIGFuZCAxKQotIGB0aGV0YWA6IHRocmVzaG9sZCBmb3IgdmFsdWUgaXRlcmF0aW9uIGNvbnZlcmdlbmNlIChkZWZhdWx0ICQxZS05JCkKCioqUmV0dXJuczoqKgotIGBWYDogYXJyYXkvbGlzdCBvZiBsZW5ndGggMTAxLCAkVltzXSQgaXMgdGhlIHZhbHVlIGZvciBzdGF0ZSAkcyQKLSBgcG9saWN5YDogYXJyYXkvbGlzdCBvZiBsZW5ndGggMTAxLCAkcG9saWN5W3NdJCBpcyB0aGUgb3B0aW1hbCBzdGFrZSBpbiBzdGF0ZSAkcyQgKDAgaWYgJHM9MCQgb3IgJHM9MTAwJCkK",
    "starter_code": "\n\nclass Solution:\n    def gambler_value_iteration(self, ph, theta=1e-9):\n        \"\"\"\n        Computes the optimal value function and policy for the Gambler's Problem.\n        Args:\n          ph: probability of heads\n          theta: convergence threshold\n        Returns:\n          V: list of values for all states 0..100\n          policy: list of optimal stakes for all states 0..100\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "ph = 0.4\nV, policy = gambler_value_iteration(ph)\nprint(round(V[50], 4))\nprint(policy[50])",
    "example_output": "0.0178\n1",
    "example_reasoning": "From state 50, the optimal action is to bet 1, with a probability of reaching 100 of about 0.0178 when ph=0.4.",
    "learn_content": "IyAqKkdhbWJsZXIncyBQcm9ibGVtIGFuZCBWYWx1ZSBJdGVyYXRpb24qKgoKSW4gdGhlIEdhbWJsZXIncyBQcm9ibGVtLCBhIGdhbWJsZXIgcmVwZWF0ZWRseSBiZXRzIG9uIGEgY29pbiBmbGlwIHdpdGggcHJvYmFiaWxpdHkgJHBfaCQgb2YgaGVhZHMuIFRoZSBnb2FsIGlzIHRvIHJlYWNoIDEwMCBzdGFydGluZyBmcm9tIHNvbWUgY2FwaXRhbCAkcyQuIEF0IGVhY2ggc3RhdGUsIHRoZSBnYW1ibGVyIGNob29zZXMgYSBzdGFrZSAkYSQgKGJldHdlZW4gJDEkIGFuZCAkXG1pbihzLCAxMDAtcykkKS4gSWYgaGVhZHMsIHRoZSBnYW1ibGVyIGdhaW5zICRhJDsgaWYgdGFpbHMsIGxvc2VzICRhJC4gVGhlIGdhbWUgZW5kcyBhdCAkMCQgb3IgJDEwMCQuCgpUaGUgb2JqZWN0aXZlIGlzIHRvIGZpbmQgdGhlIHBvbGljeSB0aGF0IG1heGltaXplcyB0aGUgcHJvYmFiaWxpdHkgb2YgcmVhY2hpbmcgMTAwICh0aGUgc3RhdGUtdmFsdWUgZnVuY3Rpb24gJFYocykkIGdpdmVzIHRoaXMgcHJvYmFiaWxpdHkpLiBUaGUgdmFsdWUgaXRlcmF0aW9uIHVwZGF0ZSBpczoKCiQkClYocykgPSBcbWF4X3thIFxpbiBcdGV4dHtBY3Rpb25zfShzKX0gXEJpZ1sgcF9oIChcdGV4dHtyZXdhcmR9ICsgVihzICsgYSkpICsgKDEtcF9oKVYocy1hKSBcQmlnXQokJAoKd2hlcmUgdGhlIHJld2FyZCBpcyAkKzEkIG9ubHkgaWYgJHMgKyBhID0gMTAwJC4KCkFmdGVyIGNvbnZlcmdlbmNlLCB0aGUgZ3JlZWR5IHBvbGljeSBjaG9vc2VzIHRoZSBzdGFrZSBtYXhpbWl6aW5nIHRoaXMgdmFsdWUuIFRoaXMgaXMgYSBjbGFzc2ljIGVwaXNvZGljIE1EUCwgYW5kIHRoZSBvcHRpbWFsIHBvbGljeSBtYXkgbm90IGJlIHVuaXF1ZSAodGllcyBhcmUgcG9zc2libGUpLg==",
    "solution_code": "\n\nclass Solution:\n    def gambler_value_iteration(self, ph, theta=1e-9):\n        # Initialize value function for states 0 to 100; terminal states 0 and 100 have value 0\n        V = [0.0] * 101\n        # Initialize policy array (bet amount for each state)\n        policy = [0] * 101\n\n        # Value iteration loop\n        while True:\n            delta = 0\n            # Iterate over non-terminal states (1 to 99)\n            for s in range(1, 100):\n                # Possible actions: bet between 1 and min(s, 100 - s)\n                actions = range(1, min(s, 100 - s) + 1)\n                action_returns = []\n                # Evaluate each action\n                for a in actions:\n                    win_state = s + a\n                    lose_state = s - a\n                    # Reward is 1 if transition reaches 100, else 0\n                    reward = 1.0 if win_state == 100 else 0.0\n                    # Expected value: ph * (reward + V[win]) + (1 - ph) * V[lose]\n                    ret = ph * (reward + V[win_state]) + (1 - ph) * V[lose_state]\n                    action_returns.append(ret)\n                # Update V[s] with the maximum expected value\n                max_value = max(action_returns)\n                delta = max(delta, abs(V[s] - max_value))\n                V[s] = max_value\n            # Check for convergence\n            if delta < theta:\n                break\n\n        # Extract optimal policy\n        for s in range(1, 100):\n            actions = range(1, min(s, 100 - s) + 1)\n            best_action = 0\n            best_return = -float('inf')\n            # Find action that maximizes expected value\n            for a in actions:\n                win_state = s + a\n                lose_state = s - a\n                reward = 1.0 if win_state == 100 else 0.0\n                ret = ph * (reward + V[win_state]) + (1 - ph) * V[lose_state]\n                if ret > best_return:\n                    best_return = ret\n                    best_action = a\n            policy[s] = best_action\n\n        return V, policy",
    "test_cases": [
      {
        "test": [
          "ph = 0.4\nV, policy = gambler_value_iteration(ph)\nprint(round(V[50], 4))\nprint(policy[50])"
        ],
        "expected_output": "0.4\n50"
      },
      {
        "test": [
          "ph = 0.25\nV, policy = gambler_value_iteration(ph)\nprint(round(V[80], 4))\nprint(policy[80])"
        ],
        "expected_output": "0.4534\n5"
      }
    ],
    "function_name": "gambler_value_iteration"
  },
  {
    "id": 165,
    "title": "Compute Discounted Return",
    "difficulty": "easy",
    "category": "reinforcement learning",
    "problem_description": "V3JpdGUgYSBmdW5jdGlvbiB0byBjb21wdXRlIHRoZSBkaXNjb3VudGVkIHJldHVybiBmb3IgYSBzZXF1ZW5jZSBvZiByZXdhcmRzIGdpdmVuIGEgZGlzY291bnQgZmFjdG9yIGdhbW1hLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBsaXN0IG9yIE51bVB5IGFycmF5IG9mIHJld2FyZHMgYW5kIGEgZGlzY291bnQgZmFjdG9yIGdhbW1hICgwIDwgZ2FtbWEgPD0gMSkgYW5kIHJldHVybiB0aGUgc2NhbGFyIHZhbHVlIG9mIHRoZSB0b3RhbCBkaXNjb3VudGVkIHJldHVybi4gT25seSB1c2UgTnVtUHku",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def discounted_return(self, rewards, gamma):\n        \"\"\"\n        Compute the total discounted return for a sequence of rewards.\n        Args:\n            rewards (list or np.ndarray): List or array of rewards [r_0, r_1, ..., r_T-1]\n            gamma (float): Discount factor (0 < gamma <= 1)\n        Returns:\n            float: Total discounted return\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "rewards = [1, 1, 1]\ngamma = 0.5\nprint(discounted_return(rewards, gamma))",
    "example_output": "1.75",
    "example_reasoning": "Discounted return: 1*1 + 1*0.5 + 1*0.25 = 1 + 0.5 + 0.25 = 1.75",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def discounted_return(self, rewards, gamma):\n        if not (0 < gamma <= 1):\n            raise ValueError('gamma must be in (0, 1]')\n        rewards = np.array(rewards, dtype=np.float32)\n        powers = gamma ** np.arange(len(rewards))\n        return float(np.sum(powers * rewards))",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nrewards = [1, 1, 1, 1, 1]\ngamma = 0.9\nprint(round(discounted_return(rewards, gamma), 4))"
        ],
        "expected_output": 4.0951
      },
      {
        "test": [
          "import numpy as np\nrewards = [5, 0, 0, 0, 2]\ngamma = 0.5\nprint(round(discounted_return(rewards, gamma), 4))"
        ],
        "expected_output": 5.125
      }
    ],
    "function_name": "discounted_return"
  },
  {
    "id": 166,
    "title": "Evaluate Expected Value in a Markov Decision Process",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "R2l2ZW4gYW4gTURQIChNYXJrb3YgRGVjaXNpb24gUHJvY2Vzcykgc3BlY2lmaWVkIGJ5IGEgc2V0IG9mIHN0YXRlcywgYWN0aW9ucywgdHJhbnNpdGlvbiBwcm9iYWJpbGl0aWVzLCBhbmQgcmV3YXJkcywgd3JpdGUgYSBmdW5jdGlvbiB0byBjb21wdXRlIHRoZSBleHBlY3RlZCB2YWx1ZSBvZiB0YWtpbmcgYSBwYXJ0aWN1bGFyIGFjdGlvbiBpbiBhIHBhcnRpY3VsYXIgc3RhdGUsIGFzc3VtaW5nIGEgZGlzY291bnQgZmFjdG9yIGdhbW1hLiBVc2Ugb25seSBOdW1QeS4=",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def expected_action_value(self, state, action, P, R, V, gamma):\n        \"\"\"\n        Computes the expected value of taking `action` in `state` for the given MDP.\n        Args:\n          state: int or str, the current state\n          action: str, the chosen action\n          P: dict of dicts, P[s][a][s'] = prob of next state s' if a in s\n          R: dict of dicts, R[s][a][s'] = reward for (s, a, s')\n          V: np.ndarray, the value function vector, indexed by state\n          gamma: float, discount factor\n        Returns:\n          float: expected value\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "states = [0, 1]\nactions = ['a', 'b']\nP = {0: {'a': {0: 0.5, 1: 0.5}, 'b': {0: 1.0}}, 1: {'a': {1: 1.0}, 'b': {0: 0.7, 1: 0.3}}}\nR = {0: {'a': {0: 5, 1: 10}, 'b': {0: 2}}, 1: {'a': {1: 0}, 'b': {0: -1, 1: 3}}}\ngamma = 0.9\nV = np.array([1.0, 2.0])\nprint(expected_action_value(0, 'a', P, R, V, gamma))",
    "example_output": "8.85",
    "example_reasoning": "For state 0 and action 'a':\n  - Next state 0: 0.5 * (5 + 0.9*1.0) = 0.5 * 5.9 = 2.95\n  - Next state 1: 0.5 * (10 + 0.9*2.0) = 0.5 * 11.8 = 5.9\n  Total: 2.95 + 5.9 = 8.85",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def expected_action_value(self, state, action, P, R, V, gamma):\n        \"\"\"\n        Computes the expected value of taking `action` in `state` for the given MDP.\n        Args:\n          state: int or str, the current state\n          action: str, the chosen action\n          P: dict of dicts, P[s][a][s'] = prob of next state s' if a in s\n          R: dict of dicts, R[s][a][s'] = reward for (s, a, s')\n          V: np.ndarray, the value function vector, indexed by state\n          gamma: float, discount factor\n        Returns:\n          float: expected value\n        \"\"\"\n        value = 0.0\n        for next_state, prob in P[state][action].items():\n            reward = R[state][action][next_state]\n            value += prob * (reward + gamma * V[next_state])\n        return value",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nstates = [0, 1]\nactions = ['a', 'b']\n# P[s][a][s'] = prob of s' if taking action a in s\nP = {\n    0: {'a': {0: 0.5, 1: 0.5}, 'b': {0: 1.0}},\n    1: {'a': {1: 1.0}, 'b': {0: 0.7, 1: 0.3}}\n}\nR = {\n    0: {'a': {0: 5, 1: 10}, 'b': {0: 2}},\n    1: {'a': {1: 0}, 'b': {0: -1, 1: 3}}\n}\ngamma = 0.9\nV = np.array([1.0, 2.0])\nprint(round(expected_action_value(0, 'a', P, R, V, gamma), 4))"
        ],
        "expected_output": 8.85
      },
      {
        "test": [
          "import numpy as np\nstates = [0, 1]\nactions = ['a', 'b']\nP = {\n    0: {'a': {0: 0.5, 1: 0.5}, 'b': {0: 1.0}},\n    1: {'a': {1: 1.0}, 'b': {0: 0.7, 1: 0.3}}\n}\nR = {\n    0: {'a': {0: 5, 1: 10}, 'b': {0: 2}},\n    1: {'a': {1: 0}, 'b': {0: -1, 1: 3}}\n}\ngamma = 0.5\nV = np.array([2.0, 0.0])\nprint(round(expected_action_value(1, 'b', P, R, V, gamma), 4))"
        ],
        "expected_output": 0.9
      }
    ],
    "function_name": "expected_action_value"
  },
  {
    "id": 167,
    "title": "Calculate the Discounted Return for a Given Trajectory",
    "difficulty": "easy",
    "category": "reinforcement learning",
    "problem_description": "V3JpdGUgYSBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIHRoZSBkaXNjb3VudGVkIHJldHVybiAkR190ID0gXHN1bV97az0wfV5caW5mdHkgXGdhbW1hXmsgUl97dCtrKzF9JCBmb3IgYSBnaXZlbiBzZXF1ZW5jZSBvZiByZXdhcmRzIGFuZCBkaXNjb3VudCBmYWN0b3IgZ2FtbWEuIFRoaXMgcXVhbnRpdHkgY29ycmVzcG9uZHMgdG8gdGhlIGV4cGVjdGVkIHJldHVybiAkdl9ccGkocykkIGluIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcsIGFzIGRlZmluZWQgYnkgdGhlIGVxdWF0aW9uIGluIHRoZSBpbWFnZS4gT25seSB1c2UgTnVtUHku",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def discounted_return(self, rewards, gamma):\n        \"\"\"\n        Compute the discounted return for a given list of rewards.\n        Args:\n          rewards (list of float): sequence of rewards R_{t+1}, R_{t+2}, ...\n          gamma (float): discount factor (0 <= gamma <= 1)\n        Returns:\n          float: discounted return G_t\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "rewards = [1, 2, 3, 4]\ngamma = 0.9\nprint(discounted_return(rewards, gamma))",
    "example_output": "8.146",
    "example_reasoning": "G = 1 + 0.9*2 + 0.9^2*3 + 0.9^3*4 = 1 + 1.8 + 2.43 + 2.916 = 8.146",
    "learn_content": "IyAqKkRpc2NvdW50ZWQgUmV0dXJuIGluIFJlaW5mb3JjZW1lbnQgTGVhcm5pbmcqKgoKVGhlICoqZGlzY291bnRlZCByZXR1cm4qKiAkR190JCBpcyB0aGUgdG90YWwgZXhwZWN0ZWQgc3VtIG9mIHJld2FyZHMgdGhhdCBhbiBhZ2VudCB3aWxsIHJlY2VpdmUgaW4gdGhlIGZ1dHVyZSwgd2hlcmUgZnV0dXJlIHJld2FyZHMgYXJlIG11bHRpcGxpZWQgYnkgYSBkaXNjb3VudCBmYWN0b3IgJFxnYW1tYSQgKCQwIFxsZXEgXGdhbW1hIFxsZXEgMSQpLiBUaGlzIGNvbmNlcHQgaXMgY2VudHJhbCBpbiByZWluZm9yY2VtZW50IGxlYXJuaW5nIGFuZCBpcyBjYXB0dXJlZCBieToKCiQkCkdfdCA9IFxzdW1fe2s9MH1eXGluZnR5IFxnYW1tYV5rIFJfe3QraysxfQokJAoKLSAkR190JDogRGlzY291bnRlZCByZXR1cm4gYXQgdGltZSAkdCQKLSAkUl97dCtrKzF9JDogUmV3YXJkIHJlY2VpdmVkICRrJCBzdGVwcyBpbnRvIHRoZSBmdXR1cmUKLSAkXGdhbW1hJDogRGlzY291bnQgZmFjdG9yIChjb250cm9scyBob3cgbXVjaCB0aGUgYWdlbnQgY2FyZXMgYWJvdXQgZnV0dXJlIHJld2FyZHMpCgojIyAqKldoeSBEaXNjb3VudD8qKgotIEVuc3VyZXMgdGhlIHN1bSBjb252ZXJnZXMgKGZvciBpbmZpbml0ZSBob3Jpem9ucykKLSBFbmNvdXJhZ2VzIHRoZSBhZ2VudCB0byBwcmVmZXIgaW1tZWRpYXRlIHJld2FyZHMgb3ZlciBkaXN0YW50IHJld2FyZHMgKGlmICRcZ2FtbWEgPCAxJCkKLSBNb2RlbHMgdW5jZXJ0YWludHkgb3IgdGltZSBwcmVmZXJlbmNlCgojIyAqKkNvbm5lY3Rpb24gdG8gVmFsdWUgRnVuY3Rpb24qKgpUaGUgc3RhdGUtdmFsdWUgZnVuY3Rpb24gJHZfXHBpKHMpJCB1bmRlciBwb2xpY3kgJFxwaSQgaXMgdGhlIGV4cGVjdGVkIGRpc2NvdW50ZWQgcmV0dXJuIHdoZW4gc3RhcnRpbmcgaW4gc3RhdGUgJHMkIGFuZCBmb2xsb3dpbmcgJFxwaSQ6CgokJAp2X1xwaShzKSA9IFxtYXRoYmJ7RX1fXHBpW0dfdCB8IFNfdCA9IHNdCiQkCgpUaGlzIG1lYW5zIHRoZSB2YWx1ZSBvZiBhIHN0YXRlIGlzIHRoZSBleHBlY3RlZCBzdW0gb2YgZGlzY291bnRlZCBmdXR1cmUgcmV3YXJkcyBzdGFydGluZyBmcm9tIHRoYXQgc3RhdGUuCgojIyAqKlN1bW1hcnkqKgotIERpc2NvdW50ZWQgcmV0dXJuIHF1YW50aWZpZXMgdGhlIGZ1dHVyZSByZXdhcmQgYW4gYWdlbnQgZXhwZWN0cywgYWNjb3VudGluZyBmb3IgdGltZSBhbmQgdW5jZXJ0YWludHkKLSBJdCBpcyBmb3VuZGF0aW9uYWwgZm9yIGRlZmluaW5nIG9wdGltYWxpdHkgYW5kIGxlYXJuaW5nIGluIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcKLSBJbXBsZW1lbnRpbmcgdGhlIHN1bSBpcyBvZnRlbiB0aGUgZmlyc3Qgc3RlcCB0byB1bmRlcnN0YW5kaW5nIHBvbGljeSBldmFsdWF0aW9u",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def discounted_return(self, rewards, gamma):\n        \"\"\"\n        Compute the discounted return for a given list of rewards.\n        Args:\n          rewards (list of float): sequence of rewards R_{t+1}, R_{t+2}, ...\n          gamma (float): discount factor (0 <= gamma <= 1)\n        Returns:\n          float: discounted return G_t\n        \"\"\"\n        G = 0.0\n        for k, r in enumerate(rewards):\n            G += (gamma ** k) * r\n        return G",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nrewards = [1, 2, 3, 4]\ngamma = 0.9\nprint(round(discounted_return(rewards, gamma), 4))"
        ],
        "expected_output": 8.146
      },
      {
        "test": [
          "import numpy as np\nrewards = [10, 0, 0, 0, 0]\ngamma = 0.5\nprint(round(discounted_return(rewards, gamma), 4))"
        ],
        "expected_output": 10.0
      }
    ],
    "function_name": "discounted_return"
  },
  {
    "id": 168,
    "title": "Calculate Conditional Probability from Data",
    "difficulty": "easy",
    "category": "probability",
    "problem_description": "R2l2ZW4gYSBkYXRhc2V0IG9mIG9ic2VydmF0aW9ucyBhcyBhIGxpc3Qgb2YgdHVwbGVzLCBlYWNoIHR1cGxlIGlzIG9mIHRoZSBmb3JtIChYLCBZKSwgd2hlcmUgWCBhbmQgWSBhcmUgY2F0ZWdvcmljYWwgdmFyaWFibGVzIChlLmcuLCBjb2xvciwgYW5pbWFsKS4gSW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgY29uZGl0aW9uYWwgcHJvYmFiaWxpdHkgJFAoWT15fFg9eCkkLCB0aGUgcHJvYmFiaWxpdHkgdGhhdCBZIGVxdWFscyBhIHNwZWNpZmljIHZhbHVlIHksIGdpdmVuIHRoYXQgWCBlcXVhbHMgYSBzcGVjaWZpYyB2YWx1ZSB4LgoKKipZb3VyIFRhc2s6KioKV3JpdGUgYSBmdW5jdGlvbiBgY29uZGl0aW9uYWxfcHJvYmFiaWxpdHkoZGF0YSwgeCwgeSlgIHRoYXQgdGFrZXMgYXMgaW5wdXQ6Ci0gYGRhdGFgOiBMaXN0IG9mIChYLCBZKSB0dXBsZXMuCi0gYHhgOiBWYWx1ZSBmb3IgdmFyaWFibGUgWCB0byBjb25kaXRpb24gb24uCi0gYHlgOiBWYWx1ZSBmb3IgdmFyaWFibGUgWSB3aG9zZSBwcm9iYWJpbGl0eSB5b3Ugd2FudCBnaXZlbiBYPXguCgpUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgcHJvYmFiaWxpdHkgJFAoWT15fFg9eCkkLiBSZXR1cm4gMC4wIGlmIHRoZXJlIGFyZSBubyBpbnN0YW5jZXMgd2hlcmUgWD14LgoKKipFeGFtcGxlOioqCmBgYHB5dGhvbgpkYXRhID0gWwogICAgKCdyZWQnLCAnY2F0JyksCiAgICAoJ2JsdWUnLCAnZG9nJyksCiAgICAoJ3JlZCcsICdkb2cnKSwKICAgICgncmVkJywgJ2NhdCcpLAogICAgKCdibHVlJywgJ2NhdCcpLAogICAgKCdyZWQnLCAnZG9nJykKXQpwcmludChjb25kaXRpb25hbF9wcm9iYWJpbGl0eShkYXRhLCAncmVkJywgJ2NhdCcpKSAgICMgMC41CnByaW50KGNvbmRpdGlvbmFsX3Byb2JhYmlsaXR5KGRhdGEsICdibHVlJywgJ2NhdCcpKSAgIyAwLjUKcHJpbnQoY29uZGl0aW9uYWxfcHJvYmFiaWxpdHkoZGF0YSwgJ2dyZWVuJywgJ2NhdCcpKSAjIDAuMApgYGAKCioqUmVhc29uaW5nOioqCi0gRm9yIFg9J3JlZCcsIHRoZXJlIGFyZSA0IGluc3RhbmNlczogMiB3aXRoIFk9J2NhdCcgYW5kIDIgd2l0aCBZPSdkb2cnLiBTbyAkUChZPWNhdHxYPXJlZCkgPSAyLzQgPSAwLjUkLgotIEZvciBYPSdibHVlJywgMiBpbnN0YW5jZXM6IDEgd2l0aCBZPSdjYXQnLCAxIHdpdGggWT0nZG9nJy4gU28gJFAoWT1jYXR8WD1ibHVlKSA9IDEvMiA9IDAuNSQuCi0gRm9yIFg9J2dyZWVuJywgdGhlcmUgYXJlIDAgaW5zdGFuY2VzLCBzbyBvdXRwdXQgMC4wLg==",
    "starter_code": "\n\nclass Solution:\n    def conditional_probability(self, data, x, y):\n        \"\"\"\n        Returns the probability P(Y=y|X=x) from list of (X, Y) pairs.\n        Args:\n          data: List of (X, Y) tuples\n          x: value of X to condition on\n          y: value of Y to check\n        Returns:\n          float: conditional probability, rounded to 4 decimal places\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "data = [('red', 'cat'), ('blue', 'dog'), ('red', 'dog'), ('red', 'cat'), ('blue', 'cat'), ('red', 'dog')]\nprint(conditional_probability(data, 'red', 'cat'))",
    "example_output": "0.5",
    "example_reasoning": "Out of 4 red observations, 2 are cat. So probability is 2/4 = 0.5.",
    "learn_content": "IyAqKkNvbmRpdGlvbmFsIFByb2JhYmlsaXR5KioKCkNvbmRpdGlvbmFsIHByb2JhYmlsaXR5IGlzIHRoZSBwcm9iYWJpbGl0eSBvZiBhbiBldmVudCAkQSQgb2NjdXJyaW5nIGdpdmVuIHRoYXQgYW5vdGhlciBldmVudCAkQiQgaGFzIGFscmVhZHkgb2NjdXJyZWQuIEl0IGlzIGRlbm90ZWQgYXMgJFAoQXxCKSQgYW5kIGlzIGRlZmluZWQgYnkgdGhlIGZvcm11bGE6CgokJApQKEF8QikgPSBcZnJhY3tQKEEgXGNhcCBCKX17UChCKX0KJCQKCkluIHByYWN0aWNlLCB5b3UgY2FuIGNvbXB1dGUgdGhpcyBmcm9tIGRhdGEgYnkgY291bnRpbmc6Ci0gVGhlIG51bWJlciBvZiB0aW1lcyBib3RoICRBJCBhbmQgJEIkIG9jY3VyIHRvZ2V0aGVyICh0aGUgbnVtZXJhdG9yKQotIFRoZSBudW1iZXIgb2YgdGltZXMgJEIkIG9jY3VycyAodGhlIGRlbm9taW5hdG9yKQoKRm9yIGNhdGVnb3JpY2FsIGRhdGEsICRQKFk9eXxYPXgpJCBjYW4gYmUgZXN0aW1hdGVkIGFzOgoKJCQKUChZPXl8WD14KSA9IFxmcmFje1x0ZXh0e2NvdW50IG9mIChYPXgsIFk9eSl9fXtcdGV4dHtjb3VudCBvZiBYPXh9fQokJAoKSWYgJFg9eCQgbmV2ZXIgb2NjdXJzIGluIHlvdXIgZGF0YSwgdGhlbiAkUChZPXl8WD14KSQgaXMgZGVmaW5lZCB0byBiZSAkMCQuCgpDb25kaXRpb25hbCBwcm9iYWJpbGl0eSBpcyBhIGZ1bmRhbWVudGFsIGNvbmNlcHQgZm9yIHByb2JhYmlsaXN0aWMgbW9kZWxpbmcsIEJheWVz4oCZIHRoZW9yZW0sIE5haXZlIEJheWVzIGNsYXNzaWZpZXJzLCBhbmQgbXVjaCBtb3JlLg==",
    "solution_code": "\n\nclass Solution:\n    def conditional_probability(self, data, x, y):\n        # Filter data for all instances where X = x\n        filtered = [item for item in data if item[0] == x]\n        if not filtered:\n            return 0.0\n        count_y = sum(1 for item in filtered if item[1] == y)\n        return round(count_y / len(filtered), 4)",
    "test_cases": [
      {
        "test": [
          "data = [('sunny', 'walk'), ('rainy', 'read'), ('sunny', 'run'), ('cloudy', 'read'), ('sunny', 'walk')]\nprint(conditional_probability(data, 'sunny', 'walk'))"
        ],
        "expected_output": 0.6667
      },
      {
        "test": [
          "data = [('male', 'yes'), ('female', 'no'), ('male', 'no'), ('male', 'yes'), ('female', 'yes')]\nprint(conditional_probability(data, 'male', 'yes'))"
        ],
        "expected_output": 0.6667
      }
    ],
    "function_name": "conditional_probability"
  },
  {
    "id": 169,
    "title": "Implement AdamW Optimizer Step",
    "difficulty": "medium",
    "category": "optimization",
    "problem_description": "SW1wbGVtZW50IGEgc2luZ2xlIHVwZGF0ZSBzdGVwIG9mIHRoZSBBZGFtVyBvcHRpbWl6ZXIgZm9yIGEgcGFyYW1ldGVyIHZlY3RvciAkdyQgYW5kIGl0cyBncmFkaWVudHMgJGckLiBBZGFtVyBpcyBhIHZhcmlhbnQgb2YgdGhlIEFkYW0gb3B0aW1pemVyIHRoYXQgZGVjb3VwbGVzIHdlaWdodCBkZWNheSBmcm9tIHRoZSBncmFkaWVudCB1cGRhdGUsIGxlYWRpbmcgdG8gYmV0dGVyIGdlbmVyYWxpemF0aW9uLgoKKipZb3VyIFRhc2s6KioKV3JpdGUgYSBmdW5jdGlvbiBgYWRhbXdfdXBkYXRlKHcsIGcsIG0sIHYsIHQsIGxyLCBiZXRhMSwgYmV0YTIsIGVwc2lsb24sIHdlaWdodF9kZWNheSlgIHRoYXQgcGVyZm9ybXMgb25lIHVwZGF0ZSBzdGVwIGZvciB0aGUgcGFyYW1ldGVyIHZlY3RvciAkdyQgd2l0aCBpdHMgZ3JhZGllbnQgJGckIHVzaW5nIEFkYW1XLiBUaGUgZnVuY3Rpb24gc2hvdWxkOgotIFVwZGF0ZSB0aGUgZmlyc3QgbW9tZW50ICRtJCBhbmQgc2Vjb25kIG1vbWVudCAkdiQgKG1vdmluZyBhdmVyYWdlcyBvZiBncmFkaWVudHMgYW5kIHNxdWFyZWQgZ3JhZGllbnRzKQotIEFwcGx5IGJpYXMgY29ycmVjdGlvbiBmb3IgJG0kIGFuZCAkdiQKLSBBcHBseSB0aGUgQWRhbVcgdXBkYXRlIHJ1bGUgKHdpdGggZGVjb3VwbGVkIHdlaWdodCBkZWNheSkKLSBSZXR1cm4gdGhlIHVwZGF0ZWQgcGFyYW1ldGVyIHZlY3RvciBhbmQgdGhlIG5ldyB2YWx1ZXMgb2YgJG0kIGFuZCAkdiQKCioqQXJndW1lbnRzOioqCi0gYHdgOiBOdW1QeSBhcnJheSwgY3VycmVudCBwYXJhbWV0ZXIgdmVjdG9yCi0gYGdgOiBOdW1QeSBhcnJheSwgZ3JhZGllbnQgdmVjdG9yIChzYW1lIHNoYXBlIGFzIGB3YCkKLSBgbWA6IE51bVB5IGFycmF5LCBmaXJzdCBtb21lbnQgdmVjdG9yIChzYW1lIHNoYXBlIGFzIGB3YCkKLSBgdmA6IE51bVB5IGFycmF5LCBzZWNvbmQgbW9tZW50IHZlY3RvciAoc2FtZSBzaGFwZSBhcyBgd2ApCi0gYHRgOiBJbnRlZ2VyLCBjdXJyZW50IHRpbWUgc3RlcCAoc3RhcnRpbmcgZnJvbSAxKQotIGBscmA6IExlYXJuaW5nIHJhdGUgKGZsb2F0KQotIGBiZXRhMWA6IERlY2F5IHJhdGUgZm9yIHRoZSBmaXJzdCBtb21lbnQgKGZsb2F0KQotIGBiZXRhMmA6IERlY2F5IHJhdGUgZm9yIHRoZSBzZWNvbmQgbW9tZW50IChmbG9hdCkKLSBgZXBzaWxvbmA6IFNtYWxsIGNvbnN0YW50IGZvciBudW1lcmljYWwgc3RhYmlsaXR5IChmbG9hdCkKLSBgd2VpZ2h0X2RlY2F5YDogV2VpZ2h0IGRlY2F5IGNvZWZmaWNpZW50IChmbG9hdCkKCg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def adamw_update(self, w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay):\n        \"\"\"\n        Perform one AdamW optimizer step.\n        Args:\n          w: parameter vector (np.ndarray)\n          g: gradient vector (np.ndarray)\n          m: first moment vector (np.ndarray)\n          v: second moment vector (np.ndarray)\n          t: integer, current time step\n          lr: float, learning rate\n          beta1: float, beta1 parameter\n          beta2: float, beta2 parameter\n          epsilon: float, small constant\n          weight_decay: float, weight decay coefficient\n        Returns:\n          w_new, m_new, v_new\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "import numpy as np\nw = np.array([1.0, 2.0])\ng = np.array([0.1, -0.2])\nm = np.zeros(2)\nv = np.zeros(2)\nt = 1\nlr = 0.01\nbeta1 = 0.9\nbeta2 = 0.999\nepsilon = 1e-8\nweight_decay = 0.1\nw_new, m_new, v_new = adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay)\nprint(np.round(w_new, 4))",
    "example_output": "[0.989 2.008]",
    "example_reasoning": "After applying AdamW update, the weights are moved in the negative gradient direction and decayed by 1%. The result is [0.989, 2.001].",
    "learn_content": "IyAqKkFkYW1XIE9wdGltaXplcioqCgpBZGFtVyBpcyBhIHZhcmlhbnQgb2YgdGhlIEFkYW0gb3B0aW1pemVyIHRoYXQgZGVjb3VwbGVzIHdlaWdodCBkZWNheSBmcm9tIHRoZSBncmFkaWVudC1iYXNlZCB1cGRhdGUsIHByb3ZpZGluZyBiZXR0ZXIgcmVndWxhcml6YXRpb24gYW5kIGdlbmVyYWxpemF0aW9uLCBlc3BlY2lhbGx5IGZvciB0cmFpbmluZyBkZWVwIG5ldXJhbCBuZXR3b3Jrcy4KCi0gKipBZGFtOioqIEFkYW0gY29tYmluZXMgbW9tZW50dW0gKG1vdmluZyBhdmVyYWdlIG9mIGdyYWRpZW50cykgYW5kIFJNU3Byb3AgKG1vdmluZyBhdmVyYWdlIG9mIHNxdWFyZWQgZ3JhZGllbnRzKSBmb3IgYWRhcHRpdmUgbGVhcm5pbmcgcmF0ZXMuCi0gKipXZWlnaHQgRGVjYXk6KiogUmVndWxhcml6ZXMgYnkgcGVuYWxpemluZyBsYXJnZSB3ZWlnaHRzLiBJbiBjbGFzc2ljIEFkYW0sIHdlaWdodCBkZWNheSBpcyBpbXBsZW1lbnRlZCBhcyAkTF8yJCByZWd1bGFyaXphdGlvbiAoYWRkZWQgdG8gdGhlIGdyYWRpZW50KS4gSW4gQWRhbVcsIHdlaWdodCBkZWNheSBpcyBhcHBsaWVkICoqZGlyZWN0bHkqKiB0byB0aGUgd2VpZ2h0cywgZGVjb3VwbGVkIGZyb20gdGhlIGdyYWRpZW50IHVwZGF0ZS4KCioqQWRhbVcgVXBkYXRlIFJ1bGU6KioKCjEuIFVwZGF0ZSB0aGUgbW92aW5nIGF2ZXJhZ2VzIChmaXJzdCBtb21lbnQgJG0kIGFuZCBzZWNvbmQgbW9tZW50ICR2JCk6CiAgICQkbV90ID0gXGJldGFfMSBtX3t0LTF9ICsgKDEgLSBcYmV0YV8xKSBnX3QkJAogICAkJHZfdCA9IFxiZXRhXzIgdl97dC0xfSArICgxIC0gXGJldGFfMikgZ190XjIkJAoyLiBCaWFzIGNvcnJlY3Rpb246CiAgICQkXGhhdHttfV90ID0gbV90IC8gKDEgLSBcYmV0YV8xXnQpJCQKICAgJCRcaGF0e3Z9X3QgPSB2X3QgLyAoMSAtIFxiZXRhXzJedCkkJAozLiBQYXJhbWV0ZXIgdXBkYXRlIHdpdGggZGVjb3VwbGVkIHdlaWdodCBkZWNheToKICAgJCR3IFxsZWZ0YXJyb3cgdyAtIFxldGEgXGNkb3QgXHRleHR7d2VpZ2h0XF9kZWNheX0gXGNkb3QgdyQkCiAgICQkdyBcbGVmdGFycm93IHcgLSBcZXRhIFxjZG90IFxmcmFje1xoYXR7bX1fdH17XHNxcnR7XGhhdHt2fV90fSArIFxlcHNpbG9ufSQkCgpUaGlzIGRlY291cGxlZCB1cGRhdGUgaXMgY3J1Y2lhbCBmb3IgY29ycmVjdCByZWd1bGFyaXphdGlvbiBpbiBtb2Rlcm4gZGVlcCBsZWFybmluZy4KCioqUmVmZXJlbmNlczoqKgotIFtMb3NoY2hpbG92ICYgSHV0dGVyLCAyMDE5LiBEZWNvdXBsZWQgV2VpZ2h0IERlY2F5IFJlZ3VsYXJpemF0aW9uIChBZGFtVyldKGh0dHBzOi8vYXJ4aXYub3JnL2Ficy8xNzExLjA1MTAxKQoKQWRhbVcgaXMgbm93IHRoZSBkZWZhdWx0IG9wdGltaXplciBpbiBtYW55IGRlZXAgbGVhcm5pbmcgZnJhbWV3b3JrcyAoZS5nLiwgUHlUb3JjaCku",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def adamw_update(self, w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay):\n        m_new = beta1 * m + (1 - beta1) * g\n        v_new = beta2 * v + (1 - beta2) * (g ** 2)\n        m_hat = m_new / (1 - beta1 ** t)\n        v_hat = v_new / (1 - beta2 ** t)\n        w = w - lr * weight_decay * w  # decoupled weight decay\n        w_new = w - lr * m_hat / (np.sqrt(v_hat) + epsilon)\n        return w_new, m_new, v_new",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nw = np.array([1.0, 2.0])\ng = np.array([0.1, -0.2])\nm = np.zeros(2)\nv = np.zeros(2)\nt = 1\nlr = 0.01\nbeta1 = 0.9\nbeta2 = 0.999\nepsilon = 1e-8\nweight_decay = 0.1\nw_new, m_new, v_new = adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay)\nprint(np.round(w_new, 4))"
        ],
        "expected_output": [
          0.989,
          2.008
        ]
      },
      {
        "test": [
          "import numpy as np\nw = np.array([1.5, -1.0])\ng = np.array([-0.3, 0.5])\nm = np.zeros(2)\nv = np.zeros(2)\nt = 1\nlr = 0.005\nbeta1 = 0.9\nbeta2 = 0.999\nepsilon = 1e-8\nweight_decay = 0.01\nw_new, m_new, v_new = adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay)\nprint(np.round(w_new, 4))"
        ],
        "expected_output": [
          1.5049,
          -1.0049
        ]
      }
    ],
    "function_name": "adamw_update"
  },
  {
    "id": 170,
    "title": "Muon Optimizer Step with Matrix Preconditioning",
    "difficulty": "medium",
    "category": "optimization",
    "problem_description": "VGhlIE11b24gb3B0aW1pemVyIGlzIGFuIGFsZ29yaXRobSB0aGF0IGNvbWJpbmVzIG1vbWVudHVtIHdpdGggYSBtYXRyaXggcHJlY29uZGl0aW9uaW5nIHN0ZXAgYmFzZWQgb24gdGhlIE5ld3Rvbi1TY2h1bHogaXRlcmF0aW9uLiBJbiB0aGlzIHRhc2ssIHlvdSB3aWxsIGltcGxlbWVudCBhIHNpbmdsZSBNdW9uIG9wdGltaXplciB1cGRhdGUgZm9yIGEgMkQgTnVtUHkgYXJyYXkgb2YgcGFyYW1ldGVycy4gWW91ciBpbXBsZW1lbnRhdGlvbiBzaG91bGQ6Ci0gVXBkYXRlIHRoZSBtb21lbnR1bSB1c2luZyB0aGUgY3VycmVudCBncmFkaWVudCBhbmQgcHJldmlvdXMgbW9tZW50dW0uCi0gQXBwbHkgdGhlIE5ld3Rvbi1TY2h1bHogbWF0cml4IGl0ZXJhdGlvbiAob3JkZXIgNSkgdG8gcHJlY29uZGl0aW9uIHRoZSB1cGRhdGUgZGlyZWN0aW9uLiBUaGlzIGludm9sdmVzIG5vcm1hbGl6aW5nIHRoZSB1cGRhdGUsIHBvc3NpYmx5IHRyYW5zcG9zaW5nIGZvciB3aWRlIG1hdHJpY2VzLCBhbmQgcnVubmluZyBhIGZpeGVkIG1hdHJpeCBpdGVyYXRpb24gZm9yIGEgbnVtYmVyIG9mIHN0ZXBzLgotIFVzZSBhIHNjYWxlIGZhY3RvciBiYXNlZCBvbiB0aGUgUk1TIG9wZXJhdG9yIG5vcm0gZm9yIHN0YWJpbGl0eS4KLSBVcGRhdGUgdGhlIHBhcmFtZXRlcnMgdXNpbmcgdGhlIHByZWNvbmRpdGlvbmVkIGRpcmVjdGlvbiwgbGVhcm5pbmcgcmF0ZSwgYW5kIHNjYWxlLgoKUmV0dXJuIGJvdGggdGhlIHVwZGF0ZWQgcGFyYW1ldGVyIG1hdHJpeCBhbmQgbW9tZW50dW0u",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def newton_schulz5(self, G, steps=5, eps=1e-7):\n        # Implement the Newton-Schulz iteration for matrix orthogonalization/preconditioning\n        pass\n\n    def muon_step(self, theta, B, grad, eta, mu, ns_steps=5, eps=1e-7):\n        \"\"\"\n        theta: np.ndarray, shape (M, N)\n        B: np.ndarray, shape (M, N)\n        grad: np.ndarray, shape (M, N)\n        eta: float (learning rate)\n        mu: float (momentum coefficient)\n        ns_steps: int (Newton-Schulz steps)\n        eps: float (numerical stability)\n        Returns: updated theta, updated B\n        \"\"\"\n        # Implement the Muon optimizer update step\n        pass\n",
    "example_input": "theta = np.eye(2)\nB = np.zeros((2,2))\ngrad = np.ones((2,2))\neta = 0.1\nmu = 0.9\ntheta_new, B_new = muon_step(theta, B, grad, eta, mu, ns_steps=2)\nprint(np.round(theta_new, 3))",
    "example_output": "[[ 0.944 -0.056] [-0.056 0.944]]",
    "example_reasoning": "After the momentum and Newton-Schulz preconditioning, the parameters are updated in the direction of the scaled matrix.",
    "learn_content": "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",
    "solution_code": "\n\nclass Solution:\n    def newton_schulz5(self, G, steps=5, eps=1e-7):\n        a, b, c = 3.4445, -4.7750, 2.0315\n        X = G.astype(np.float32)\n        X /= np.linalg.norm(X, 'fro') + eps\n        transposed = False\n        if X.shape[0] > X.shape[1]:\n            X = X.T\n            transposed = True\n        for _ in range(steps):\n            A = X @ X.T\n            X = a * X + (b * A + c * A @ A) @ X\n        if transposed:\n            X = X.T\n        return X\n\n    def muon_step(self, theta, B, grad, eta, mu, ns_steps=5, eps=1e-7):\n        B_new = mu * B + grad\n        O = newton_schulz5(B_new, steps=ns_steps)\n        scale = np.sqrt(np.prod(theta.shape)) / (np.linalg.norm(B_new, 'fro') + eps)\n        theta_new = theta - eta * scale * O\n        return theta_new, B_new",
    "test_cases": [
      {
        "test": [
          "theta = np.eye(2)\nB = np.zeros((2,2))\ngrad = np.ones((2,2))\neta = 0.1\nmu = 0.9\ntheta_new, B_new = muon_step(theta, B, grad, eta, mu, ns_steps=2)\nprint(np.round(theta_new, 3))"
        ],
        "expected_output": [
          [
            0.944,
            -0.056
          ],
          [
            -0.056,
            0.944
          ]
        ]
      },
      {
        "test": [
          "theta = np.eye(6)\nB = np.zeros((6,6))\ngrad = np.ones((6,6))\neta = 0.7\nmu = 0.95\ntheta_new, B_new = muon_step(theta, B, grad, eta, mu, ns_steps=2)\nprint(np.round(theta_new, 3))"
        ],
        "expected_output": [
          [
            0.87,
            -0.13,
            -0.13,
            -0.13,
            -0.13,
            -0.13
          ],
          [
            -0.13,
            0.87,
            -0.13,
            -0.13,
            -0.13,
            -0.13
          ],
          [
            -0.13,
            -0.13,
            0.87,
            -0.13,
            -0.13,
            -0.13
          ],
          [
            -0.13,
            -0.13,
            -0.13,
            0.87,
            -0.13,
            -0.13
          ],
          [
            -0.13,
            -0.13,
            -0.13,
            -0.13,
            0.87,
            -0.13
          ],
          [
            -0.13,
            -0.13,
            -0.13,
            -0.13,
            -0.13,
            0.87
          ]
        ]
      }
    ],
    "function_name": "newton_schulz5"
  },
  {
    "id": 171,
    "title": "Minimax Algorithm for Tic-Tac-Toe",
    "difficulty": "medium",
    "category": "game theory",
    "problem_description": "SW1wbGVtZW50IHRoZSBNaW5pbWF4IGFsZ29yaXRobSB0byBjaG9vc2UgdGhlIGJlc3QgbW92ZSBmb3IgYSBUaWMtVGFjLVRvZSBBSSBwbGF5ZXIuIEdpdmVuIGEgVGljLVRhYy1Ub2UgYm9hcmQgYW5kIGEgcGxheWVyIChlaXRoZXIgJ1gnIG9yICdPJyksIHdyaXRlIGEgZnVuY3Rpb24gdGhhdCByZXR1cm5zIHRoZSBvcHRpbWFsIG5leHQgbW92ZSBhcyBhIHR1cGxlIChyb3csIGNvbCkuIFlvdXIgZnVuY3Rpb24gc2hvdWxkIGFzc3VtZSBib3RoIHBsYXllcnMgcGxheSBvcHRpbWFsbHkgYW5kIHJldHVybiBhIG1vdmUgdGhhdCBtYXhpbWl6ZXMgdGhlIEFJJ3MgY2hhbmNlIG9mIHdpbm5pbmcgKG9yIG1pbmltaXplcyB0aGUgY2hhbmNlIG9mIGxvc2luZyBpZiBubyB3aW4gaXMgcG9zc2libGUpLiBUaGUgYm9hcmQgaXMgZ2l2ZW4gYXMgYSAzeDMgTnVtUHkgYXJyYXkgd2l0aCBlbnRyaWVzICdYJywgJ08nLCBvciAnJywgYW5kIHRoZSBwbGF5ZXIgdG8gbW92ZSBpcyBhIHN0cmluZyAoJ1gnIG9yICdPJykuIERvIG5vdCB1c2UgYW55IGV4dGVybmFsIGdhbWUgbGlicmFyaWVzLg==",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def is_winner(self, board, player):\n        # Check rows, columns, and diagonals\n        for i in range(3):\n            if all(board[i, j] == player for j in range(3)):\n                return True\n            if all(board[j, i] == player for j in range(3)):\n                return True\n        if all(board[i, i] == player for i in range(3)):\n            return True\n        if all(board[i, 2-i] == player for i in range(3)):\n            return True\n        return False\n\n    def is_full(self, board):\n        return not any(board[i, j] == '' for i in range(3) for j in range(3))\n\n    def get_available_moves(self, board):\n        return [(i, j) for i in range(3) for j in range(3) if board[i, j] == '']\n\n    def minimax_tictactoe(self, board: np.ndarray, player: str) -> tuple:\n        \"\"\"\n        Returns the optimal move (row, col) for the given player ('X' or 'O') on the current board using Minimax.\n        Args:\n            board: 3x3 NumPy array with entries 'X', 'O', or ''\n            player: 'X' or 'O'\n        Returns:\n            Tuple (row, col) for the optimal move\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "import numpy as np\nboard = np.array([['X', 'O', 'X'], ['', 'O', ''], ['', '', '']])\nmove = minimax_tictactoe(board, 'X')\nprint(move)",
    "example_output": "(2, 2)",
    "example_reasoning": "X can win by playing in (2, 2) if both sides play optimally.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def is_winner(self, board, player):\n        for i in range(3):\n            if all(board[i, j] == player for j in range(3)):\n                return True\n            if all(board[j, i] == player for j in range(3)):\n                return True\n        if all(board[i, i] == player for i in range(3)):\n            return True\n        if all(board[i, 2-i] == player for i in range(3)):\n            return True\n        return False\n\n    def is_full(self, board):\n        return not any(board[i, j] == '' for i in range(3) for j in range(3))\n\n    def get_available_moves(self, board):\n        return [(i, j) for i in range(3) for j in range(3) if board[i, j] == '']\n\n    def minimax(self, board, player, maximizing):\n        if is_winner(board, 'X'): return 1, None\n        if is_winner(board, 'O'): return -1, None\n        if is_full(board): return 0, None\n        moves = get_available_moves(board)\n        if maximizing:\n            best_score, best_move = -float('inf'), None\n            for i, j in moves:\n                board[i, j] = 'X'\n                score, _ = minimax(board, 'O', False)\n                board[i, j] = ''\n                if score > best_score:\n                    best_score, best_move = score, (i, j)\n            return best_score, best_move\n        else:\n            best_score, best_move = float('inf'), None\n            for i, j in moves:\n                board[i, j] = 'O'\n                score, _ = minimax(board, 'X', True)\n                board[i, j] = ''\n                if score < best_score:\n                    best_score, best_move = score, (i, j)\n            return best_score, best_move\n\n    def minimax_tictactoe(self, board: np.ndarray, player: str) -> tuple:\n        maximizing = (player == 'X')\n        _, move = minimax(board.copy(), player, maximizing)\n        return move",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nboard = np.array([['X', '', 'X'], ['', 'O', ''], ['', 'O', '']])\nmove = minimax_tictactoe(board, 'X')\nprint(move)"
        ],
        "expected_output": "(0, 1)"
      },
      {
        "test": [
          "import numpy as np\nboard = np.array([['X', 'O', 'X'], ['', 'O', ''], ['', '', '']])\nmove = minimax_tictactoe(board, 'X')\nprint(move)"
        ],
        "expected_output": "(2, 1)"
      }
    ],
    "function_name": "is_winner"
  },
  {
    "id": 172,
    "title": "Muon Optimizer Update with Newton-Schulz Iteration",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgc2luZ2xlIE11b24gb3B0aW1pemVyIHVwZGF0ZSB1c2luZyBvbmx5IE51bVB5LiBHaXZlbiBhIHBhcmFtZXRlciBtYXRyaXggdGhldGEsIGl0cyBncmFkaWVudCBHLCBtb21lbnR1bSBtdSwgbGVhcm5pbmcgcmF0ZSBsLCBhbmQgYSBudW1iZXIgb2YgTmV3dG9uLVNjaHVseiBzdGVwcywgcGVyZm9ybSBvbmUgdXBkYXRlIHN0ZXAgZm9sbG93aW5nIHRoZSBNdW9uIGFsZ29yaXRobSBiZWxvdy4gWW91IG11c3QgYWxzbyBpbXBsZW1lbnQgdGhlIE5ld3Rvbi1TY2h1bHo1IGZ1bmN0aW9uICg1IHF1aW50aWMgc3RlcHMpIGZvciBtYXRyaXggcHJlY29uZGl0aW9uaW5nKE5vIHNjYWxlIGZhY3RvciBpcyBuZWVkZWQgZm9yIHRoaXMgcXVlc3Rpb24p",
    "starter_code": "import numpy as np\n\nclass Solution:\n    def newtonschulz5(self, G: np.ndarray, steps=5, eps=1e-7) -> np.ndarray:\n        \"\"\"\n        Apply the Newton-Schulz (quintic) iteration for 5 steps to matrix G.\n        Args:\n            G: 2D NumPy array\n            steps: Number of iteration steps (default 5)\n            eps: Small constant for stability\n        Returns:\n            Matrix after Newton-Schulz iteration\n        \"\"\"\n        # Your code here\n        pass\n\n    def muon_update(self, theta: np.ndarray, grad: np.ndarray, B_prev: np.ndarray, mu: float, lr: float) -> tuple:\n        \"\"\"\n        Performs one Muon optimizer update (Algorithm 2). Returns the updated parameter, new B, and the preconditioned update.\n        Args:\n            theta: Parameter matrix (2D NumPy array)\n            grad: Gradient matrix (same shape)\n            B_prev: Previous B matrix (momentum)\n            mu: Momentum factor (0 <= mu < 1)\n            lr: Learning rate (step size)\n        Returns:\n            theta_new: Updated parameter matrix\n            B_new: Updated B matrix\n            O: Preconditioned update\n        \"\"\"\n        # Step 1: Compute B_t\n        # Step 2: Precondition with Newton-Schulz5\n        # Step 3: Update theta\n        # Your code here\n        pass",
    "example_input": "import numpy as np\nnp.random.seed(0)\ntheta = np.ones((3, 3))\ngrad = np.arange(1, 10).reshape(3, 3)\nB_prev = np.zeros((3, 3))\nmu = 0.9\nlr = 0.01\ntheta_new, B_new, O = muon_update(theta, grad, B_prev, mu, lr)\nprint(B_new.round(3))\nprint(O.round(4))",
    "example_output": "[[1. 2. 3.]\n [4. 5. 6.]\n [7. 8. 9.]]\n[[-0.0267  0.0855  0.1979]\n [ 0.1701  0.3036  0.4371]\n [ 0.3669  0.5216  0.6764]]",
    "example_reasoning": "The first B update is grad (since B_prev = 0). O is the Newton-Schulz5 result. \u03b8_new = \u03b8 - lr * O.",
    "learn_content": "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",
    "solution_code": "import numpy as np\n\nclass Solution:\n    def newtonschulz5(self, G: np.ndarray, steps=5, eps=1e-7) -> np.ndarray:\n        a, b, c = 3.4445, -4.7750, 2.0315\n        X = G.astype(np.float32)\n        orig_shape = X.shape\n        if X.shape[0] > X.shape[1]:\n            X = X.T\n        norm = np.sqrt(np.sum(X**2)) + eps\n        X = X / norm\n        for _ in range(steps):\n            A = X @ X.T\n            B = b * A + c * (A @ A)\n            X = a * X + B @ X\n        if orig_shape[0] > orig_shape[1]:\n            X = X.T\n        return X\n\n    def muon_update(self, theta: np.ndarray, grad: np.ndarray, B_prev: np.ndarray, mu: float, lr: float) -> tuple:\n        # Step 1: Update momentum\n        B = mu * B_prev + grad\n        # Step 2: Precondition with Newton-Schulz5\n        O = newtonschulz5(B)\n        # Step 3: Parameter update\n        theta_new = theta - lr * O\n        return theta_new, B, O",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(1)\ntheta = np.eye(3)\ngrad = np.ones((3, 3))\nB_prev = np.zeros((3, 3))\nmu = 0.8\nlr = 0.05\ntheta_new, B_new, O = muon_update(theta, grad, B_prev, mu, lr)\nprint(B_new.round(2))\nprint(O.round(4))"
        ],
        "expected_output": [
          [
            [
              1.0,
              1.0,
              1.0
            ],
            [
              1.0,
              1.0,
              1.0
            ],
            [
              1.0,
              1.0,
              1.0
            ]
          ],
          [
            [
              0.2321,
              0.2321,
              0.2321
            ],
            [
              0.2321,
              0.2321,
              0.2321
            ],
            [
              0.2321,
              0.2321,
              0.2321
            ]
          ]
        ]
      },
      {
        "test": [
          "import numpy as np\nnp.random.seed(2)\ntheta = np.ones((2, 4)) * 2\ngrad = np.arange(1, 9).reshape(2, 4)\nB_prev = np.zeros((2, 4))\nmu = 0.7\nlr = 0.02\ntheta_new, B_new, O = muon_update(theta, grad, B_prev, mu, lr)\nprint(B_new.round(2))\nprint(O.round(4))"
        ],
        "expected_output": [
          [
            [
              1.0,
              2.0,
              3.0,
              4.0
            ],
            [
              5.0,
              6.0,
              7.0,
              8.0
            ]
          ],
          [
            [
              -0.4198,
              -0.1,
              0.2198,
              0.5396
            ],
            [
              0.4367,
              0.3761,
              0.3155,
              0.2549
            ]
          ]
        ]
      }
    ],
    "function_name": "newtonschulz5"
  },
  {
    "id": 173,
    "title": "Implement K-Nearest Neighbors",
    "difficulty": "medium",
    "category": "machine learning",
    "problem_description": "R2l2ZW4gYSBsaXN0IG9mIHBvaW50cyBpbiBuLWRpbWVuc2lvbmFsIHNwYWNlIHJlcHJlc2VudGVkIGFzIHR1cGxlcyBhbmQgYSBxdWVyeSBwb2ludCwgaW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gZmluZCB0aGUgayBuZWFyZXN0IG5laWdoYm9ycyB0byB0aGUgcXVlcnkgcG9pbnQgdXNpbmcgRXVjbGlkZWFuIGRpc3RhbmNlLg==",
    "starter_code": "\n\nclass Solution:\n    def k_nearest_neighbors(self, points, query_point, k):\n        \"\"\"\n        Find k nearest neighbors to a query point\n\n        Args:\n            points: List of tuples representing points [(x1, y1), (x2, y2), ...]\n            query_point: Tuple representing query point (x, y)\n            k: Number of nearest neighbors to return\n\n        Returns:\n            List of k nearest neighbor points as tuples\n        \"\"\"\n        pass",
    "example_input": "points = [(1, 2), (3, 4), (1, 1), (5, 6), (2, 3)], query_point = (2, 2), k = 3",
    "example_output": "[(1, 2), (2, 3), (1, 1)]",
    "example_reasoning": "The 3 closest points to (2, 2) are (1, 2), (2, 3), and (1, 1), so the output is [(1, 2), (2, 3), (1, 1)].",
    "learn_content": "IyMgU29sdXRpb24gRXhwbGFuYXRpb24KClRoZSBrZXkgaW5zaWdodCBpcyB0byB1c2UgbnVtcHkncyB2ZWN0b3JpemVkIG9wZXJhdGlvbnMgdG8gZWZmaWNpZW50bHkgY2FsY3VsYXRlIGRpc3RhbmNlcyBiZXR3ZWVuIHRoZSBxdWVyeSBwb2ludCBhbmQgYWxsIGRhdGEgcG9pbnRzIHNpbXVsdGFuZW91c2x5LCB0aGVuIHNlbGVjdCB0aGUgayBzbWFsbGVzdCBkaXN0YW5jZXMuCgojIyBBbGdvcml0aG0gU3RlcHM6CgpDb252ZXJ0IHRvIG51bXB5IGFycmF5cyAtIFRyYW5zZm9ybSB0aGUgaW5wdXQgdHVwbGVzIGludG8gbnVtcHkgYXJyYXlzIGZvciB2ZWN0b3JpemVkIG9wZXJhdGlvbnMKQ2FsY3VsYXRlIGRpc3RhbmNlcyAtIFVzZSBicm9hZGNhc3RpbmcgdG8gY29tcHV0ZSBFdWNsaWRlYW4gZGlzdGFuY2UgZnJvbSBxdWVyeSBwb2ludCB0byBhbGwgcG9pbnRzIGF0IG9uY2UKRmluZCBrIG5lYXJlc3QgLSBVc2UgbnAuYXJnc29ydCgpIHRvIGdldCBpbmRpY2VzIG9mIHBvaW50cyBzb3J0ZWQgYnkgZGlzdGFuY2UsIHRoZW4gdGFrZSBmaXJzdCBrClJldHVybiBhcyB0dXBsZXMgLSBDb252ZXJ0IHRoZSBzZWxlY3RlZCBwb2ludHMgYmFjayB0byB0dXBsZSBmb3JtYXQ=",
    "solution_code": "\n\nclass Solution:\n    def k_nearest_neighbors(self, points, query_point, k):\n        \"\"\"\n        Find k nearest neighbors to a query point\n\n        Args:\n            points: List of tuples representing points [(x1, y1), (x2, y2), ...]\n            query_point: Tuple representing query point (x, y)\n            k: Number of nearest neighbors to return\n\n        Returns:\n            List of k nearest neighbor points as tuples\n        \"\"\"\n        if not points or k <= 0:\n            return []\n\n        if k > len(points):\n            k = len(points)\n\n        # Convert to numpy arrays for vectorized operations\n        points_array = np.array(points)\n        query_array = np.array(query_point)\n\n        # Calculate Euclidean distances using broadcasting\n        distances = np.sqrt(np.sum((points_array - query_array) ** 2, axis=1))\n\n        # Get indices of k smallest distances\n        k_nearest_indices = np.argsort(distances)[:k]\n\n        # Return the k nearest points as tuples\n        return [tuple(points_array[i]) for i in k_nearest_indices]",
    "test_cases": [
      {
        "test": [
          [
            [
              1,
              2
            ],
            [
              3,
              4
            ],
            [
              1,
              1
            ],
            [
              5,
              6
            ],
            [
              2,
              3
            ]
          ],
          [
            2,
            2
          ],
          3
        ],
        "expected_output": [
          [
            1,
            2
          ],
          [
            2,
            3
          ],
          [
            1,
            1
          ]
        ]
      },
      {
        "test": [
          [
            [
              0,
              0
            ],
            [
              1,
              1
            ],
            [
              2,
              2
            ],
            [
              3,
              3
            ]
          ],
          [
            1.5,
            1.5
          ],
          2
        ],
        "expected_output": [
          [
            1,
            1
          ],
          [
            2,
            2
          ]
        ]
      }
    ],
    "function_name": "k_nearest_neighbors"
  },
  {
    "id": 174,
    "title": "Train a Simple GAN on 1D Gaussian Data",
    "difficulty": "hard",
    "category": "deep learning",
    "problem_description": "SW4gdGhpcyB0YXNrLCB5b3Ugd2lsbCB0cmFpbiBhIEdlbmVyYXRpdmUgQWR2ZXJzYXJpYWwgTmV0d29yayAoR0FOKSB0byBsZWFybiBhIG9uZS1kaW1lbnNpb25hbCBHYXVzc2lhbiBkaXN0cmlidXRpb24uIFRoZSBHQU4gY29uc2lzdHMgb2YgYSBnZW5lcmF0b3IgdGhhdCBwcm9kdWNlcyBzYW1wbGVzIGZyb20gbGF0ZW50IG5vaXNlIGFuZCBhIGRpc2NyaW1pbmF0b3IgdGhhdCBlc3RpbWF0ZXMgdGhlIHByb2JhYmlsaXR5IHRoYXQgYSBnaXZlbiBzYW1wbGUgaXMgcmVhbC4gQm90aCBuZXR3b3JrcyBzaG91bGQgaGF2ZSBvbmUgaGlkZGVuIGxheWVyIHdpdGggUmVMVSBhY3RpdmF0aW9uIGluIHRoZSBoaWRkZW4gbGF5ZXIuIFRoZSBnZW5lcmF0b3LigJlzIG91dHB1dCBsYXllciBpcyBsaW5lYXIsIHdoaWxlIHRoZSBkaXNjcmltaW5hdG9yJ3Mgb3V0cHV0IGxheWVyIHVzZXMgYSBzaWdtb2lkIGFjdGl2YXRpb24uCgpZb3UgbXVzdCB0cmFpbiB0aGUgR0FOIHVzaW5nIHRoZSBzdGFuZGFyZCBub24tc2F0dXJhdGluZyBHQU4gbG9zcyBmb3IgdGhlIGdlbmVyYXRvciBhbmQgYmluYXJ5IGNyb3NzLWVudHJvcHkgbG9zcyBmb3IgdGhlIGRpc2NyaW1pbmF0b3IuIEluIHRoZSBOdW1QeSB2ZXJzaW9uLCBwYXJhbWV0ZXJzIHNob3VsZCBiZSB1cGRhdGVkIHVzaW5nIHZhbmlsbGEgZ3JhZGllbnQgZGVzY2VudC4gSW4gdGhlIFB5VG9yY2ggdmVyc2lvbiwgcGFyYW1ldGVycyBzaG91bGQgYmUgdXBkYXRlZCB1c2luZyBzdG9jaGFzdGljIGdyYWRpZW50IGRlc2NlbnQgKFNHRCkgd2l0aCB0aGUgc3BlY2lmaWVkIGxlYXJuaW5nIHJhdGUuIFRoZSB0cmFpbmluZyBsb29wIHNob3VsZCBhbHRlcm5hdGUgYmV0d2VlbiB1cGRhdGluZyB0aGUgZGlzY3JpbWluYXRvciBhbmQgdGhlIGdlbmVyYXRvciBlYWNoIGl0ZXJhdGlvbi4KCllvdXIgZnVuY3Rpb24gbXVzdCByZXR1cm4gdGhlIHRyYWluZWQgZ2VuZXJhdG9yIGZvcndhcmQgZnVuY3Rpb24gYGdlbl9mb3J3YXJkKHopYCwgd2hpY2ggcHJvZHVjZXMgZ2VuZXJhdGVkIHNhbXBsZXMgZ2l2ZW4gbGF0ZW50IG5vaXNlLg==",
    "starter_code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass Solution:\n    def train_gan(self, mean_real: float, std_real: float, latent_dim: int = 1, hidden_dim: int = 16, learning_rate: float = 0.001, epochs: int = 5000, batch_size: int = 128, seed: int = 42):\n        torch.manual_seed(seed)\n        # Your PyTorch implementation here\n        pass",
    "example_input": "gen_forward = train_gan(4.0, 1.25, epochs=1000, seed=42)\nz = np.random.normal(0, 1, (500, 1))\nx_gen, _, _ = gen_forward(z)\n(round(np.mean(x_gen), 4), round(np.std(x_gen), 4))",
    "example_output": "(0.0004, 0.0002)",
    "example_reasoning": "The test cases call `gen_forward` after training, sample 500 points, and then compute the mean and std.",
    "learn_content": "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",
    "solution_code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass Solution:\n    def train_gan(self, mean_real: float, std_real: float, latent_dim: int = 1, hidden_dim: int = 16, learning_rate: float = 0.001, epochs: int = 5000, batch_size: int = 128, seed: int = 42):\n        torch.manual_seed(seed)\n\n        class Generator(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.net = nn.Sequential(\n                    nn.Linear(latent_dim, hidden_dim),\n                    nn.ReLU(),\n                    nn.Linear(hidden_dim, 1)\n                )\n            def forward(self, self, z):\n                return self.net(z)\n\n        class Discriminator(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.net = nn.Sequential(\n                    nn.Linear(1, hidden_dim),\n                    nn.ReLU(),\n                    nn.Linear(hidden_dim, 1),\n                    nn.Sigmoid()\n                )\n            def forward(self, self, x):\n                return self.net(x)\n\n        G = Generator()\n        D = Discriminator()\n\n        # Use SGD as requested\n        opt_G = optim.SGD(G.parameters(), lr=learning_rate)\n        opt_D = optim.SGD(D.parameters(), lr=learning_rate)\n        criterion = nn.BCELoss()\n\n        for _ in range(epochs):\n            # Real and fake batches\n            real_data = torch.normal(mean_real, std_real, size=(batch_size, 1))\n            noise = torch.randn(batch_size, latent_dim)\n            fake_data = G(noise)\n\n            # ----- Discriminator step -----\n            opt_D.zero_grad()\n            pred_real = D(real_data)\n            pred_fake = D(fake_data.detach())\n            loss_real = criterion(pred_real, torch.ones_like(pred_real))\n            loss_fake = criterion(pred_fake, torch.zeros_like(pred_fake))\n            loss_D = loss_real + loss_fake\n            loss_D.backward()\n            opt_D.step()\n\n            # ----- Generator step -----\n            opt_G.zero_grad()\n            pred_fake = D(fake_data)\n            # non-saturating generator loss: maximize log D(G(z)) -> minimize -log D(G(z))\n            loss_G = criterion(pred_fake, torch.ones_like(pred_fake))\n            loss_G.backward()\n            opt_G.step()\n\n        return G.forward",
    "test_cases": [
      {
        "test": [
          "gen_forward = train_gan(4.0, 1.25, epochs=1000, seed=42)\nz = np.random.normal(0, 1, (500, 1))\nx_gen, _, _ = gen_forward(z)\nprint((round(np.mean(x_gen), 4), round(np.std(x_gen), 4)))"
        ],
        "expected_output": "(0.0004, 0.0002)"
      },
      {
        "test": [
          "gen_forward = train_gan(0.0, 1.0, epochs=500, seed=0)\nz = np.random.normal(0, 1, (300, 1))\nx_gen, _, _ = gen_forward(z)\nprint((round(np.mean(x_gen), 4), round(np.std(x_gen), 4)))"
        ],
        "expected_output": "(-0.0002, 0.0002)"
      }
    ],
    "function_name": "train_gan"
  },
  {
    "id": 175,
    "title": "Implement the SARSA Algorithm on policy",
    "difficulty": "medium",
    "category": "reinforcement learning",
    "problem_description": "SW1wbGVtZW50IHRoZSAqKlNBUlNBKiogYWxnb3JpdGhtIHRvIGVzdGltYXRlIFEtdmFsdWVzIGZvciBhIGdpdmVuIHNldCBvZiBkZXRlcm1pbmlzdGljIHRyYW5zaXRpb25zIHVzaW5nIGdyZWVkeSBhY3Rpb24gc2VsZWN0aW9uLgoKLSBBbGwgUS12YWx1ZXMgYXJlIGluaXRpYWxpemVkIHRvIHplcm8uCi0gRWFjaCBlcGlzb2RlIHN0YXJ0cyBmcm9tIGEgZ2l2ZW4gaW5pdGlhbCBzdGF0ZS4KLSBUaGUgZXBpc29kZSBlbmRzIHdoZW4gaXQgcmVhY2hlcyB0aGUgJHRlcm1pbmFsJCBzdGF0ZSBvciB3aGVuIHRoZSBudW1iZXIgb2Ygc3RlcHMgZXhjZWVkcyAkbWF4c3RlcHMkLgotIENoYW5nZXMgbWFkZSB0byBRLXZhbHVlcyBhcmUgcGVyc2lzdGVudCBhY3Jvc3MgZXBpc29kZXMu",
    "starter_code": "\n\nclass Solution:\n    def sarsa_update(self, transitions, initial_states, alpha, gamma, max_steps):\n        \"\"\"\n        Perform SARSA updates on the given environment transitions.\n        Args:\n            transitions (dict): mapping (state, action) -> (reward, next_state)\n            initial_states (list): list of starting states to simulate episodes from\n            alpha (float): learning rate\n            gamma (float): discount factor\n            max_steps (int): maximum steps allowed per episode\n        Returns:\n            dict: final Q-table as a dictionary {(state, action): value}\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "transitions = {\n    ('A', 'left'): (5.0, 'B'),\n    ('A', 'right'): (1.0, 'C'),\n    ('B', 'left'): (2.0, 'A'),\n    ('B', 'right'): (0.0, 'C'),\n    ('C', 'down'): (1.0, 'terminal')\n}\n\ninitial_states = ['A', 'B']\nalpha = 0.1\ngamma = 0.9\nmax_steps = 10\n\nQ = sarsa_update(transitions, initial_states, alpha, gamma, max_steps)\n\nfor k in sorted(transitions):\n    print(f\"Q{str(k):15} = {Q[k]:.4f}\")",
    "example_output": "Q('A', 'left')   = 4.2181\nQ('A', 'right')  = 0.0000\nQ('B', 'left')   = 2.7901\nQ('B', 'right')  = 0.0000",
    "example_reasoning": "The SARSA update rule is:\nQ(s,a) <- Q(s,a) + alpha * [reward + gamma * Q(s',a') - Q(s,a)]\n\nStarting from initial Q-values of 0, each episode updates Q-values based on the transitions.\n- Q('A', 'left') increases because it leads to B, and B can eventually return to A or C with additional rewards.\n- Q('A', 'right') and Q('B', 'right') remain 0.0 because the next state C leads directly to terminal with small reward.\n- Q('B', 'left') increases due to cyclic transitions giving non-zero rewards.",
    "learn_content": "IyBTQVJTQTogT24tUG9saWN5IFREIENvbnRyb2wKCioqR29hbCoqOiBFc3RpbWF0ZSB0aGUgYWN0aW9uLXZhbHVlIGZ1bmN0aW9uICRRXlxwaSBcYXBwcm94IHFeKiQgdXNpbmcgdGhlIFNBUlNBIGFsZ29yaXRobSAob24tcG9saWN5IFRlbXBvcmFsLURpZmZlcmVuY2UgY29udHJvbCkuCgojIyBQYXJhbWV0ZXJzCi0gU3RlcCBzaXplICRcYWxwaGEgXGluICgwLCAxXSQKLSBEaXNjb3VudCBmYWN0b3IgJFxnYW1tYSBcaW4gWzAsIDFdJAoKIyMgSW5pdGlhbGl6YXRpb24KLSBJbml0aWFsaXplICRRKHMsIGEpJCBhcmJpdHJhcmlseSBmb3IgYWxsICRzIFxpbiBcbWF0aGNhbHtTfV4rJCwgJGEgXGluIFxtYXRoY2Fse0F9KHMpJCAgCi0gU2V0ICRRKFx0ZXh0e3Rlcm1pbmFsfSwgXGNkb3QpID0gMCQKCiMjIEFsZ29yaXRobQoKKipMb29wIGZvciBlYWNoIGVwaXNvZGU6KioKMS4gSW5pdGlhbGl6ZSBzdGF0ZSAkUyQKMi4gQ2hvb3NlIGFjdGlvbiAkQSQgZnJvbSAkUyQgdXNpbmcgYSBwb2xpY3kgZGVyaXZlZCBmcm9tICRRJCAoZS5nLiwgZ3JlZWR5KQoKICAgICoqTG9vcCBmb3IgZWFjaCBzdGVwIG9mIHRoZSBlcGlzb2RlOioqCiAgICAxLiBUYWtlIGFjdGlvbiAkQSQsIG9ic2VydmUgcmV3YXJkICRSJCBhbmQgbmV4dCBzdGF0ZSAkUyckCiAgICAyLiBDaG9vc2UgbmV4dCBhY3Rpb24gJEEnJCBmcm9tICRTJyQgdXNpbmcgYSBwb2xpY3kgZGVyaXZlZCBmcm9tICRRJCAoZS5nLiwgZ3JlZWR5KQogICAgMy4gVXBkYXRlIHRoZSBhY3Rpb24tdmFsdWU6CiAgICAgICAkCiAgICAgICBRKFMsIEEpIFxsZWZ0YXJyb3cgUShTLCBBKSArIFxhbHBoYSBcbGVmdFsgUiArIFxnYW1tYSBRKFMnLCBBJykgLSBRKFMsIEEpIFxyaWdodF0KICAgICAgICQKICAgIDQuIFNldCAkUyBcbGVmdGFycm93IFMnJCwgJEEgXGxlZnRhcnJvdyBBJyQKICAgIDUuIFJlcGVhdCB1bnRpbCAkUyQgaXMgdGVybWluYWwKClRoaXMgYWxnb3JpdGhtIGNvbnRpbnVvdXNseSBpbXByb3ZlcyB0aGUgcG9saWN5IGFzIGl0IGV4cGxvcmVzIGFuZCBsZWFybnMgZnJvbSBpbnRlcmFjdGlvbiwgbWFraW5nIGl0IHN1aXRhYmxlIGZvciBvbmxpbmUgcmVpbmZvcmNlbWVudCBsZWFybmluZyBzY2VuYXJpb3Mu",
    "solution_code": "from collections import defaultdict\n\nclass Solution:\n    def select_greedy_action(self, state,action_after_state,Q):\n        actions = action_after_state.get(state,[])\n        if not actions:\n            return None\n        else:\n            max_q = max(Q[(state,a)] for a in actions)\n            action_required = []\n            for a in actions:\n                if(Q[(state,a)] == max_q):\n                    action_required.append(a)\n            final_action = min(action_required)\n            return final_action\n    def sarsa_update(self, transitions, initial_states, alpha, gamma, max_steps):\n        Q = defaultdict(float)\n        action_after_state = defaultdict(set)\n        for (s,a) in transitions:\n            action_after_state[s].add(a)\n\n        for state in initial_states:\n            steps = 0\n            s = state\n            action = select_greedy_action(s,action_after_state,Q)\n            while s!=\"terminal\" and steps<max_steps:\n                reward,next_state = transitions[(s,action)]\n                steps+=1\n                if next_state == \"terminal\":\n                    action_next = None\n                    next_q = 0\n                else:\n                    action_next = select_greedy_action(next_state,action_after_state,Q)\n                    next_q = Q[next_state,action_next]\n\n                Q[(s,action)] += alpha*(reward+ gamma*next_q- Q[(s,action)])\n                s = next_state\n                action = action_next\n\n        return Q",
    "test_cases": [
      {
        "test": [
          "transitions = {\n    ('A', 'go'): (1.0, 'B'),\n    ('B', 'go'): (2.0, 'C'),\n    ('C', 'go'): (3.0, 'terminal')\n}\ninitial_states = ['A']\nalpha = 0.5\ngamma = 0.9\nmax_steps = 5\nQ = sarsa_update(transitions, initial_states, alpha, gamma, max_steps)\nfor k in sorted(Q):\n    print(f\"Q{str(k):15} = {Q[k]:.4f}\")"
        ],
        "expected_output": "Q('A', 'go')     = 0.5000\nQ('B', 'go')     = 1.0000\nQ('C', 'go')     = 1.5000"
      },
      {
        "test": [
          "transitions = {\n    ('S1', 'left'): (2.0, 'S2'),\n    ('S1', 'right'): (1.0, 'S3'),\n    ('S2', 'left'): (0.5, 'terminal'),\n    ('S3', 'right'): (0.5, 'terminal')\n}\ninitial_states = ['S1', 'S2', 'S3']\nalpha = 0.1\ngamma = 0.8\nmax_steps = 10\nQ = sarsa_update(transitions, initial_states, alpha, gamma, max_steps)\nfor k in sorted(Q):\n    print(f\"Q{str(k):15} = {Q[k]:.4f}\")"
        ],
        "expected_output": "Q('S1', 'left')  = 0.2000\nQ('S1', 'right') = 0.0000\nQ('S2', 'left')  = 0.0950\nQ('S3', 'right') = 0.0500"
      }
    ],
    "function_name": "sarsa_update"
  },
  {
    "id": 176,
    "title": "Chi-square Probability Distribution",
    "difficulty": "medium",
    "category": "probability",
    "problem_description": "Q2FsY3VsYXRlIHRoZSBwcm9iYWJpbGl0eSBvZiBvY2N1cnJlbmNlIG9mIHggaW4gYSBDaGktU3F1YXJlZCBkaXN0cmlidXRpb24gd2l0aCB0aGUgZ2l2ZW4gc2FtcGxlIHZhbHVlICd4JyBhbmQgZGVncmVlcyBvZiBmcmVlZG9tICdrJy4=",
    "starter_code": "import math\n\nclass Solution:\n    def chi_square_probability(self, x, k):\n        \"\"\"\n        Calculate the probability density of x in a Chi-square distribution\n        with k degrees of freedom.\n        \"\"\"\n        # your code here\n        return round(probability, 3)",
    "example_input": "x = 2, k = 2",
    "example_output": "0.184",
    "example_reasoning": "To calculate the probability density for x in a chi-square distribution with k degrees of freedom, we use the formula:\n\nf(x; k) = (1 / (2^(k/2) * Gamma(k/2))) * x^((k/2) - 1) * exp(-x/2)\n\nSubstituting the values:\n\nf(2; 2) = (1 / (2^1 * Gamma(1))) * 2^(1 - 1) * exp(-2/2)\n= (1 / (2 * 1)) * 1 * exp(-1)\n= 0.5 * 0.3679 \u2248 0.184\n\nSo, the probability density at x = 2 for k = 2 is approximately 0.184.",
    "learn_content": "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",
    "solution_code": "import math\n\nclass Solution:\n    def chi_square_probability(self, x, k):\n        \"\"\"\n        Calculate the probability density of x in a Chi-square distribution\n        with k degrees of freedom.\n        \"\"\"\n        if x < 0 or k <= 0:\n            return 0.0\n\n        coeff = 1 / (math.pow(2, k / 2) * math.gamma(k / 2))\n        probability = coeff * math.pow(x, (k / 2) - 1) * math.exp(-x / 2)\n\n        return round(probability, 3)",
    "test_cases": [
      {
        "test": [
          2,
          2
        ],
        "expected_output": 0.184
      },
      {
        "test": [
          0,
          4
        ],
        "expected_output": 0.0
      }
    ],
    "function_name": "chi_square_probability"
  },
  {
    "id": 177,
    "title": "Implement MuonClip (qk-clip) for Stabilizing Attention",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "V3JpdGUgYSBmdW5jdGlvbiB0aGF0IGFwcGxpZXMgdGhlIE11b25DbGlwIHFrLWNsaXAgc3RlcCB0byBhdHRlbnRpb24gcHJvamVjdGlvbiB3ZWlnaHRzIFdfcSBhbmQgV19rLiBHaXZlbiBpbnB1dCBmZWF0dXJlcyB4LCBhIHRocmVzaG9sZCB0LCBhbmQgYSBtaXhpbmcgcGFyYW1ldGVyIGFscGhhLCB5b3VyIGZ1bmN0aW9uIG11c3Q6ICgxKSBjb21wdXRlIHRoZSBtYXhpbXVtIHByZS1jbGlwIFFLIHNjb3JlIChzY2FsZWQgYnkgMS9zcXJ0KGRfaGVhZCkpOyAoMikgaWYgdGhpcyBtYXggc2NvcmUgZXhjZWVkcyB0LCByZXNjYWxlIFdfcSBhbmQgV19rIGJ5IGV0YV5hbHBoYSBhbmQgZXRhXigxLWFscGhhKSwgd2hlcmUgZXRhID0gdCAvIG1heF9zY29yZTsgKDMpIHJldHVybiB0aGUgKHBvc3NpYmx5KSByZXNjYWxlZCB3ZWlnaHRzLCBhIGJvb2xlYW4gaW5kaWNhdGluZyB3aGV0aGVyIGNsaXBwaW5nIG9jY3VycmVkLCBhbmQgdGhlIHBvc3QtY2xpcCBtYXggc2NvcmUuIFJvdW5kIGFsbCByZXR1cm5lZCBmbG9hdGluZyB2YWx1ZXMgdG8gNCBkZWNpbWFscyBmb3IgcmVwcm9kdWNpYmlsaXR5IGluIHRlc3RzLg==",
    "starter_code": "import torch\n\nclass Solution:\n    @torch.no_grad()\n    def muonclip_qk_clip(self, W_q: torch.Tensor, W_k: torch.Tensor, x: torch.Tensor, t: float, alpha: float = 0.5, eps: float = 1e-7):\n    \t\"\"\"PyTorch version of MuonClip qk-clip (no grad). Returns (W_q_new, W_k_new, clipped, max_post).\"\"\"\n    \t# compute q, k, scores; rescale if needed; round outputs\n    \tpass\n",
    "example_input": "W_q = [[2.0, 0.0],[0.0, 2.0]]\nW_k = [[2.0, 0.0],[0.0, 2.0]]\nx   = [[[1.0, 0.0],[0.0, 1.0]]]  # (batch=1, seq=2, d_model=2)\nprint(muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.5))",
    "example_output": "([[1.1892, 0.0], [0.0, 1.1892]], [[1.1892, 0.0], [0.0, 1.1892]], True, 1.0)",
    "example_reasoning": "Pre-clip max score is 4/\u221a2 \u2248 2.8284 (> 1.0). With \u03b7 = 1/2.8284 and \u03b1 = 0.5, both W_q and W_k are scaled by \u221a\u03b7, bringing the max score to t.",
    "learn_content": "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",
    "solution_code": "import torch\n\nclass Solution:\n    @torch.no_grad()\n    def muonclip_qk_clip(self, W_q: torch.Tensor, W_k: torch.Tensor, x: torch.Tensor, t: float, alpha: float = 0.5, eps: float = 1e-7):\n    \tW_q = W_q.clone().float()\n    \tW_k = W_k.clone().float()\n    \tx   = x.clone().float()\n    \t# q, k, scores\n    \td_head = W_q.size(0)\n    \tq = torch.einsum('bsd,hd->bsh', x, W_q)\n    \tk = torch.einsum('bsd,hd->bsh', x, W_k)\n    \tscores = torch.einsum('bih,bjh->bij', q, k) / (d_head ** 0.5)\n    \tmax_pre = float(scores.max())\n    \tclipped = False\n    \tif max_pre > t:\n    \t\teta = t / (max_pre + eps)\n    \t\tW_q.mul_(eta ** alpha)\n    \t\tW_k.mul_(eta ** (1 - alpha))\n    \t\tclipped = True\n    \t# post-clip report\n    \tq_post = torch.einsum('bsd,hd->bsh', x, W_q)\n    \tk_post = torch.einsum('bsd,hd->bsh', x, W_k)\n    \tscores_post = torch.einsum('bih,bjh->bij', q_post, k_post) / (d_head ** 0.5)\n    \tmax_post = float(scores_post.max())\n    \t# round for tests\n    \tW_q = torch.round(W_q, decimals=4)\n    \tW_k = torch.round(W_k, decimals=4)\n    \tmax_post = round(max_post, 4)\n    \treturn W_q, W_k, clipped, max_post\n",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nW_q = torch.tensor([[2.0, 0.0],[0.0, 2.0]])\nW_k = torch.tensor([[2.0, 0.0],[0.0, 2.0]])\nx   = torch.tensor([[[1.0, 0.0],[0.0, 1.0]])]\nWq, Wk, c, m = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.5)\n# Round again in the test so users don't manage rounding\nprint(np.round(np.array(Wq),4).tolist(), np.round(np.array(Wk),4).tolist(), bool(c), round(m,4))"
        ],
        "expected_output": "[[1.1892, 0.0], [0.0, 1.1892]] [[1.1892, 0.0], [0.0, 1.1892]] True 1.0"
      },
      {
        "test": [
          "import numpy as np\nW_q = torch.tensor([[0.4, 0.0],[0.0, 0.4]])\nW_k = torch.tensor([[0.4, 0.0],[0.0, 0.4]])\nx   = torch.tensor([[[1.0, 0.0],[0.0, 1.0]])]\nWq, Wk, c, m = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.5)\nprint(np.round(np.array(Wq),4).tolist(), np.round(np.array(Wk),4).tolist(), bool(c), round(m,4))"
        ],
        "expected_output": "[[0.4, 0.0], [0.0, 0.4]] [[0.4, 0.0], [0.0, 0.4]] False 0.1131"
      }
    ],
    "function_name": "muonclip_qk_clip"
  },
  {
    "id": 178,
    "title": "Implement Position-wise Feed-Forward Block with Residual and Dropout",
    "difficulty": "medium",
    "category": "deep learning",
    "problem_description": "SW1wbGVtZW50IGEgUG9zaXRpb24td2lzZSBGZWVkLUZvcndhcmQgTmV0d29yayAoRkZOKSB3aXRoIHJlc2lkdWFsIGNvbm5lY3Rpb24gYW5kIGRyb3BvdXQsIGFzIHVzZWQgaW4gVHJhbnNmb3JtZXIgYXJjaGl0ZWN0dXJlcy4gVGhlIGJsb2NrIHNob3VsZCB0YWtlIGFuIGlucHV0IHZlY3RvciwgYXBwbHkgdHdvIGxpbmVhciB0cmFuc2Zvcm1hdGlvbnMgd2l0aCBhIFJlTFUgYWN0aXZhdGlvbiBpbiBiZXR3ZWVuLCB0aGVuIGFkZCBhIHJlc2lkdWFsIGNvbm5lY3Rpb24gZnJvbSB0aGUgaW5wdXQgYW5kIGFwcGx5IGRyb3BvdXQuIFJvdW5kIG91dHB1dHMgdG8gNCBkZWNpbWFsIHBsYWNlcyBmb3IgcmVwcm9kdWNpYmlsaXR5Lg==",
    "starter_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Solution:\n    class FFNBlock(nn.Module):\n    def __init__(self, self, d_model, d_hidden, dropout_p=0.1):\n        # Your code here\n    def forward(self, self, x):\n        # Your code here\n        pass",
    "example_input": "ffn([1.0, -1.0], W1=[[1.0,2.0],[3.0,4.0]], b1=[0.5, -0.5], W2=[[2.0,1.0],[0.5,1.0]], b2=[0.0, 0.5], dropout_p=0.0, seed=42)",
    "example_output": "[1.0, -0.5]",
    "example_reasoning": "First compute hidden = ReLU(W1*x + b1) = ReLU([1*1 + -1*2 + 0.5, 1*3 + -1*4 - 0.5]) = ReLU([-0.5, -1.5]) = [0,0]. Then output = W2*hidden + b2 = [0, 0.5]. Residual adds input [1,-1] => [1, -0.5]. Since dropout_p=0, nothing is dropped. Final output = [1.0, -0.5].",
    "learn_content": "IyMgUG9zaXRpb24td2lzZSBGZWVkLUZvcndhcmQgQmxvY2sgaW4gVHJhbnNmb3JtZXJzCgpJbiB0aGUgVHJhbnNmb3JtZXIgYXJjaGl0ZWN0dXJlLCBlYWNoIGVuY29kZXIgYW5kIGRlY29kZXIgbGF5ZXIgY29udGFpbnMgYSAqKnBvc2l0aW9uLXdpc2UgZmVlZC1mb3J3YXJkIG5ldHdvcmsgKEZGTikqKi4gSXQgb3BlcmF0ZXMgaW5kZXBlbmRlbnRseSBvbiBlYWNoIHBvc2l0aW9uIGluIHRoZSBzZXF1ZW5jZS4KClRoZSBGRk4gaXMgZGVmaW5lZCBhczoKCiQkRkZOKHgpID0gXHRleHR7RHJvcG91dH0oV18yKFx0ZXh0e1JlTFV9KFdfMXggKyBiXzEpKSArIGJfMikgKyB4JCQKCndoZXJlOgotICR4JDogSW5wdXQgdmVjdG9yCi0gJFdfMSwgYl8xJDogRmlyc3QgbGluZWFyIHRyYW5zZm9ybWF0aW9uIChleHBhbnNpb24pCi0gJFdfMiwgYl8yJDogU2Vjb25kIGxpbmVhciB0cmFuc2Zvcm1hdGlvbiAocHJvamVjdGlvbiBiYWNrIHRvIGlucHV0IGRpbWVuc2lvbikKLSBSZXNpZHVhbCBjb25uZWN0aW9uOiBBZGRzIHRoZSBpbnB1dCAkeCQgdG8gdGhlIHRyYW5zZm9ybWVkIG91dHB1dAotIERyb3BvdXQ6IFJhbmRvbWx5IHplcm9zIHNvbWUgZWxlbWVudHMgZHVyaW5nIHRyYWluaW5nIHRvIHByZXZlbnQgb3ZlcmZpdHRpbmcKCiMjIyBLZXkgQ29uY2VwdHMKMS4gKipSZXNpZHVhbCBDb25uZWN0aW9uOioqIEhlbHBzIHByZXZlbnQgdmFuaXNoaW5nIGdyYWRpZW50cyBieSBhbGxvd2luZyB0aGUgaW5wdXQgdG8gYnlwYXNzIHRyYW5zZm9ybWF0aW9ucy4KMi4gKipEcm9wb3V0OioqIEltcHJvdmVzIGdlbmVyYWxpemF0aW9uIGJ5IHByZXZlbnRpbmcgY28tYWRhcHRhdGlvbiBvZiBuZXVyb25zLgozLiAqKlBvc2l0aW9uLXdpc2U6KiogVGhlIHNhbWUgRkZOIGlzIGFwcGxpZWQgaW5kZXBlbmRlbnRseSB0byBlYWNoIHBvc2l0aW9uLgoKVGhpcyBtYWtlcyBGRk5zIGNydWNpYWwgZm9yIGNhcHR1cmluZyBub24tbGluZWFyIHJlcHJlc2VudGF0aW9ucyB3aXRoaW4gVHJhbnNmb3JtZXIgYmxvY2tzLg==",
    "solution_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Solution:\n    class FFNBlock(nn.Module):\n    def __init__(self, self, d_model, d_hidden, dropout_p=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_hidden)\n        self.linear2 = nn.Linear(d_hidden, d_model)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, self, x):\n        residual = x\n        out = F.relu(self.linear1(x))\n        out = self.linear2(out)\n        out = self.dropout(out)\n        out = out + residual\n        return torch.round(out * 10000) / 10000",
    "test_cases": [
      {
        "test": [
          "print(ffn(torch.tensor([1.0, -1.0]), torch.tensor([[1.0,2.0],[3.0,4.0]]), torch.tensor([0.5,-0.5]), torch.tensor([[2.0,1.0],[0.5,1.0]]), torch.tensor([0.0,0.5]), dropout_p=0.0, seed=42))"
        ],
        "expected_output": [
          1.0,
          -0.5
        ]
      },
      {
        "test": [
          "print(ffn(torch.tensor([0.5, 0.5]), torch.tensor([[1.0, -1.0],[2.0, 0.5]]), torch.tensor([0.0,0.0]), torch.tensor([[1.0,1.0],[0.5,2.0]]), torch.tensor([0.5,0.5]), dropout_p=0.0, seed=123))"
        ],
        "expected_output": [
          2.25,
          3.5
        ]
      }
    ],
    "function_name": "__init__"
  },
  {
    "id": 179,
    "title": "Expected Value and Variance of an n-Sided Die",
    "difficulty": "easy",
    "category": "probability",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgZXhwZWN0ZWQgdmFsdWUgYW5kIHZhcmlhbmNlIG9mIGEgZmFpciBuLXNpZGVkIGRpZSByb2xsLiBUaGUgZGllIGhhcyBmYWNlcyBudW1iZXJlZCAxIHRocm91Z2ggbiwgZWFjaCBlcXVhbGx5IGxpa2VseS4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gYSB0dXBsZSAoZXhwZWN0ZWRfdmFsdWUsIHZhcmlhbmNlKS4=",
    "starter_code": "import torch\n\nclass Solution:\n    def dice_statistics(self, n: int) -> tuple[float, float]:\n        \"\"\"\n        Compute the expected value and variance of a fair n-sided die roll using PyTorch.\n\n        Args:\n            n (int): Number of sides of the die\n\n        Returns:\n            tuple: (expected_value, variance)\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "dice_statistics(6)",
    "example_output": "(3.5, 2.9167)",
    "example_reasoning": "For n=6, the expected value is (6+1)/2 = 3.5 and the variance is (6^2-1)/12 = 35/12 \u2248 2.9167.",
    "learn_content": "IyMgRXhwZWN0ZWQgVmFsdWUgYW5kIFZhcmlhbmNlIG9mIGFuIG4tU2lkZWQgRGllCgpXaGVuIHJvbGxpbmcgYSBmYWlyIG4tc2lkZWQgZGllLCBlYWNoIG91dGNvbWUgKDEgdGhyb3VnaCBuKSBpcyBlcXVhbGx5IGxpa2VseSB3aXRoIHByb2JhYmlsaXR5ICRcZnJhY3sxfXtufSQuCgojIyMgRXhwZWN0ZWQgVmFsdWUKVGhlIGV4cGVjdGVkIHZhbHVlIG9mIGEgcmFuZG9tIHZhcmlhYmxlICRYJCBpcyBnaXZlbiBieToKCiQkCkVbWF0gPSBcc3VtX3tpPTF9XntufSB4X2kgUCh4X2kpCiQkCgpGb3IgYSBmYWlyIG4tc2lkZWQgZGllOgoKJCQKRVtYXSA9IFxmcmFjezErMiszK1xjZG90cytufXtufSA9IFxmcmFje24rMX17Mn0KJCQKCiMjIyBWYXJpYW5jZQpWYXJpYW5jZSBpcyBjb21wdXRlZCBhczoKCiQkClZhcihYKSA9IEVbWF4yXSAtIChFW1hdKV4yCiQkCgpGaXJzdCwgY29tcHV0ZSAkRVtYXjJdJDoKCiQkCkVbWF4yXSA9IFxmcmFjezFeMiArIDJeMiArIDNeMiArIFxjZG90cyArIG5eMn17bn0gPSBcZnJhY3sobisxKSgybisxKX17Nn0KJCQKClRodXM6CgokJApWYXIoWCkgPSBcZnJhY3sobisxKSgybisxKX17Nn0gLSBcbGVmdChcZnJhY3tuKzF9ezJ9XHJpZ2h0KV4yCiQkCgojIyMgS2V5IFJlc3VsdHMKLSBFeHBlY3RlZCBWYWx1ZTogJFxmcmFje24rMX17Mn0kCi0gVmFyaWFuY2U6ICRcZnJhY3tuXjIgLSAxfXsxMn0kCg==",
    "solution_code": "import torch\n\nclass Solution:\n    def dice_statistics(self, n: int) -> tuple[float, float]:\n        outcomes = torch.arange(1, n+1, dtype=torch.float)\n        expected_value = outcomes.mean()\n        expected_square = (outcomes**2).mean()\n        variance = expected_square - expected_value**2\n        return expected_value.item(), variance.item()",
    "test_cases": [
      {
        "test": [
          "import random\nrandom.seed(0)\nE,V = dice_statistics(6)\nprint((round(E,4), round(V,4)))"
        ],
        "expected_output": "(3.5, 2.9167)"
      },
      {
        "test": [
          "import random\nrandom.seed(42)\nE,V = dice_statistics(4)\nprint((round(E,4), round(V,4)))"
        ],
        "expected_output": "(2.5, 1.25)"
      }
    ],
    "function_name": "dice_statistics"
  },
  {
    "id": 180,
    "title": "Conditional Probability from Joint Distribution",
    "difficulty": "medium",
    "category": "probability",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgY29uZGl0aW9uYWwgcHJvYmFiaWxpdHkgUChBfEIpLCBnaXZlbiBhIGpvaW50IHByb2JhYmlsaXR5IGRpc3RyaWJ1dGlvbiBvdmVyIGV2ZW50cyBBIGFuZCBCLiBUaGUgZGlzdHJpYnV0aW9uIGlzIHByb3ZpZGVkIGFzIGEgZGljdGlvbmFyeSB3aXRoIGtleXMgKCdBJywnQicpLCAoJ0EnLCdgQicpLCAoJ2BBJywnQicpLCAoJ2BBJywnYEInKSwgd2hlcmUgdGhlIGJhY2t0aWNrIGAgZGVub3RlcyBsb2dpY2FsIE5PVC4=",
    "starter_code": "import torch\n\nclass Solution:\n    def conditional_probability(self, joint_distribution: dict) -> float:\n        \"\"\"\n        Compute conditional probability P(A|B) from a joint probability distribution using PyTorch.\n\n        Args:\n            joint_distribution (dict): keys ('A','B'), ('A','`B'), ('`A','B'), ('`A','`B')\n\n        Returns:\n            float: P(A|B)\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "conditional_probability({('A','B'):0.2, ('A','`B'):0.3, ('`A','B'):0.1, ('`A','`B'):0.4})",
    "example_output": "0.6667",
    "example_reasoning": "P(B)=0.2+0.1=0.3 and P(A\u2229B)=0.2, so P(A|B)=0.2/0.3=0.6667.",
    "learn_content": "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",
    "solution_code": "import torch\n\nclass Solution:\n    def conditional_probability(self, joint_distribution: dict) -> float:\n        p_ab = torch.tensor(joint_distribution[('A','B')], dtype=torch.float)\n        p_notA_b = torch.tensor(joint_distribution[('`A','B')], dtype=torch.float)\n        p_b = p_ab + p_notA_b\n        if float(p_b.item()) == 0.0:\n            raise ValueError(\"P(B) cannot be zero.\")\n        return (p_ab / p_b).item()",
    "test_cases": [
      {
        "test": [
          "import random\nrandom.seed(0)\nprint(round(conditional_probability({('A','B'):0.2, ('A','`B'):0.3, ('`A','B'):0.1, ('`A','`B'):0.4}), 4))"
        ],
        "expected_output": 0.6667
      },
      {
        "test": [
          "import random\nrandom.seed(42)\nprint(round(conditional_probability({('A','B'):0.1, ('A','`B'):0.2, ('`A','B'):0.4, ('`A','`B'):0.3}), 4))"
        ],
        "expected_output": 0.2
      }
    ],
    "function_name": "conditional_probability"
  },
  {
    "id": 181,
    "title": "Central Limit Theorem Simulation",
    "difficulty": "medium",
    "category": "probability",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBzaW11bGF0ZXMgdGhlIENlbnRyYWwgTGltaXQgVGhlb3JlbSAoQ0xUKS4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGFzIGlucHV0IHRoZSBudW1iZXIgb2Ygc2FtcGxlcywgdGhlIHNhbXBsZSBzaXplLCBhbmQgdGhlIGRpc3RyaWJ1dGlvbiB0eXBlICgndW5pZm9ybScgb3IgJ2V4cG9uZW50aWFsJykuIEl0IHNob3VsZCByZXR1cm4gdGhlIG1lYW4gb2YgdGhlIHNhbXBsZSBtZWFucy4=",
    "starter_code": "import torch\n\nclass Solution:\n    def simulate_clt(self, num_samples: int, sample_size: int, distribution: str = 'uniform') -> torch.Tensor:\n        \"\"\"\n        Simulate the Central Limit Theorem (CLT) using PyTorch tensors.\n        \"\"\"\n        # Your code here\n        pass",
    "example_input": "simulate_clt(num_samples=1000, sample_size=30, distribution='uniform')",
    "example_output": "0.4996",
    "example_reasoning": "We draw 1000 samples of size 30 each from a uniform(0,1) distribution. The mean of sample means is close to 0.5 due to the Central Limit Theorem.",
    "learn_content": "IyMgQ2VudHJhbCBMaW1pdCBUaGVvcmVtIChDTFQpCgpUaGUgQ2VudHJhbCBMaW1pdCBUaGVvcmVtIGlzIG9uZSBvZiB0aGUgbW9zdCBmdW5kYW1lbnRhbCByZXN1bHRzIGluIHByb2JhYmlsaXR5IGFuZCBzdGF0aXN0aWNzLiBJdCBzdGF0ZXMgdGhhdCBpZiB5b3UgdGFrZSByZXBlYXRlZCByYW5kb20gc2FtcGxlcyBvZiBzaXplICRuJCBmcm9tIGFueSBkaXN0cmlidXRpb24gd2l0aCBmaW5pdGUgbWVhbiAkXG11JCBhbmQgdmFyaWFuY2UgJFxzaWdtYV4yJCwgdGhlbiBhcyAkbiQgZ3Jvd3MgbGFyZ2UsIHRoZSBkaXN0cmlidXRpb24gb2YgdGhlIHNhbXBsZSBtZWFucyB3aWxsIGFwcHJveGltYXRlIGEgKipub3JtYWwgZGlzdHJpYnV0aW9uKiouCgojIyMgS2V5IEZvcm11bGEKCi0gSWYgJFhfMSwgWF8yLCBcbGRvdHMsIFhfbiQgYXJlIGkuaS5kLiByYW5kb20gdmFyaWFibGVzIHdpdGggbWVhbiAkXG11JCBhbmQgdmFyaWFuY2UgJFxzaWdtYV4yJDoKCiQkClxiYXJ7WH0gPSBcZnJhY3sxfXtufSBcc3VtX3tpPTF9Xm4gWF9pCiQkCgpUaGVuOgoKJCQKXGZyYWN7XGJhcntYfSAtIFxtdX17XHNpZ21hL1xzcXJ0e259fSBceHJpZ2h0YXJyb3d7ZH0gTigwLDEpCiQkCgojIyMgSW50ZXJwcmV0YXRpb24KLSBSZWdhcmRsZXNzIG9mIHRoZSBvcmlnaW5hbCBkaXN0cmlidXRpb24gKHVuaWZvcm0sIGV4cG9uZW50aWFsLCBldGMuKSwgdGhlIGRpc3RyaWJ1dGlvbiBvZiB0aGUgc2FtcGxlIG1lYW4gYXBwcm9hY2hlcyBub3JtYWxpdHkgYXMgc2FtcGxlIHNpemUgaW5jcmVhc2VzLgoKIyMjIEFwcGxpY2F0aW9ucwotIEh5cG90aGVzaXMgdGVzdGluZwotIENvbmZpZGVuY2UgaW50ZXJ2YWxzCi0gQXBwcm94aW1hdGlvbiBvZiB1bmtub3duIGRpc3RyaWJ1dGlvbnM=",
    "solution_code": "import torch\n\nclass Solution:\n    def simulate_clt(self, num_samples: int, sample_size: int, distribution: str = 'uniform') -> torch.Tensor:\n        sample_means = []\n        for _ in range(num_samples):\n            if distribution == 'uniform':\n                data = torch.rand(sample_size)\n            elif distribution == 'exponential':\n                data = torch.distributions.Exponential(1.0).sample((sample_size,))\n            else:\n                raise ValueError('Unsupported distribution')\n            sample_means.append(torch.mean(data).item())\n        return torch.tensor(float(torch.tensor(sample_means).mean()))",
    "test_cases": [
      {
        "test": [
          "import numpy as np\nnp.random.seed(42)\nprint(round(simulate_clt(1000, 30, 'uniform'), 4))"
        ],
        "expected_output": 0.4996
      },
      {
        "test": [
          "import numpy as np\nnp.random.seed(123)\nprint(round(simulate_clt(1000, 50, 'uniform'), 4))"
        ],
        "expected_output": 0.5015
      }
    ],
    "function_name": "simulate_clt"
  },
  {
    "id": 182,
    "title": "Central Limit Theorem Simulation",
    "difficulty": "medium",
    "category": "probability",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gZGVtb25zdHJhdGUgdGhlIENlbnRyYWwgTGltaXQgVGhlb3JlbSAoQ0xUKS4gWW91ciBmdW5jdGlvbiBzaG91bGQgZHJhdyBtYW55IHNhbXBsZXMgZnJvbSBhIGNob3NlbiBkaXN0cmlidXRpb24sIGNvbXB1dGUgdGhlaXIgc2FtcGxlIG1lYW5zLCBzdGFuZGFyZGl6ZSB0aGVtIHRvIFotc2NvcmVzLCBhbmQgcmV0dXJuIHRoZSBtZWFuIGFuZCBzdGFuZGFyZCBkZXZpYXRpb24gb2YgdGhlc2Ugc3RhbmRhcmRpemVkIHZhbHVlcy4gVGhlIGltcGxlbWVudGF0aW9uIHNob3VsZCBoYW5kbGUgYXQgbGVhc3QgdGhlIGZvbGxvd2luZyBkaXN0cmlidXRpb25zOiBVbmlmb3JtKDAsMSksIEV4cG9uZW50aWFsKHNjYWxlPTEuMCksIGFuZCBCZXJub3VsbGkocD0wLjMpLg==",
    "starter_code": "import torch\n\nclass Solution:\n    def simulate_clt(self, distribution: str, n: int, runs: int = 10000, seed: int = 42) -> dict:\n        \"\"\"\n        Simulate the Central Limit Theorem using PyTorch tensors.\n        \"\"\"\n        torch.manual_seed(seed)\n        # Your implementation here\n        pass",
    "example_input": "simulate_clt('exponential', n=30, runs=10000, seed=42)",
    "example_output": "{'mean': -0.003, 'std': 1.002}",
    "example_reasoning": "Drawing 10,000 samples of size 30 from an exponential distribution and standardizing the means produces a distribution very close to $N(0,1)$.",
    "learn_content": "IyMgQ2VudHJhbCBMaW1pdCBUaGVvcmVtIChDTFQpCgpUaGUgQ2VudHJhbCBMaW1pdCBUaGVvcmVtIHN0YXRlcyB0aGF0IGlmIHdlIHRha2UgbWFueSBpbmRlcGVuZGVudCBzYW1wbGVzIG9mIHNpemUgJG4kIGZyb20gYW55IGRpc3RyaWJ1dGlvbiB3aXRoIGZpbml0ZSBtZWFuICRcbXUkIGFuZCB2YXJpYW5jZSAkXHNpZ21hXjIkLCB0aGUgZGlzdHJpYnV0aW9uIG9mIHRoZSBzdGFuZGFyZGl6ZWQgc2FtcGxlIG1lYW4gYXBwcm9hY2hlcyBhIHN0YW5kYXJkIG5vcm1hbCBkaXN0cmlidXRpb24gYXMgJG4kIGJlY29tZXMgbGFyZ2UuCgojIyMgU2FtcGxlIE1lYW4KR2l2ZW4gYSBzYW1wbGUgJFhfMSwgWF8yLCBcZG90cywgWF9uJCBmcm9tIGEgZGlzdHJpYnV0aW9uIHdpdGggbWVhbiAkXG11JCBhbmQgdmFyaWFuY2UgJFxzaWdtYV4yJDoKJCQKXGJhcntYfSA9IFxmcmFjezF9e259IFxzdW1fe2k9MX1ebiBYX2kKJCQKCiMjIyBTdGFuZGFyZGl6ZWQgRm9ybQpUaGUgc3RhbmRhcmRpemVkIG1lYW4gaXM6CiQkClogPSBcZnJhY3tcYmFye1h9IC0gXG11fXtcc2lnbWEvXHNxcnR7bn19CiQkCgojIyMgQ0xUIFJlc3VsdApBcyAkbiBcdG8gXGluZnR5JDoKJCQKWiBcc2ltIE4oMCwgMSkKJCQKClRoaXMgcmVzdWx0IGhvbGRzIHJlZ2FyZGxlc3Mgb2YgdGhlIG9yaWdpbmFsIGRpc3RyaWJ1dGlvbiwgYXMgbG9uZyBhcyB2YXJpYW5jZSBpcyBmaW5pdGUuCg==",
    "solution_code": "import torch\nimport math\n\nclass Solution:\n    def simulate_clt(self, distribution: str, n: int, runs: int = 10000, seed: int = 42) -> dict:\n        torch.manual_seed(seed)\n        if distribution == 'uniform':\n            data = torch.rand(runs, n)\n            mu, sigma = 0.5, math.sqrt(1/12)\n        elif distribution == 'exponential':\n            data = torch.distributions.Exponential(1.0).sample((runs, n))\n            mu, sigma = 1.0, 1.0\n        elif distribution == 'bernoulli':\n            p = 0.3\n            data = torch.bernoulli(torch.full((runs, n), p))\n            mu, sigma = p, math.sqrt(p*(1-p))\n        else:\n            raise ValueError('Unsupported distribution')\n\n        xbar = data.mean(dim=1)\n        z = (xbar - mu) / (sigma / math.sqrt(n))\n        return {\"mean\": float(z.mean()), \"std\": float(z.std())}",
    "test_cases": [
      {
        "test": [
          "res = simulate_clt('uniform', n=5, runs=5000, seed=0)\nprint({'mean': round(res['mean'],3), 'std': round(res['std'],3)})"
        ],
        "expected_output": {
          "mean": -0.024,
          "std": 1.005
        }
      },
      {
        "test": [
          "res = simulate_clt('exponential', n=30, runs=10000, seed=42)\nprint({'mean': round(res['mean'],3), 'std': round(res['std'],3)})"
        ],
        "expected_output": {
          "mean": 0.006,
          "std": 1.009
        }
      }
    ],
    "function_name": "simulate_clt"
  },
  {
    "id": 183,
    "title": "Calculate Portfolio Variance",
    "difficulty": "easy",
    "category": "financial engineering",
    "problem_description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSB2YXJpYW5jZSBvZiBhIHBvcnRmb2xpbyBnaXZlbiBhIGNvdmFyaWFuY2UgbWF0cml4IG9mIGFzc2V0IHJldHVybnMgYW5kIGEgc2V0IG9mIHBvcnRmb2xpbyB3ZWlnaHRzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIGVuc3VyZSB0aGF0IHRoZSBpbnB1dHMgYXJlIHZhbGlkIGFuZCBjb21wYXRpYmxlLCBhbmQgcmV0dXJuIGEgc2luZ2xlIGZsb2F0aW5nLXBvaW50IG51bWJlciByZXByZXNlbnRpbmcgdGhlIHZhcmlhbmNlLg==",
    "starter_code": "import torch\n\nclass Solution:\n    def calculate_portfolio_variance(self, cov_matrix: list[list[float]], weights: list[float]) -> float:\n        \"\"\"\n        Calculate the variance of a portfolio using PyTorch.\n        \"\"\"\n        # Convert inputs to tensors\n        cov_matrix = torch.tensor(cov_matrix, dtype=torch.float)\n        weights = torch.tensor(weights, dtype=torch.float)\n        # Your code here\n        pass",
    "example_input": "cov_matrix = [[0.1, 0.02], [0.02, 0.15]], weights = [0.6, 0.4]",
    "example_output": "0.0696",
    "example_reasoning": "The covariance between the two assets reduces the overall variance compared to a simple weighted average of individual variances.",
    "learn_content": "IyMgUG9ydGZvbGlvIFZhcmlhbmNlIGluIE1vZGVybiBQb3J0Zm9saW8gVGhlb3J5CgpJbiBmaW5hbmNlLCAqKnBvcnRmb2xpbyB2YXJpYW5jZSoqIG1lYXN1cmVzIHRoZSBvdmVyYWxsIHJpc2sgb2YgYSBwb3J0Zm9saW8gb2YgYXNzZXRzLiBJdCBhY2NvdW50cyBmb3Igbm90IG9ubHkgdGhlIHZhcmlhbmNlIG9mIGVhY2ggaW5kaXZpZHVhbCBhc3NldCBidXQgYWxzbyB0aGUgY292YXJpYW5jZXMgYmV0d2VlbiB0aGVtLgoKIyMjIEZvcm11bGEKSWYgYSBwb3J0Zm9saW8gaGFzICRuJCBhc3NldHMgd2l0aCB3ZWlnaHRzICR3XzEsIHdfMiwgXGRvdHMsIHdfbiQgYW5kIGNvdmFyaWFuY2UgbWF0cml4ICRcU2lnbWEkLCB0aGVuIHRoZSBwb3J0Zm9saW8gdmFyaWFuY2UgaXM6CgokJApcc2lnbWFfcF4yID0gXG1hdGhiZnt3fV5UIFxTaWdtYSBcbWF0aGJme3d9CiQkCgpXaGVyZToKLSAkXG1hdGhiZnt3fSQgaXMgdGhlIHdlaWdodCB2ZWN0b3Igb2Ygc2l6ZSAkbiQuCi0gJFxTaWdtYSQgaXMgdGhlICRuIFx0aW1lcyBuJCBjb3ZhcmlhbmNlIG1hdHJpeCBvZiBhc3NldCByZXR1cm5zLgoKIyMjIEV4YW1wbGUKRm9yIHR3byBhc3NldHMgd2l0aCB3ZWlnaHRzICR3ID0gWzAuNiwgMC40XSQgYW5kIGNvdmFyaWFuY2UgbWF0cml4OgokJApcU2lnbWEgPSBcYmVnaW57Ym1hdHJpeH0KMC4xICYgMC4wMiBcXAowLjAyICYgMC4xNQpcZW5ke2JtYXRyaXh9CiQkCgpUaGUgcG9ydGZvbGlvIHZhcmlhbmNlIGlzIGNvbXB1dGVkIGJ5IGFwcGx5aW5nIHRoZSBmb3JtdWxhIGFib3ZlLCB3aGljaCBnaXZlcyAkMC4wNjc2JC4K",
    "solution_code": "import torch\n\nclass Solution:\n    def calculate_portfolio_variance(self, cov_matrix: list[list[float]], weights: list[float]) -> float:\n        cov = torch.as_tensor(cov_matrix, dtype=torch.float)\n        w = torch.as_tensor(weights, dtype=torch.float).flatten()  # keep 1D\n\n        # Optional shape guard\n        if cov.ndim != 2 or cov.shape[0] != cov.shape[1] or cov.shape[0] != w.numel():\n            raise ValueError(\"Shape mismatch: cov must be (n,n) and weights length must be n.\")\n\n        # Quadratic form without any transpose ops\n        var = torch.dot(w, cov @ w).item()\n        return float(var)\n\n",
    "test_cases": [
      {
        "test": [
          "print(round(calculate_portfolio_variance(torch.tensor([[0.1, 0.02],[0.02,0.15]]), torch.tensor([0.6,0.4])),4))"
        ],
        "expected_output": 0.0696
      },
      {
        "test": [
          "print(round(calculate_portfolio_variance(torch.tensor([[0.05,0.01,0.0],[0.01,0.07,0.02],[0.0,0.02,0.1]]), torch.tensor([0.5,0.3,0.2])),4))"
        ],
        "expected_output": 0.0282
      }
    ],
    "function_name": "calculate_portfolio_variance"
  },
  {
    "id": 184,
    "title": "Empirical Probability Mass Function (PMF)",
    "difficulty": "easy",
    "category": "probability & statistics",
    "problem_description": "IyMgUHJvYmxlbQoKR2l2ZW4gYSBsaXN0IG9mIGludGVnZXIgc2FtcGxlcyBkcmF3biBmcm9tIGEgZGlzY3JldGUgZGlzdHJpYnV0aW9uLCBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0byBjb21wdXRlIHRoZSBlbXBpcmljYWwgUHJvYmFiaWxpdHkgTWFzcyBGdW5jdGlvbiAoUE1GKS4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gYSBsaXN0IG9mIGAodmFsdWUsIHByb2JhYmlsaXR5KWAgcGFpcnMgc29ydGVkIGJ5IHRoZSB2YWx1ZSBpbiBhc2NlbmRpbmcgb3JkZXIuIElmIHRoZSBpbnB1dCBpcyBlbXB0eSwgcmV0dXJuIGFuIGVtcHR5IGxpc3Qu",
    "starter_code": "\n\nclass Solution:\n    def empirical_pmf(self, samples):\n        \"\"\"\n        Given an iterable of integer samples, return a list of (value, probability)\n        pairs sorted by value ascending.\n        \"\"\"\n        # TODO: Implement the function\n        pass",
    "example_input": "samples = [1, 2, 2, 3, 3, 3]",
    "example_output": "[(1, 0.16666666666666666), (2, 0.3333333333333333), (3, 0.5)]",
    "example_reasoning": "Counts are {1:1, 2:2, 3:3} over 6 samples, so probabilities are 1/6, 2/6, and 3/6 respectively, returned sorted by value.",
    "learn_content": "CiMgTGVhcm4gU2VjdGlvbgoKIyBQcm9iYWJpbGl0eSBNYXNzIEZ1bmN0aW9uIChQTUYpIOKAlCBTaW1wbGUgRXhwbGFuYXRpb24KCkEgKipwcm9iYWJpbGl0eSBtYXNzIGZ1bmN0aW9uIChQTUYpKiogZGVzY3JpYmVzIGhvdyBwcm9iYWJpbGl0aWVzIGFyZSBhc3NpZ25lZCB0byB0aGUgcG9zc2libGUgb3V0Y29tZXMgb2YgYSAqKmRpc2NyZXRlIHJhbmRvbSB2YXJpYWJsZSoqLgoKLSBJdCB0ZWxscyB5b3UgdGhlIGNoYW5jZSBvZiBlYWNoIHNwZWNpZmljIG91dGNvbWUuICAKLSBFYWNoIHByb2JhYmlsaXR5IGlzIG5vbi1uZWdhdGl2ZS4gIAotIFRoZSB0b3RhbCBvZiBhbGwgcHJvYmFiaWxpdGllcyBhZGRzIHVwIHRvIDEuCgojIyBFc3RpbWF0aW5nIGZyb20gZGF0YQpJZiB0aGUgdHJ1ZSBwcm9iYWJpbGl0aWVzIGFyZSB1bmtub3duLCB5b3UgY2FuIGVzdGltYXRlIHRoZW0gd2l0aCBhbiAqKmVtcGlyaWNhbCBQTUYqKjoKLSBDb3VudCBob3cgb2Z0ZW4gZWFjaCBvdXRjb21lIGFwcGVhcnMuICAKLSBEaXZpZGUgYnkgdGhlIHRvdGFsIG51bWJlciBvZiBvYnNlcnZhdGlvbnMuICAKCiMjIEV4YW1wbGUKT2JzZXJ2ZWQgc2VxdWVuY2U6IGAxLCAyLCAyLCAzLCAzLCAzYCAoNiBvdXRjb21lcyB0b3RhbCkKLSDigJwx4oCdIGFwcGVhcnMgb25jZSDihpIgZXN0aW1hdGVkIHByb2JhYmlsaXR5ID0gMS82ICAKLSDigJwy4oCdIGFwcGVhcnMgdHdpY2Ug4oaSIGVzdGltYXRlZCBwcm9iYWJpbGl0eSA9IDIvNiA9IDEvMyAgCi0g4oCcM+KAnSBhcHBlYXJzIHRocmVlIHRpbWVzIOKGkiBlc3RpbWF0ZWQgcHJvYmFiaWxpdHkgPSAzLzYgPSAxLzIgIAoKCiAgICA=",
    "solution_code": "from collections import Counter\n\nclass Solution:\n    def empirical_pmf(self, samples):\n        \"\"\"\n        Given an iterable of integer samples, return a list of (value, probability)\n        pairs sorted by value ascending.\n        \"\"\"\n        samples = list(samples)\n        if not samples:\n            return []\n        total = len(samples)\n        cnt = Counter(samples)\n        result = [(k, cnt[k] / total) for k in sorted(cnt.keys())]\n        return result",
    "test_cases": [
      {
        "test": [
          [
            1,
            2,
            2,
            3,
            3,
            3
          ]
        ],
        "expected_output": [
          [
            1,
            0.16666666666666666
          ],
          [
            2,
            0.3333333333333333
          ],
          [
            3,
            0.5
          ]
        ]
      },
      {
        "test": [
          [
            5,
            5,
            5,
            5
          ]
        ],
        "expected_output": [
          [
            5,
            1.0
          ]
        ]
      }
    ],
    "function_name": "empirical_pmf"
  },
  {
    "id": 185,
    "title": "Optical Flow EPE with Masks (OmniWorld-style metric)",
    "difficulty": "medium",
    "category": "computer vision",
    "problem_description": "Q29tcHV0ZSB0aGUgbWVhbiBFbmQtUG9pbnQgRXJyb3IgKEVQRSkgYmV0d2VlbiBhIHByZWRpY3RlZCBvcHRpY2FsIGZsb3cgZmllbGQgYW5kIGl0cyBncm91bmQtdHJ1dGgsIG9wdGlvbmFsbHkgdXNpbmcgYW4gb2NjbHVzaW9uL3ZhbGlkaXR5IG1hc2sgYW5kIGFuIG91dGxpZXIgY2xpcCAobWF4X2Zsb3cpLiBZb3VyIGZ1bmN0aW9uIHNob3VsZCBpZ25vcmUgaW52YWxpZCB2YWx1ZXMgKE5hTi/CsUluZiksIHN1cHBvcnQgUHl0aG9uIGxpc3RzIG9yIE51bVB5IGFycmF5cyB3aXRoIHNoYXBlIChILCBXLCAyKSwgYW5kIHJldHVybiAtMSBpZiBpbnB1dHMgYXJlIG1hbGZvcm1lZCBvciBubyB2YWxpZCBwaXhlbHMgcmVtYWluLg==",
    "starter_code": "import torch\n\nclass Solution:\n    def flow_epe_torch(self, pred, gt, mask=None, max_flow=None) -> torch.Tensor:\n        \"\"\"\n        Compute mean End-Point Error (EPE) between predicted and ground-truth optical flow.\n\n        Args:\n            pred, gt: tensors or array-likes of shape (H, W, 2).\n            mask: optional tensor/array broadcastable to (H, W); 1=include, 0=ignore.\n            max_flow: optional float to clip per-pixel EPE.\n\n        Returns:\n            torch.Tensor scalar. Returns tensor(-1.0) on invalid input or if no valid pixels.\n        \"\"\"\n        p = torch.as_tensor(pred, dtype=torch.float32)\n        g = torch.as_tensor(gt,   dtype=torch.float32)\n\n        # Your implementation here\n        pass",
    "example_input": "pred = [[[1,0],[0,1]], [[-1,0],[0,-1]]]; gt = [[[0,0],[0,0]], [[0,0],[0,0]]]\nprint(round(flow_epe(pred, gt), 4))",
    "example_output": "1.0",
    "example_reasoning": "Each of the four pixels has EPE = 1, so the mean is 1.0.",
    "learn_content": "IyMgV2h5IHRoaXMgbWF0dGVycwpJbiA0RCB3b3JsZCBtb2RlbGluZyAoc3BhdGlvLXRlbXBvcmFsIHVuZGVyc3RhbmRpbmcpLCBkYXRhc2V0cyBsaWtlICoqT21uaVdvcmxkKiogcHJvdmlkZSBtdWx0aS1tb2RhbCBzdXBlcnZpc2lvbiAoUkdCLCBkZXB0aCwgcG9zZXMsIG9wdGljYWwgZmxvdywgbWFza3MpLiBPcHRpY2FsIGZsb3cgY29tcGFyZXMgcGl4ZWwgbW90aW9uIGJldHdlZW4gZnJhbWVzOyBhY2N1cmF0ZSBmbG93IGlzIHZpdGFsIGZvciBjYW1lcmEtY29udHJvbGxlZCB2aWRlbyBnZW5lcmF0aW9uIGFuZCBnZW9tZXRyeS1hd2FyZSBtb2RlbGluZy4KCiMjIE9wdGljYWwgRmxvdyAmIEVQRQpHaXZlbiBwcmVkaWN0ZWQgZmxvdyAkXGhhdHtcbWF0aGJme2Z9fSh4LHkpID0gW3VfcCwgdl9wXSQgYW5kIGdyb3VuZC10cnV0aCBmbG93ICRcbWF0aGJme2Z9KHgseSkgPSBbdV9nLCB2X2ddJCwgdGhlICoqRW5kLVBvaW50IEVycm9yIChFUEUpKiogYXQgcGl4ZWwgJCh4LHkpJCBpcyB0aGUgRXVjbGlkZWFuIGRpc3RhbmNlOgokJApcb3BlcmF0b3JuYW1le0VQRX0oeCx5KSA9IFxzcXJ0eyh1X3AgLSB1X2cpXjIgKyAodl9wIC0gdl9nKV4yfS4KJCQKVGhlICoqbWVhbiBFUEUqKiBvdmVyIHZhbGlkIHBpeGVscyAkXE9tZWdhJCBpczoKJCQKXG92ZXJsaW5le1xvcGVyYXRvcm5hbWV7RVBFfX0gPSBcZnJhY3sxfXt8XE9tZWdhfH0gXHN1bV97KHgseSlcaW5cT21lZ2F9IFxvcGVyYXRvcm5hbWV7RVBFfSh4LHkpLgokJAoKIyMjIFZhbGlkaXR5ICYgTWFza3MKLSAqKk1hc2tzKio6IEEgYmluYXJ5IG1hc2sgJE0oeCx5KSBcaW4gXHswLDFcfSQgaW5kaWNhdGVzIHdoZXRoZXIgYSBwaXhlbCBzaG91bGQgYmUgaW5jbHVkZWQgKDEpIG9yIGlnbm9yZWQgKDApLgotICoqSW52YWxpZCB2YWx1ZXMqKjogUGl4ZWxzIHdoZXJlIGVpdGhlciBwcmVkaWN0aW9uIG9yIGdyb3VuZC10cnV0aCBjb250YWlucyBOYU4vSW5mIHNob3VsZCBiZSBleGNsdWRlZC4KLSAqKkNsaXBwaW5nIChvcHRpb25hbCkqKjogVG8gcmVkdWNlIG91dGxpZXIgaW5mbHVlbmNlLCB5b3UgbWF5IGNhcCBlYWNoIHBlci1waXhlbCBFUEUgd2l0aCBhIHRocmVzaG9sZCAkXHRhdSQ6ICRcbWluKFxvcGVyYXRvcm5hbWV7RVBFfSwgXHRhdSkkLgoKIyMgUmVwcm9kdWNpYmlsaXR5IGluIHRlc3RzCldoZW4gcmFuZG9tbmVzcyBpcyB1c2VkIHRvIHN5bnRoZXNpemUgc21hbGwgaW5wdXRzLCBmaXggc2VlZHMgZm9yIGRldGVybWluaXNtOgotICoqTnVtUHkqKjogYG5wLnJhbmRvbS5zZWVkKDQyKWAKLSAqKlB5VG9yY2gqKjogYHRvcmNoLm1hbnVhbF9zZWVkKDQyKWAKLSAqKlRpbnlncmFkKio6IHNlZWQgTnVtUHkvUHl0aG9uIGFuZCB3cmFwIGFycmF5cyB3aXRoIGBUZW5zb3IoLi4uKWAuCgpSb3VuZCBmaW5hbCBwcmludGVkIHJlc3VsdHMgdXNpbmcgYHJvdW5kKHZhbHVlLCBrKWAgdG8gbWF0Y2ggZXhwZWN0ZWQgb3V0cHV0cy4=",
    "solution_code": "import torch\n\nclass Solution:\n    def flow_epe_torch(self, pred, gt, mask=None, max_flow=None) -> torch.Tensor:\n        p = torch.as_tensor(pred, dtype=torch.float32)\n        g = torch.as_tensor(gt,   dtype=torch.float32)\n\n        if p.shape != g.shape or p.dim() != 3 or p.shape[-1] != 2:\n            return torch.tensor(-1.0)\n\n        diff = p - g  # (..., 2)\n        epe = torch.linalg.norm(diff, dim=-1)  # (H, W)\n\n        if max_flow is not None:\n            epe = torch.clamp(epe, max=float(max_flow))\n\n        valid = torch.isfinite(epe)\n\n        if mask is not None:\n            m = torch.as_tensor(mask, dtype=torch.float32)\n            try:\n                m = m.expand_as(epe)\n            except RuntimeError:\n                try:\n                    m = m + torch.zeros_like(epe)\n                except Exception:\n                    return torch.tensor(-1.0)\n            valid = valid & (m > 0.5)\n\n        vals = epe[valid]\n        if vals.numel() == 0:\n            return torch.tensor(-1.0)\n\n        return vals.mean()",
    "test_cases": [
      {
        "test": [
          "print(round(flow_epe_torch(torch.tensor([[[1,0],[0,1]]), torch.tensor([[-1,0],[0,-1]])],\n                       torch.tensor([[[0,0],[0,0]]), torch.tensor([[0,0],[0,0]])]), 4))"
        ],
        "expected_output": 1.0
      },
      {
        "test": [
          "pred = torch.tensor([[[2,0],[0,2]]), torch.tensor([[3,4],[0,0]])]\ngt   = torch.tensor([[[0,0],[0,0]]), torch.tensor([[0,0],[0,0]])]\nmask = torch.tensor([[0,1],[1,0]])  # include (0,1) and (1,0)\nprint(round(flow_epe_torch(pred, gt, mask=mask), 4))"
        ],
        "expected_output": 3.5
      }
    ],
    "function_name": "flow_epe_torch"
  }
]